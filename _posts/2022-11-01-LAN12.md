---
title: "Word2vec: Distributed Representations of Words and Phrases and their Compositionality ì •ë¦¬"
date:   2022-10-10
excerpt: "Distributed Representations of Words and Phrases and their Compositionality"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# ì› ë…¼ë¬¸
[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)    

**[ì†ŒìŠ¤ì½”ë“œ]**     
* [git adress]()


**[ì‚¬ì „ í•™ìŠµ]**
* ì½ê¸° ì „ ì•„ë˜ì˜ í¬ìŠ¤íŠ¸ë“¤ì„ ì½ì–´ì•¼ ë¬´ìŠ¨ì†Œë¦°ì§€ ì•Œì•„ë“£ê¸° í¸í•˜ë‹¤..!   
* 

---

# **Abstract**
 

---
-----

# **Introduction**

## ë¶„ì‚° í‘œí˜„(Distributed representations)   
ë²¡í„° ê³µê°„ì—ì„œ ë‹¨ì–´ì˜ **ë¶„ì‚° í‘œí˜„(Distributed representations)** ì€ í•™ìŠµ algorithmsì´ **ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ê·¸ë£¹í™”**í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ë° ë„ì›€ì´ ëœë‹¤.     
ë‹¨ì–´ í‘œí˜„ì˜ ê°€ì¥ ì´ˆê¸° ì‚¬ìš© ì¤‘ í•˜ë‚˜ëŠ” [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0) ì´ë‹¤.     
â¡ ì´ ì•„ì´ë””ì–´ëŠ” ì´í›„ í†µê³„ ì–¸ì–´ ëª¨ë¸ë§ì— ìƒë‹¹í•œ ì„±ê³µì„ ê±°ë‘ë©° ì ìš©ë˜ì—ˆë‹¤. [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)           
â¡ í›„ì† ì‘ì—…ì€ ì•„ë˜ì˜ ë…¼ë¬¸ë“¤ì´ í¬í•¨ëœë‹¤.    
* ìë™ ìŒì„± ì¸ì‹ ë° ê¸°ê³„ ë²ˆì—­ì— ëŒ€í•œ ì‘ìš© í”„ë¡œê·¸ë¨     
    * [Statistical Language Models Based on Neural Networks](https://www.semanticscholar.org/paper/Statistical-Language-Models-Based-on-Neural-U%C4%8Den%C3%AD-Brn%C4%9B/96364af2d208ea75ca3aeb71892d2f7ce7326b55)    
    * [Continuous space language models](https://www.sciencedirect.com/science/article/pii/S0885230806000325)          
* ê´‘ë²”ìœ„í•œ NLP ì‘ì—…     
    * [A unified architecture for natural language processing: deep neural networks with multitask learning](https://dl.acm.org/doi/10.1145/1390156.1390177)     
    *  [Domain adaptation for large-scale sentiment classification: a deep learning approach](https://dl.acm.org/doi/10.5555/3104482.3104547)      
    *  [Linguistic Regularities in Continuous Space Word Representations](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)     
    *  [Recursive Deep Models for Discourse Parsing](https://aclanthology.org/D14-1220.pdf)      
    *  [From Frequency to Meaning: Vector Space Models of Semantics](https://arxiv.org/abs/1003.1141)      
    *  [Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase](https://aclanthology.org/Q13-1029/)      
    *  [WSABIE: Scaling Up To Large Vocabulary Image Annotation](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37180.pdf)   

---

## Skip-gram


ìµœê·¼ì—, ëŒ€ëŸ‰ì˜ ë¹„ì •í˜• í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë‹¨ì–´ì˜ ê³ í’ˆì§ˆ ë²¡í„° í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•ì¸ [Skip-gram ëª¨ë¸](https://arxiv.org/abs/1301.3781)ì„ ì†Œê°œí–ˆë‹¤. ([ë” ìì„¸íˆ ì•Œì•„ë³´ê¸°](https://yerimoh.github.io/DL14/))      
ì´ì „ì— ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœ ëŒ€ë¶€ë¶„ì˜ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì™€ ë‹¬ë¦¬,   
Skip-gram ëª¨ë¸[Figure 1]ì˜ í›ˆë ¨ì€ ì¡°ë°€í•œ í–‰ë ¬ ê³±ì…ˆ(dense matrix multiplications)ì„ ìˆ˜ë°˜í•˜ì§€ ì•ŠëŠ”ë‹¤.      
ì´ëŠ” í›ˆë ¨ì„ ë§¤ìš° **íš¨ìœ¨ì **ìœ¼ë¡œ ë§Œë“ ë‹¤.     
ìµœì í™”ëœ ë‹¨ì¼ ê¸°ê³„ êµ¬í˜„ì€ í•˜ë£¨ì— 1,000ì–µ ê°œ ì´ìƒì˜ ë‹¨ì–´ë¥¼ í›ˆë ¨í•  ìˆ˜ ìˆë‹¤.      


![image](https://user-images.githubusercontent.com/76824611/202935563-4528c5c7-fcd1-4493-bc60-a5c1a0948d06.png)
[Figure 1] The Skip-gram model architecture. í›ˆë ¨ ëª©í‘œëŠ” ê·¼ì²˜ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ì¢‹ì€ ë‹¨ì–´ ë²¡í„° í‘œí˜„ì„ ë°°ìš°ëŠ” ê²ƒì´ë‹¤.       


ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ëœ ë‹¨ì–´ í‘œí˜„ì€ í•™ìŠµëœ ë²¡í„°ê°€ ë§ì€ **ì–¸ì–´ì  ê·œì¹™ì„±ê³¼ íŒ¨í„´ì„ ëª…ì‹œì ìœ¼ë¡œ ì¸ì½”ë”©**í•˜ê¸° ë•Œë¬¸ì— ë§¤ìš° í¥ë¯¸ë¡­ë‹¤. ë‹¤ì†Œ ë†€ëê²Œë„, ì´ëŸ¬í•œ íŒ¨í„´ë“¤ ì¤‘ ë§ì€ ê²ƒë“¤ì´ ì„ í˜• ë³€í™˜ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.   
ex) ë²¡í„° ê³„ì‚° ```vec("ë§ˆë“œë¦¬ë“œ") - vec("ìŠ¤í˜ì¸") + vec("í”„ë‘ìŠ¤")```ì˜ ê²°ê³¼ëŠ” ë‹¤ë¥¸ ë‹¨ì–´ ë²¡í„°ë³´ë‹¤ ```vec("íŒŒë¦¬")```ì— ë” ê°€ ê°€ê¹Œì›€.


---

## ë³¸ ë…¼ë¬¸ì˜ Skip-gram ê°œì„ 

ë³¸ ë…¼ë¬¸ì—ì„œ ìš°ë¦¬ëŠ” ì›ë˜ì˜ Skip-gram modelì˜ ëª‡ ê°€ì§€ í™•ì¥ì„ ì œì‹œí•œë‹¤.        
* ë³¸ ë…¼ë¬¸ì˜ ì¥ì ì€ í›ˆë ¨ ì¤‘ **ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ì˜ subsamplinì´ ìƒë‹¹í•œ ì†ë„ í–¥ìƒ**(ì•½ 2ë°° - 10ë°°)ì„ ê°€ì ¸ì˜¤ê³ ,     
**ë¹ˆë„ê°€ ë‚®ì€ ë‹¨ì–´ í‘œí˜„ì˜ ì •í™•ë„ë¥¼ í–¥ìƒ**ì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.             
* ë³¸ ë…¼ë¬¸ì€ ì´ì „ ì—°êµ¬ì—ì„œ ì‚¬ìš©ëœ more complex hierarchical softmaxì— ë¹„í•´ ë¹ˆë²ˆí•œ ë‹¨ì–´ì— ëŒ€í•´ ë” ë¹ ë¥¸ í›ˆë ¨ê³¼ ë” ë‚˜ì€ ë²¡í„° í‘œí˜„ì„ ì´ˆë˜í•˜ëŠ” ìŠ¤í‚µê·¸ë¨ ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•œ **ë‹¨ìˆœí™”ëœ Noise Contrastive Estimation** (NCE)ì„ ì œì‹œí•œë‹¤.

<details>
<summary>ğŸ“œ Noise Contrastive Estimation (NCE) ë€? </summary>
<div markdown="1">
   
CBOWì™€ Skip-Gram ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë¹„ìš© ê³„ì‚° ì•Œê³ ë¦¬ì¦˜ì„ ì¹­í•œë‹¤.      
ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ softMax í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ **ìƒ˜í”Œë§ìœ¼ë¡œ ì¶”ì¶œí•œ ì¼ë¶€ì— ëŒ€í•´ì„œë§Œ ì ìš©**í•˜ëŠ” ë°©ë²•ì„ ë§í•œë‹¤.     
kê°œì˜ ëŒ€ë¹„ë˜ëŠ”(contrastive) ë‹¨ì–´ë“¤ì„ noise distributionì—ì„œ êµ¬í•´ì„œ (ëª¬í…Œì¹´ë¥¼ë¡œ) í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒì´ ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.     
â¡ [Hierarchical SoftMax](https://yerimoh.github.io/DL15/#1%EF%B8%8F%E2%83%A3-embedding-%EA%B3%84%EC%B8%B5-%EB%8F%84%EC%9E%85)ì™€ [Negative Sampling](https://yerimoh.github.io/DL15/#2%EF%B8%8F%E2%83%A3-%EB%84%A4%EA%B1%B0%ED%8B%B0%EB%B8%8C-%EC%83%98%ED%94%8C%EB%A7%81%EC%9D%B4%EB%9E%80-%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98-%EB%8F%84%EC%9E%85) ë“±ì˜ ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ ë‹¨ì–´ ê°¯ìˆ˜ê°€ ë§ì„ ë•Œ ì‚¬ìš©í•˜ê³ , NCEë¥¼ ì‚¬ìš©í•˜ë©´ ë¬¸ì œë¥¼ (ì‹¤ì œ ë¶„í¬ì—ì„œ ì–»ì€ ìƒ˜í”Œ)ê³¼ (ì¸ê³µì ìœ¼ë¡œ ë§Œë“  ì¡ìŒ ë¶„í¬ì—ì„œ ì–»ì€ ìƒ˜í”Œ)ì„ êµ¬ë³„í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ë°”ê¿€ ìˆ˜ ìˆê²Œ ëœë‹¤.

 Negative Samplingì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª©ì  í•¨ìˆ˜ëŠ” ê²°ê³¼ê°’ì´ ìµœëŒ€í™”ë  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ êµ¬ì„±í•œë‹¤. í˜„ì¬(ëª©í‘œ, target, positive) ë‹¨ì–´ì—ëŠ” ë†’ì€ í™•ë¥ ì„ ë¶€ì—¬í•˜ê³ , ë‚˜ë¨¸ì§€ ë‹¨ì–´(negative, noise)ì—ëŠ” ë‚®ì€ í™•ë¥ ì„ ë¶€ì—¬í•´ì„œ ê°€ì¥ í° ê°’ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê³µì‹ì„ ì‚¬ìš©í•œë‹¤. 

   
</div>
</details>  


**[ê´€ìš©êµ¬ í‘œí˜„ì˜ ê°œì„ ]**         
* ë‹¨ì–´ í‘œí˜„ì€ ê°œë³„ ë‹¨ì–´ì˜ êµ¬ì„±ì´ ì•„ë‹Œ **ê´€ìš©êµ¬ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ì œí•œ**ëœë‹¤.      
ex) â€œBoston Globeâ€ëŠ” ì‹ ë¬¸ì´ê¸° ë•Œë¬¸ì— â€œBostonâ€ê³¼ â€œGlobeâ€ì˜ ì˜ë¯¸ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ê²°í•©ëœ ê²ƒì´ ì•„ë‹ˆë‹¤.     
* ë”°ë¼ì„œ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ êµ¬ë¬¸ì„ í‘œí˜„í•˜ë©´ Skip-gram modelì´ í›¨ì”¬ ë” í‘œí˜„ë ¥ì´ ë›°ì–´ë‚˜ë‹¤.       
* [recursive autoencoders](https://aclanthology.org/D14-1220.pdf)ì™€ ê°™ì´ ë‹¨ì–´ ë²¡í„°ë¥¼ êµ¬ì„±í•˜ì—¬ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ í‘œí˜„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ëŠ” ë‹¤ë¥¸ ê¸°ìˆ ë„ **ë‹¨ì–´ ë²¡í„° ëŒ€ì‹  êµ¬ë¬¸ ë²¡í„°ë¥¼ ì‚¬ìš©**í•¨ìœ¼ë¡œì¨ ë” ë‚˜ì€ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.      


**[ë‹¨ì–´ ê¸°ë°˜ ëª¨ë¸ì—ì„œ êµ¬ë¬¸ ê¸°ë°˜ ëª¨ë¸ë¡œì˜ í™•ì¥ ë°©ë²•]**      
* ë‹¨ì–´ ê¸°ë°˜ ëª¨ë¸ì—ì„œ êµ¬ë¬¸ ê¸°ë°˜ ëª¨ë¸ë¡œì˜ í™•ì¥ ë°©ë²•        
  **1)** ë¨¼ì € ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë§ì€ ìˆ˜ì˜ êµ¬ë¬¸ì„ ì‹ë³„    
  **2)** í›ˆë ¨ ì¤‘ì— êµ¬ë¬¸ì„ ê°œë³„ í† í°ìœ¼ë¡œ ì²˜ë¦¬    
* êµ¬ë¬¸ ë²¡í„°ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê¸° ìœ„í•´ ë‹¨ì–´ì™€ êµ¬ë¬¸ì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” ì¼ë ¨ì˜ ì•„ë‚ ë¡œê·¸ analogy ì‘ì—…ì„ ê°œë°œí–ˆë‹¤.    
  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì¼ë°˜ì ì¸ analogy ìŒì€ â€œMontrealâ€:â€œMontreal Canadiensâ€::â€œTorontoâ€:â€œToronto Maple Leafsâ€  
  ```vec(â€œMontreal Canadiensâ€) - vec(â€œMontrealâ€) + vec(â€œTorontoâ€)```ì— ê°€ì¥ ê°€ê¹Œìš´ í‘œí˜„ì´ ```vec(â€œToronto Maple Leafsâ€)```ì´ë©´ ì •ë‹µìœ¼ë¡œ ê°„ì£¼ëœë‹¤.        




**[ìŠ¤í‚µê·¸ë¨ ëª¨ë¸ì˜ ë˜ ë‹¤ë¥¸ í¥ë¯¸ë¡œìš´ íŠ¹ì„±]**       
* ë§ˆì§€ë§‰ìœ¼ë¡œ, ë³¸ ë…¼ë¬¸ì€ ìŠ¤í‚µê·¸ë¨ ëª¨ë¸ì˜ ë˜ ë‹¤ë¥¸ í¥ë¯¸ë¡œìš´ íŠ¹ì„±ì„ ì„¤ëª…í•œë‹¤.    
ìš°ë¦¬ëŠ” **ê°„ë‹¨í•œ ë²¡í„° ì¶”ê°€ê°€ ì¢…ì¢… ì˜ë¯¸ ìˆëŠ” ê²°ê³¼ë¥¼ ì‚°ì¶œ**í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.    
* ex) ```vec(â€œRussiaâ€) + vec(â€œriverâ€)ì€ vec(â€œVolga Riverâ€)```ì— ê°€ê¹ë‹¤.           
     ```vec(â€œGermanyâ€) + vec(â€œcapitalâ€)ì€ vec(â€œBerlinâ€)```ì— ê°€ê¹ë‹¤.       
* ì´ëŸ¬í•œ êµ¬ì„±ì„±ì€ ë‹¨ì–´ ë²¡í„° í‘œí˜„ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ **ìˆ˜í•™ì  ì—°ì‚°ì„ ì‚¬ìš©**í•˜ì—¬ ëª…í™•í•˜ì§€ ì•Šì€ ìˆ˜ì¤€ì˜ **ì–¸ì–´ ì´í•´**ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤.


----
----

# **2 The Skip-gram Model**
Skip-gram modelì˜ í›ˆë ¨ ëª©í‘œëŠ” ë¬¸ì¥ì´ë‚˜ ë¬¸ì„œì—ì„œ ì£¼ë³€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ìœ ìš©í•œ ë‹¨ì–´ í‘œí˜„ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ì¦‰, sequence of training words $$w_1, w_2, w_3, ., w_T$$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, Skip-gram modelì˜ ëª©í‘œëŠ” **í‰ê·  ë¡œê·¸ í™•ë¥ ì„ ìµœëŒ€í™”**í•˜ëŠ” ê²ƒì´ë‹¤.
![image](https://user-images.githubusercontent.com/76824611/202949963-20a1a545-0f46-4bd9-80d5-72f4c071088d.png)
![image](https://user-images.githubusercontent.com/76824611/202949984-487e81dd-7fcd-4290-a3ed-82a47741ac16.png)


