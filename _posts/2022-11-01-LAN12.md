---
title: "FastText: Enriching Word Vectors with Subword Information ì •ë¦¬"
date:   2022-11-10
excerpt: "Enriching Word Vectors with Subword Information"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# ì› ë…¼ë¬¸
[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)    

**[ì†ŒìŠ¤ì½”ë“œ]**     
* [git adress]()


**[ì‚¬ì „ í•™ìŠµ]**
* ì½ê¸° ì „ ì•„ë˜ì˜ í¬ìŠ¤íŠ¸ë“¤ì„ ì½ì–´ì•¼ ë¬´ìŠ¨ì†Œë¦°ì§€ ì•Œì•„ë“£ê¸° í¸í•˜ë‹¤..!   
* 

---

# Abstract
**[ê¸°ì¡´ ëª¨ë¸ì˜ ë¬¸ì œ]**      
* ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ í° ë§ë­‰ì¹˜ì— ëŒ€í•´ í›ˆë ¨ëœ Continuous word representationsì€ ë§ì€ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ìœ ìš©í•˜ë‹¤.    
ì´ëŸ¬í•œ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ì¸ê¸° ìˆëŠ” ëª¨ë¸ì€ ê° ë‹¨ì–´ì— ê³ ìœ í•œ ë²¡í„°ë¥¼ í• ë‹¹í•˜ì—¬ **ë‹¨ì–´ì˜ í˜•íƒœë¥¼ ë¬´ì‹œ**í•œë‹¤.    
ì´ë ‡ê²Œ ë‹¨ì–´ì˜ í˜•ìƒì„ ë¬´ì‹œí•˜ëŠ” ë¬¸ì œëŠ” íŠ¹íˆ **ì–´íœ˜ê°€ ë§ê³  í¬ê·€í•œ ë‹¨ì–´ê°€ ë§ì€ ì–¸ì–´**ì˜ ê²½ìš° ë” ë¬¸ì œê°€ ëœë‹¤.    

**[ë³¸ ë…¼ë¬¸ì—ì„œì˜ í•´ê²°ë²•]**      
* ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **skipgram model**ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•œë‹¤.     
ì—¬ê¸°ì„œ ê° ë‹¨ì–´ëŠ” ë¬¸ì n-gramsì˜ bag of characterë¡œ í‘œí˜„ëœë‹¤.     
ë²¡í„° í‘œí˜„ì€ ê° ë¬¸ì n-gramsê³¼ ì—°ê´€ë˜ì–´ ìˆìœ¼ë©°, ë‹¨ì–´ëŠ” ì´ëŸ¬í•œ í‘œí˜„ì˜ í•©ìœ¼ë¡œ í‘œí˜„ëœë‹¤.     

**[skipgram modelì˜ ì¥ì ]**      
* ë¹ ë¦„      
* í° ë§ë­‰ì¹˜ì—ì„œ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ í›ˆë ¨ ê°€ëŠ¥      
* í›ˆë ¨ ë°ì´í„°ì— ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ë‹¨ì–´ì— ëŒ€í•œ ë‹¨ì–´ í‘œí˜„ì„ ê³„ì‚° ê°€ëŠ¥      
* ë‹¨ì–´ ìœ ì‚¬ì„±ê³¼ ìœ ì¶” ì‘ì—… ëª¨ë‘ì—ì„œ 9ê°œì˜ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ìš°ë¦¬ì˜ ë‹¨ì–´ í‘œí˜„ í‰ê°€í•œ(ord similarity and analogy tasks)ê²°ê³¼,          
ìµœê·¼ ì œì•ˆëœ í˜•íƒœí•™ì  ë‹¨ì–´ í‘œí˜„ê³¼ ë¹„êµí•˜ì—¬ ë²¡í„°ê°€ ì´ëŸ¬í•œ ì‘ì—…ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.     

---
-----

# Introduction

## ë¶„ì‚° í‘œí˜„(Distributed representations)   
ë²¡í„° ê³µê°„ì—ì„œ ë‹¨ì–´ì˜ **ë¶„ì‚° í‘œí˜„(Distributed representations)** ì€ í•™ìŠµ algorithmsì´ **ìœ ì‚¬í•œ ë‹¨ì–´ë¥¼ ê·¸ë£¹í™”**í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ë° ë„ì›€ì´ ëœë‹¤.     
ë‹¨ì–´ í‘œí˜„ì˜ ê°€ì¥ ì´ˆê¸° ì‚¬ìš© ì¤‘ í•˜ë‚˜ëŠ” [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0) ì´ë‹¤.     
â¡ ì´ ì•„ì´ë””ì–´ëŠ” ì´í›„ í†µê³„ ì–¸ì–´ ëª¨ë¸ë§ì— ìƒë‹¹í•œ ì„±ê³µì„ ê±°ë‘ë©° ì ìš©ë˜ì—ˆë‹¤. [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)           
â¡ í›„ì† ì‘ì—…ì€ ì•„ë˜ì˜ ë…¼ë¬¸ë“¤ì´ í¬í•¨ëœë‹¤.    
* ìë™ ìŒì„± ì¸ì‹ ë° ê¸°ê³„ ë²ˆì—­ì— ëŒ€í•œ ì‘ìš© í”„ë¡œê·¸ë¨     
    * [Statistical Language Models Based on Neural Networks](https://www.semanticscholar.org/paper/Statistical-Language-Models-Based-on-Neural-U%C4%8Den%C3%AD-Brn%C4%9B/96364af2d208ea75ca3aeb71892d2f7ce7326b55)    
    * [Continuous space language models](https://www.sciencedirect.com/science/article/pii/S0885230806000325)          
* ê´‘ë²”ìœ„í•œ NLP ì‘ì—…     
    * [A unified architecture for natural language processing: deep neural networks with multitask learning](https://dl.acm.org/doi/10.1145/1390156.1390177)     
    *  [Domain adaptation for large-scale sentiment classification: a deep learning approach](https://dl.acm.org/doi/10.5555/3104482.3104547)      
    *  [Linguistic Regularities in Continuous Space Word Representations](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)     
    *  [Recursive Deep Models for Discourse Parsing](https://aclanthology.org/D14-1220.pdf)      
    *  [From Frequency to Meaning: Vector Space Models of Semantics](https://arxiv.org/abs/1003.1141)      
    *  [Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase](https://aclanthology.org/Q13-1029/)      
    *  [WSABIE: Scaling Up To Large Vocabulary Image Annotation](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37180.pdf)   

---

## Skip-gram


ìµœê·¼ì—, ëŒ€ëŸ‰ì˜ ë¹„ì •í˜• í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë‹¨ì–´ì˜ ê³ í’ˆì§ˆ ë²¡í„° í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•ì¸ [Skip-gram ëª¨ë¸](https://arxiv.org/abs/1301.3781)ì„ ì†Œê°œí–ˆë‹¤. ([ë” ìì„¸íˆ ì•Œì•„ë³´ê¸°](https://yerimoh.github.io/DL14/))      
ì´ì „ì— ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœ ëŒ€ë¶€ë¶„ì˜ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì™€ ë‹¬ë¦¬,   
Skip-gram ëª¨ë¸[Figure 1]ì˜ í›ˆë ¨ì€ ì¡°ë°€í•œ í–‰ë ¬ ê³±ì…ˆ(dense matrix multiplications)ì„ ìˆ˜ë°˜í•˜ì§€ ì•ŠëŠ”ë‹¤.      
ì´ëŠ” í›ˆë ¨ì„ ë§¤ìš° **íš¨ìœ¨ì **ìœ¼ë¡œ ë§Œë“ ë‹¤.     
ìµœì í™”ëœ ë‹¨ì¼ ê¸°ê³„ êµ¬í˜„ì€ í•˜ë£¨ì— 1,000ì–µ ê°œ ì´ìƒì˜ ë‹¨ì–´ë¥¼ í›ˆë ¨í•  ìˆ˜ ìˆë‹¤.      


![image](https://user-images.githubusercontent.com/76824611/202935563-4528c5c7-fcd1-4493-bc60-a5c1a0948d06.png)
[Figure 1] The Skip-gram model architecture. í›ˆë ¨ ëª©í‘œëŠ” ê·¼ì²˜ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ì¢‹ì€ ë‹¨ì–´ ë²¡í„° í‘œí˜„ì„ ë°°ìš°ëŠ” ê²ƒì´ë‹¤.       


ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ëœ ë‹¨ì–´ í‘œí˜„ì€ í•™ìŠµëœ ë²¡í„°ê°€ ë§ì€ **ì–¸ì–´ì  ê·œì¹™ì„±ê³¼ íŒ¨í„´ì„ ëª…ì‹œì ìœ¼ë¡œ ì¸ì½”ë”©**í•˜ê¸° ë•Œë¬¸ì— ë§¤ìš° í¥ë¯¸ë¡­ë‹¤. ë‹¤ì†Œ ë†€ëê²Œë„, ì´ëŸ¬í•œ íŒ¨í„´ë“¤ ì¤‘ ë§ì€ ê²ƒë“¤ì´ ì„ í˜• ë³€í™˜ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.   
ex) ë²¡í„° ê³„ì‚° vec("ë§ˆë“œë¦¬ë“œ") - vec("ìŠ¤í˜ì¸") + vec("í”„ë‘ìŠ¤")ì˜ ê²°ê³¼ëŠ” ë‹¤ë¥¸ ë‹¨ì–´ ë²¡í„°ë³´ë‹¤ vec("íŒŒë¦¬")ì— ë” ê°€ ê°€ê¹Œì›€.


---

## ë³¸ ë…¼ë¬¸ì˜ Skip-gram ê°œì„ 

In this paper we present several extensions of the original Skip-gram model. We show that subsampling of frequent words during training results in a significant speedup (around 2x - 10x), and
improves accuracy of the representations of less frequent words. In addition, we present a simplified variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results
in faster training and better vector representations for frequent words, compared to more complex
hierarchical softmax that was used in the prior work [8].

ë³¸ ë…¼ë¬¸ì—ì„œ ìš°ë¦¬ëŠ” ì›ë˜ì˜ Skip-gram modelì˜ ëª‡ ê°€ì§€ í™•ì¥ì„ ì œì‹œí•œë‹¤.        
* ë³¸ ë…¼ë¬¸ì˜ ì¥ì ì€ í›ˆë ¨ ì¤‘ **ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ì˜ subsamplinì´ ìƒë‹¹í•œ ì†ë„ í–¥ìƒ**(ì•½ 2ë°° - 10ë°°)ì„ ê°€ì ¸ì˜¤ê³ ,     
**ë¹ˆë„ê°€ ë‚®ì€ ë‹¨ì–´ í‘œí˜„ì˜ ì •í™•ë„ë¥¼ í–¥ìƒ**ì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.             
* ë³¸ ë…¼ë¬¸ì€ ì´ì „ ì—°êµ¬ì—ì„œ ì‚¬ìš©ëœ more complex hierarchical softmaxì— ë¹„í•´ ë¹ˆë²ˆí•œ ë‹¨ì–´ì— ëŒ€í•´ ë” ë¹ ë¥¸ í›ˆë ¨ê³¼ ë” ë‚˜ì€ ë²¡í„° í‘œí˜„ì„ ì´ˆë˜í•˜ëŠ” ìŠ¤í‚µê·¸ë¨ ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•œ **ë‹¨ìˆœí™”ëœ Noise Contrastive Estimation** (NCE)ì„ ì œì‹œí•œë‹¤.

<details>
<summary>ğŸ“œ Noise Contrastive Estimation (NCE) ë€? </summary>
<div markdown="1">
   
CBOWì™€ Skip-Gram ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë¹„ìš© ê³„ì‚° ì•Œê³ ë¦¬ì¦˜ì„ ì¹­í•œë‹¤.      
ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ softMax í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ **ìƒ˜í”Œë§ìœ¼ë¡œ ì¶”ì¶œí•œ ì¼ë¶€ì— ëŒ€í•´ì„œë§Œ ì ìš©**í•˜ëŠ” ë°©ë²•ì„ ë§í•œë‹¤.     
kê°œì˜ ëŒ€ë¹„ë˜ëŠ”(contrastive) ë‹¨ì–´ë“¤ì„ noise distributionì—ì„œ êµ¬í•´ì„œ (ëª¬í…Œì¹´ë¥¼ë¡œ) í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒì´ ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.     
â¡ Hierarchical SoftMaxì™€ [Negative Sampling](https://yerimoh.github.io/DL15/#2%EF%B8%8F%E2%83%A3-%EB%84%A4%EA%B1%B0%ED%8B%B0%EB%B8%8C-%EC%83%98%ED%94%8C%EB%A7%81%EC%9D%B4%EB%9E%80-%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98-%EB%8F%84%EC%9E%85) ë“±ì˜ ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ ë‹¨ì–´ ê°¯ìˆ˜ê°€ ë§ì„ ë•Œ ì‚¬ìš©í•˜ê³ , NCEë¥¼ ì‚¬ìš©í•˜ë©´ ë¬¸ì œë¥¼ (ì‹¤ì œ ë¶„í¬ì—ì„œ ì–»ì€ ìƒ˜í”Œ)ê³¼ (ì¸ê³µì ìœ¼ë¡œ ë§Œë“  ì¡ìŒ ë¶„í¬ì—ì„œ ì–»ì€ ìƒ˜í”Œ)ì„ êµ¬ë³„í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ë°”ê¿€ ìˆ˜ ìˆê²Œ ëœë‹¤.

 Negative Samplingì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª©ì  í•¨ìˆ˜ëŠ” ê²°ê³¼ê°’ì´ ìµœëŒ€í™”ë  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ êµ¬ì„±í•œë‹¤. í˜„ì¬(ëª©í‘œ, target, positive) ë‹¨ì–´ì—ëŠ” ë†’ì€ í™•ë¥ ì„ ë¶€ì—¬í•˜ê³ , ë‚˜ë¨¸ì§€ ë‹¨ì–´(negative, noise)ì—ëŠ” ë‚®ì€ í™•ë¥ ì„ ë¶€ì—¬í•´ì„œ ê°€ì¥ í° ê°’ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê³µì‹ì„ ì‚¬ìš©í•œë‹¤. 
  
   
<details>
<summary>ğŸ“œ Hierarchical SoftMax ë€? </summary>
<div markdown="1">
   
CBOWì™€ Skip-Gram ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ SoftMax ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ì„œ ê³„ì‚°ì„ ì§„í–‰í•˜ëŠ”ë°, ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•´ ê³„ì‚°ì„ í•˜ê³  normalizationì„ ì§„í–‰í•´ì•¼ í•˜ëŠ”ë°, ì´ê²ƒì€ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ë°–ì— ì—†ë‹¤.    
ê³„ì‚°ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ìœ¼ë¡œ Hierarchical SoftMaxì™€ Negative Sampling ì•Œê³ ë¦¬ì¦˜ì´ ìˆë‹¤.    
Hierarchical SoftMax ì•Œê³ ë¦¬ì¦˜ì€ ê³„ì‚°ëŸ‰ì´ ë§ì€ SoftMax í•¨ìˆ˜ë¥¼ ë¹ ë¥´ê²Œ ê³„ì‚°ê°€ëŠ¥í•œ multinomial distribution í•¨ìˆ˜ë¡œ ëŒ€ì²´í•œë‹¤. íŠ¸ë¦¬ ìë£Œêµ¬ì¡°ì—ëŠ” ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë…¸ë“œê°€ ìˆê³ , ì²˜ìŒ ë…¸ë“œë¥¼ ë£¨íŠ¸(root), ë§ˆì§€ë§‰ ë…¸ë“œë¥¼ ë¦¬í”„(leaf) ë˜ëŠ” ë‹¨ë§(terminal)ì´ë¼ê³  ë¶€ë¥¸ë‹¤. multinomial distribution í•¨ìˆ˜ëŠ” ë£¨íŠ¸ì—ì„œ ë¦¬í”„ê¹Œì§€ ê°€ëŠ” ê²½ë¡œë¥¼ í™•ë¥ ê³¼ ì—°ë™ì‹œì¼œì„œ ê³„ì‚° ì‹œê°„ì„ ë‹¨ì¶•ì‹œí‚¨ë‹¤.     
Word2Vec ë…¼ë¬¸ì—ì„œëŠ” ì‚¬ìš© ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ì— ëŒ€í•´ ì§§ì€ ê²½ë¡œë¥¼ ë¶€ì—¬í•˜ëŠ” Binary Huffman Treeë¥¼ ì‚¬ìš©í•œë‹¤.  Huffman TreeëŠ” ê²½ë¡œì˜ ê¸¸ì´ê°€ ì¼ì •í•œ full treeì˜ ì„±ì§ˆì„ ê°–ê³  ìˆê¸° ë•Œë¬¸ì— ì„±ëŠ¥ í–¥ìƒì—ëŠ” ë”ìš± ì´ìƒì ì´ê²Œ ëœë‹¤.
  
</div>
</details>     
   
   
</div>
</details>  

Word representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words. For example, â€œBoston Globeâ€ is a newspaper, and so it is not a
natural combination of the meanings of â€œBostonâ€ and â€œGlobeâ€. Therefore, using vectors to represent the whole phrases makes the Skip-gram model considerably more expressive. Other techniques
that aim to represent meaning of sentences by composing the word vectors, such as the recursive
autoencoders [15], would also benefit from using phrase vectors instead of the word vectors.

ë‹¨ì–´ í‘œí˜„ì€ ê°œë³„ ë‹¨ì–´ì˜ êµ¬ì„±ì´ ì•„ë‹Œ ê´€ìš©êµ¬ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ì œí•œëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "ë³´ìŠ¤í„´ ê¸€ë¡œë¸Œ"ëŠ” ì‹ ë¬¸ì´ê¸° ë•Œë¬¸ì— "ë³´ìŠ¤í„´"ê³¼ "ê¸€ë¡œë¸Œ"ì˜ ì˜ë¯¸ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ê²°í•©ëœ ê²ƒì€ ì•„ë‹ˆë‹¤. ë”°ë¼ì„œ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ êµ¬ë¬¸ì„ í‘œí˜„í•˜ë©´ ìŠ¤í‚µê·¸ë¨ ëª¨ë¸ì´ í›¨ì”¬ ë” í‘œí˜„ë ¥ì´ ë›°ì–´ë‚˜ë‹¤. ì¬ê·€ì  ìë™ ì¸ì½”ë”[15]ì™€ ê°™ì´ ë‹¨ì–´ ë²¡í„°ë¥¼ êµ¬ì„±í•˜ì—¬ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ í‘œí˜„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ëŠ” ë‹¤ë¥¸ ê¸°ìˆ ë„ ë‹¨ì–´ ë²¡í„° ëŒ€ì‹  êµ¬ë¬¸ ë²¡í„°ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì´ìµì„ ì–»ì„ ìˆ˜ ìˆë‹¤.


https://pythonkim.tistory.com/92

The extension from word based to phrase based models is relatively simple. First we identify a large
number of phrases using a data-driven approach, and then we treat the phrases as individual tokens
during the training. To evaluate the quality of the phrase vectors, we developed a test set of analogical reasoning tasks that contains both words and phrases. A typical analogy pair from our test set is
â€œMontrealâ€:â€œMontreal Canadiensâ€::â€œTorontoâ€:â€œToronto Maple Leafsâ€. It is considered to have been
answered correctly if the nearest representation to vec(â€œMontreal Canadiensâ€) - vec(â€œMontrealâ€) +
vec(â€œTorontoâ€) is vec(â€œToronto Maple Leafsâ€).

ë‹¨ì–´ ê¸°ë°˜ ëª¨ë¸ì—ì„œ êµ¬ë¬¸ ê¸°ë°˜ ëª¨ë¸ë¡œì˜ í™•ì¥ì€ ë¹„êµì  ê°„ë‹¨í•˜ë‹¤. ë¨¼ì € ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë§ì€ ìˆ˜ì˜ êµ¬ë¬¸ì„ ì‹ë³„í•œ ë‹¤ìŒ, í›ˆë ¨ ì¤‘ì— êµ¬ë¬¸ì„ ê°œë³„ í† í°ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤. êµ¬ë¬¸ ë²¡í„°ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê¸° ìœ„í•´ ë‹¨ì–´ì™€ êµ¬ë¬¸ì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” ì¼ë ¨ì˜ ì•„ë‚ ë¡œê·¸ ì¶”ë¡  ì‘ì—…ì„ ê°œë°œí–ˆë‹¤. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì¼ë°˜ì ì¸ ìœ ì¶” ìŒì€ "ëª¬íŠ¸ë¦¬ì˜¬"ì…ë‹ˆë‹¤."ëª¬íŠ¸ë¦¬ì˜¬ ìºë‚˜ë‹¤ì¸":"í† ë¡ í† ":"í† ë¡ í†  ë‹¨í’ì" vec ("ëª¬íŠ¸ë¦¬ì˜¬ ìºë‚˜ë‹¤ì¸") - vec ("ëª¬íŠ¸ë¦¬ì˜¬") + vec ("í† ë¡ í† ")ì— ê°€ì¥ ê°€ê¹Œìš´ í‘œí˜„ì´ vec ("í† ë¡ í†  ë©”ì´í”Œ ë¦¬í”„ìŠ¤")ì´ë©´ ì •ë‹µìœ¼ë¡œ ê°„ì£¼ëœë‹¤.

Finally, we describe another interesting property of the Skip-gram model. We found that simple
vector addition can often produce meaningful results. For example, vec(â€œRussiaâ€) + vec(â€œriverâ€) is
close to vec(â€œVolga Riverâ€), and vec(â€œGermanyâ€) + vec(â€œcapitalâ€) is close to vec(â€œBerlinâ€). This
compositionality suggests that a non-obvious degree of language understanding can be obtained by
using basic mathematical operations on the word vector representations.

ë§ˆì§€ë§‰ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ìŠ¤í‚µê·¸ë¨ ëª¨ë¸ì˜ ë˜ ë‹¤ë¥¸ í¥ë¯¸ë¡œìš´ íŠ¹ì„±ì„ ì„¤ëª…í•œë‹¤. ìš°ë¦¬ëŠ” ê°„ë‹¨í•œ ë²¡í„° ì¶”ê°€ê°€ ì¢…ì¢… ì˜ë¯¸ ìˆëŠ” ê²°ê³¼ë¥¼ ì‚°ì¶œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, vec("ëŸ¬ì‹œì•„") + vec("ê°•")ì€ vec("ë³¼ê°€ ê°•")ì— ê°€ê¹ê³ , vec("ë…ì¼") + vec("ìˆ˜ë„")ì€ vec("ë² ë¥¼ë¦°")ì— ê°€ê¹ë‹¤. ì´ëŸ¬í•œ êµ¬ì„±ì„±ì€ ë‹¨ì–´ ë²¡í„° í‘œí˜„ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ìˆ˜í•™ì  ì—°ì‚°ì„ ì‚¬ìš©í•˜ì—¬ ëª…í™•í•˜ì§€ ì•Šì€ ìˆ˜ì¤€ì˜ ì–¸ì–´ ì´í•´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤.
