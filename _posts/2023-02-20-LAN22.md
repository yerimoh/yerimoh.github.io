---
title: "SCIBERT: A Pretrained Language Model for Scientific Text 정리"
date:   2023-02-20
excerpt: "SCIBERT: A Pretrained Language Model for Scientific Text"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


**[원 논문]**     
[SCIBERT: A Pretrained Language Model for Scientific Text](https://arxiv.org/pdf/1903.10676.pdf)

-----

# Abstract

<span style="background-color:#F5F5F5">**[문제]**</span>         
과학 영역에서 NLP 작업에 대한 large-scale annotated data를 얻는 것은 어렵고 비용이 많이 든다.     

<span style="background-color:#F5F5F5">**[해결: SCIBERT]**</span>         
* 우리는 high-quality의  large-scale labeled scientific data의 부족을 해결하기 위해,     
 [BERT](https://yerimoh.github.io/Lan2/)를 기반으로 pre-train된 SCIBERT를 만들었    
 * <span style="background-color:#FFE6E6">SCIBERT는 **scientific publications**의  **large multi-domain corpus에서 unsupervised pretraining 을 활용**하여 downstream scientific NLP tasks의 성능을 향상</span>시킨다.     
 * 이는 BERT에 비해 더 좋은 성능을 낸다.    
 * 코드 및 사전 교육된 모델은 [링크](https://github.com/allenai/scibert/)에서 확인 가능하다.     



----
----


# 1 Introduction
scientific publications의 양이 매우 많아, NLP는 이러한 문서의 대규모 지식 추출 및 기계 판독을 위한 필수 도구가 됨.   

<span style="background-color:#F5F5F5">**[문제]**</span>         
* NLP의 모델들을 훈련하려면 많은 양의 labeled data가 필요한 경우가 많다.     
➡ 이러한 데이터를 얻는 것은 어렵고 비용이 많이 듦     
* [ELMo](https://wikidocs.net/33930), [GPT](https://yerimoh.github.io/Lan4/) 및 [BERT](https://yerimoh.github.io/Lan2/) 통해 알 수 있듯이,  large corpora에서 언어 모델의 **unsupervised pretraining of language models은 많은 NLP 작업에서 성능을 크게 향상**시킨다.     
이러한 모델은 각 토큰에 대한 **contextualized embeddings을 반환**한다.(즉, 문맥 파악이 가능한 토큰을 준다.)     
➡ 즉, 이러한 <span style="background-color:#fff5b1">**unsupervised pretraining을 활용**하는 것은 과학적 NLP에서와 같이 **task-specific annotations을 얻기 어려운 경우에 특히 중요**</span>해졌다.      
* ⚠️ 하지만 BERT와 ELMo는 task-specific과 같은 domain이 아닌 **general domain corpora**(such as news articles and Wikipedia.)**로 훈련을 받은 모델**이다. (science와 같은 task-specific corpora로 훈련을 하지 않음)     


<span style="background-color:#F5F5F5">**[본 논문의 해결]**</span>         
이 작업에서 우리는 다음과 같은 기여를 한다:
* **(i)** 우리는 <span style="background-color:#FFE6E6">**scientific domain**에서 NLP tasks의 범위에서 성능을 향상</span>시키는 것으로 입증된 새로운 리소스인 SCIBERT를 출시한다.    
➡ SCIBERT는 BERT를 기반으로 한 pretrained language model이지만 **large corpus of scientific text**에 대해 훈련되었다.         
* **(ii)** 우리는 frozen embeddings 위의  <span style="background-color:#FFE6E6">**fine tuning VS task-specific architecture**의 성능과  in-domain vocabulary의 영향을 조사</span>하기 위해 광범위한 실험을 수행한다       
* **(iii)** 우리는 scientific domain의 일련의 작업에 대해 SCIBERT를 evaluate하고, SOTA 결과를 달성한다.        




---
----


# 2 Methods
## Background 
* [BERT](https://yerimoh.github.io/Lan2/)는 [multilayer bidirectional Transformer](https://yerimoh.github.io/Lan2/#1-pre-training-bert)를 기반으로 한다.      
* 기존의 왼쪽에서 오른쪽으로 언어 모델링 하는 대신,     
BERT는 [무작위로 마스킹된 토큰을 예측(Masked Language Model, MLM)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)하고,    
[두 문장이 서로 따르는지 여부를 예측(Next sentence prediction, NSP)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)하는 두 가지 작업에 대해 훈련된다.    

SCIBERT는 BERT와 **동일한 아키텍처**를 따르지만 대신 **scientific text에 대해 pretrained**을 받는다.     

---

## Vocabulary
BERT는 입력 텍스트의 unsupervised tokenization를 위해 [WordPiece](https://yerimoh.github.io/Lan2/#token-embeddings)를 사용한다.      
vocabulary는 가장 자주 사용되는 words or subword units를 포함하도록 구성된다.    

우리는 BERT와 함께 발표된 원래의 어휘를 **BASEVOCAB**라고 부른다.      


우리는 [SentencePiece 라이브러리](https://github.com/google/sentencepiece)를 사용하여,    
**scientific corpus에 새로운 WordPiece 어휘인 SCIVOCAB**를 구성한다.     
* 본 논문은 대문자와 소문자가 없는 어휘를 모두 생성하고 BASE VOCAB의 크기에 맞게 어휘 크기를 30K로 설정한다.   
* BASEVOCAB와 SCIVOCAB 사이의 결과 일반 VOCAB의 토큰과 SCIVOACB의 같은 토큰은 42%로 **scientific and general domain texts에서 자주 사용되는 단어의 상당한 차이**를 보여준다    


<details>
<summary>📜 SentencePiece란? </summary>
<div markdown="1">

논문 : https://arxiv.org/pdf/1808.06226.pdf
구글이 BPE 알고리즘과 Unigram Language Model Tokenizer를 구현한 것.   

SentencePiece는 **pre-tokenization을 필요로 하지 않는 tokenizer**의 하나로, 어떤 언어에도 자유롭게 적용될 수 있고 속도도 굉장히 빠르기 때문에 NLP에서 널리 사용되는 tokenizer이다. 

SetencePiece는 기존에 존재하던 unigram, BPE와 같은 tokenizer들을 모든 언어에 대해 적용이 가능하도록 generalize하고 약간의 추가적인 기능들을 더해서 구현한 것이다.    
특별히 새로운 알고리즘을 제시했다기보다는, 기존의 tokenizer들을 좀 더 사용하기 편하고 성능이 좋게 개선했다고 생각하면 되겠다. 
 
 
**[SentencePiece의 특징]**         
* **Pre-tokenization을 필요로 하지 않는다.**    
 기존의 tokenizer들은 영어를 기준으로 학습되었다면 영어가 띄어쓰기를 기반으로 해서 단어들이 대부분 분리되기 때문에, **띄어쓰기가 없는 일본어나 중국어와 같은 언어들에는 적용될 수 없다**는 한계점을 가지고 있었다.     
 하지만 SentencePiece는 **띄어쓰기를 다른 알파벳 혹은 글자처럼 하나의 character로 취급**한다.      
 ➡ 그렇기 때문에 모든 종류의 언어에 대해 general하게 적용이 가능하고, pre-tokenization이 필요하지 않다는 장점을 가진다.    
* **Character-coverage 설정이 가능하다.**     
 영어의 경우에는 알파벳의 개수가 30개 내외로 적기 때문에 괜챃지만, 중국어나 일본어와 같은 문자들의 경우에는 굉장히 많은 종류의 한자를 포함하고 있고, 이에 따라 **엄청나게 많은 개수의 character들을 다 다루어야 해서 tokenizer를 학습하는 데에 시간이 오래 걸린다**는 한계점을 가진다.     
하지만 SentencePiece는 모든 character를 다 고려하는 것이 아닌, **빈도에 따라 상위 99%의 character들만 고려**하는 등의 옵션을 주는 것이 가능하다.
* **Subword generalzation이 가능하다.**      
 우리가 학습한 tokenizer가 항상 완벽하다면 좋겠지만, 이것이 **overfitting되어 있을 가능성**을 배제할 수는 없다. 그렇기 때문에 SentencePiece에서는 우리가 학습한 tokenizer에서 나올 수 있는 token들만을 사용하는 것이 아니라, **가끔은 다른 token들도 샘플링**하면서 사용하여 overfitting하는 것이 가능하다.       
* **속도가 빠르다.**      
 직접 구현해보면 알겠지만, 알고리즘이 굉장히 최적화되어 있어서 같은 BPE와 unigram을 실행하더라도 굉장히 빠른 속도를 보여준다.

</div>
</details>  

---

## Corpus 
우리는 Semantic Scholar의 1.14M 논문의 무작위 샘플에 대해 SCIBERT를 훈련한다.      
이 corpus의 구성    
* 컴퓨터 과학 영역의 논문 18%           
* 광범위한 생물 의학 영역의 논문 82%       

우리는 논문의 전문을 사용한다. (요약본을 사용하지 않는다.)    

평균 논문 길이는 154개의 문장(2,769개의 토큰)으로, **BERT가 훈련된 3.3B 토큰과 유사한 corpus 크기**가 된다.      
우리는 과학적 텍스트에 최적화된 **[ScispaCy](https://github.com/allenai/SciSpaCy)**를 사용하여 문장을 분할한다.    




<details>
<summary>📜 Semantic Scholar란?  </summary>
<div markdown="1">


**적합한 논문**을 신속하게 검색하여 이용자들이 논문의 내용을 이해하는데 도움이 되고자 PDF파일로 간략한 발표 PPT형식으로 **요약한 내용을 열람할 수 있는 서비스를 제공**하고있는 것.  
 
 또한 특정한 연구분야를 연구하는 학자로서 최근 발표된 논문을 Semantic Scholar 논문 검색 엔진을 사용하게 되면 논문의 내용을 파악하는데 걸리는 시간을 단축할 수 있다


이 회사는 Allen Institute for Artificial Intelligence에서 설립한 비영리 기업이며 무료로 서비스를 제공하고 있다. **인공지능은 논문의 줄거리 내용을 파악하여 “Abstractive”기법을 통해 요약된 내용을 생성**하고 있다.
 
 
</div>
</details>  


---
---


# Experimental Setup
## 3.1 Tasks
우리는 아래의 core NLP tasks로 평가한다:        
**1.** Named Entity Recognition (NER)   
**2.** PICO Extraction (PICO)   
**3**. Text Classification (CLS)   
**4.** Relation Classification (REL)  
**5.** Dependency Parsing (DEP)   


PICO, like NER, is a sequence labeling task where
the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes
in a clinical trial paper (Kim et al., 2011). REL
is a special case of text classification where the
model predicts the type of relation expressed between two entities, which are encapsulated in the
sentence by inserted special tokens.


PICO는 NER와 마찬가지로 모델이 [Participants, Interventions, Comparisons, and Outcomes를 설명하는 범위를 추출](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-S2-S5)하는 sequence labeling task이다.    
그리고 REL은 모델이 두 엔티티 간에 표현되는 관계의 유형을 예측하는 텍스트 분류의 특별한 경우이며, 삽입된 특수 토큰에 의해 문장에 캡슐화(encapsulated)된다.   


<details>
<summary>📜 PICO란?  </summary>
<div markdown="1">
 
**PICO란?**        
과학 실험에서 아래 요인들을 뽑아내는 것    
PICO 프레임워크는 structuring clinical question 질문에 필요한 각 핵심 요소를 포착하기 때문에 임상 질문을 구성하는 데 가장 일반적으로 사용되는 모델입니다. PICO는 다음을 의미합니다.    
* Participants/Problem (P)   
* Intervention (I)   
* Comparison (C)   
* Outcome (O)   

**evidence-based medicine (EBM) 애플리케이션은 대규모 의학 문헌 데이터베이스를 분석하여 임상 질문에 답하는 데 의존**합니다.   

잘 정의되고 집중된 임상 질문을 공식화하기 위해 PICO라는 프레임워크가 널리 사용되며, 이는 참가자/문제(P), 개입(I), 비교(C) 및 결과(O)의 네 가지 구성 요소에 속하는 주어진 의학 텍스트의 문장을 식별합니다. 
 
아래 표는 질문의 유형에 따른 PICO이다.   

![image](https://user-images.githubusercontent.com/76824611/225283752-14153e36-7d6c-405f-8eeb-ca5347553eee.png)
![image](https://user-images.githubusercontent.com/76824611/225284104-08723df3-75bf-47b0-a336-133c3201b7f2.png)

 
 https://libguides.mssm.edu/ebm/ebp_pico
 
 
</div>
</details>  


<details>
<summary>📜 NER이란?  </summary>
<div markdown="1">
 
NER(Named Entity Recognition)은 말 그대로 Named Entity(이름을 가진 개체)를 Recognition(인식)하는 것을 의미하며, 개체명 인식이라고 합니다.
 
즉 개체마다 뭔지 태깅을 해주는 것다.

예를 들면 아래와 같다.   
![image](https://user-images.githubusercontent.com/76824611/225285580-0b1ac9d7-75d7-4412-b71a-fa9d60144b1c.png)

‍[출처](https://www.letr.ai/blog/tech-20210723)   
 
</div>
</details>  

<details>
<summary>📜 Automatic classification of sentences to support Evidence Based Medicine?  </summary>
<div markdown="1">
 
Automatic classification of sentences to support Evidence Based Medicine 란?

**Aim**     
Evidence Based Medicine에서 사용되는 미리 정의된 의료 범주 세트가 주어지면 **의학 초록의 문장에 이러한 레이블을 자동으로 추가하는 것**을 목표로 합니다. 
 
**방법**            
우리는 특정 **의료 범주(예: 개입 , 결과) 로 손으로 주석**을 단 1,000개의 의료 초록 모음을 구성했습니다 . 우리는 분류를 위해 CRF(Conditional Random Fields)를 사용하여 데이터의 어휘, 의미, 구조 및 순차 정보를 기반으로 다양한 기능의 사용을 조사했습니다.
 

**결과**    
모든 레이블에 대한 분류 작업의 경우, 우리 시스템은 sequential feature을 사용하여 구조화 및 구조화되지 않은 초록의 데이터 세트에 대해 각각 80.9% 및 66.9%의 micro-averaged f-scores를 달성했습니다. 핵심 문장에만 레이블을 지정하는 경우, 우리 시스템은 동일한 순차적 기능을 사용하여 구조화된 초록과 구조화되지 않은 초록에 대해 각각 89.3%와 74.0%의  f-scores를 생성했습니다. 외부 데이터 세트에 대한 결과는 더 낮았습니다(모든 레이블의 경우 f-점수 63.1%, 주요 문장의 경우 83.8%).

**결론**         
우리가 사용한 기능 중 abstract 에서 주어진 문장을 분류하는 데 가장 좋은 기능은 unigrams, section headings, and sequential information의 순차적 정보를 기반으로 했습니다. 이러한 기능으로 인해 simple bag-of-words 접근 방식에 비해 성능이 향상되었으며 이전 작업에서 사용된 feature sets다 성능이 뛰어났습니다.
 
</div>
</details>  





----




## 3.2 Datasets


**[Table 1]**     
![image](https://user-images.githubusercontent.com/76824611/225572942-78914c0c-d6e1-4437-a1cb-b4bd6a95cc3e.png)
* 모든 tasks 및  datasets에서 모든 BERT 변형의 성능을 테스트한다.        
* 굵은 글씨는 SOTA 결과를 나타낸다. (95% bootstrap 신뢰 구간 내에서 차이가 있을 경우 여러 개의 결과가 굵은 글씨로 표시됨).          
* 과거 작업과 함께, 우리는 아래와 같은 요소를 보고한다.        
    * NER (span-level)에 대한 macro F1 scores   
    * REL 및 CLS(문장 레벨, 문장의 시작 토큰)에 대한 macro F1 scores    
    * PICO(토큰 레벨)에 대한 macro F1 scores     
    * ChemPro에 대한 micro F1 scores       
    * DEP의 경우, 우리는 LAS에 맞게 조정된 hyperparameters를 사용하여 동일한 모델에 대한 레이블링된(LAS) 및 레이블링되지 않은(UAS) attachment scores를 보고한다.     
* 모든 결과는 서로 다른 random seeds를 사용한 여러 runs의 평균이다.         




<details>
<summary>📜 F1 scores  </summary>
<div markdown="1">
 
 
![image](https://user-images.githubusercontent.com/76824611/225508975-89b576ed-3457-433c-a7c6-7e91167e9e1f.png)

![image](https://user-images.githubusercontent.com/76824611/225509055-2a261c2e-bfb8-4fcf-83d1-04bd79b6a4ee.png)

각 범주별 Precision, Recall, F1 Score
![image](https://user-images.githubusercontent.com/76824611/225509111-83dd797e-039c-460c-b734-84374d44ada6.png)

Macro Average F1 Score
![image](https://user-images.githubusercontent.com/76824611/225510862-71947c9c-2e64-451d-b627-ae50d17d9e51.png)

Micro Average F1 Score = accuracy = micro-precision = micro-recall
![image](https://user-images.githubusercontent.com/76824611/225510902-105ad68e-a594-424c-b7a4-2f6c4d86fe2e.png)

</div>
</details>  




위 표 외의 데이터를 설명하자면,    
* [EBM-NLP](https://aclanthology.org/P18-1019/)는 임상 시험 요약에서 PICO 범위에 주석을 달았다.     
* [SCIERC](https://aclanthology.org/D18-1360.pdf)는 relations from computer science abstracts의 entities와  relations에 주석을 달았다.     
* [ACL-ARC](https://aclanthology.org/Q18-1028.pdf)와 [SciCite](https://aclanthology.org/N19-1361.pdf)는 다른 논문을 인용하는 과학 논문의 문장에  intent labels(예: Comparison, Extension 등)을 할당한다.     
* 논문 분야 dataset는 [Microsoft Academic Graph](https://www.microsoft.com/en-us/research/publication/overview-microsoft-academic-service-mas-applications/)에서 구축되었으며 논문 제목을 7개 연구 분야 중 하나에 매핑한다.     
각 연구 분야( geography, politics, economics, business, sociology, medicine, and psychology)에는 약 12K의 training example이 있다.    


---- 


## 3.3 Pretrained BERT Variants
### BERT-Base 
원래 [BERT 코드](https://github.com/google-research/bert)와 함께 릴리스된 [BERT-Base](https://yerimoh.github.io/Lan2/)에 대해 pretrained weights를 사용한다.      

어휘는 BASE VOCAB이다.   

우리는 이 모델의 cased 버전과 uncased 버전을 모두 평가한다.




### SCIBERT 
우리는 **BERT-Base와 동일한 구성과 크기로 말뭉치에서 SCIBERT를 훈련**하기 위해 원래 BERT 코드를 사용한다.    

우리는 아래 4가지 different SCIBERT case에 대해 train한다.(i와 ii에서 하나씩 조합)      
**(i)** cased or uncased      
**(ii)** BASEVOCAB or SCIVOCAB         

**BASEVOCAB**을 사용하는 두 모델은 해당 **BERT-Base 모델에서 finetuning**된다.       
**SCIVOCAB**를 사용하는 두 모델은 **처음(scratch)부터 train**된다.      

<span style="background-color:#F5F5F5">**[train 방법]**</span>              
* **1)** original BERT 코드에 따라 최대 문장 길이를 128개 토큰으로 설정함     
long sentences에 대한 BERT Pretraining은 느릴 수 있기 때문      
* **2)** training loss가 줄어들지 않을 때까지 모델을 train시킨다.  
* **3)** 그런 다음 최대 512개의 토큰까지 문장 길이를 허용하는 모델을 계속 훈련한다.   

<span style="background-color:#F5F5F5">**[train time]**</span>              
* 우리는 8개의 코어가 있는 single TPU v3를 사용한다.    
* corpus에서 SCIVOCAB 모델을 처음부터 훈련하는 데는 1주일 하고 5일이 걸렸다.   
 최대 길이 128을 사용하면 5일, 최대 길이 512를 사용하면 2일이 걸린다.      
* BASE VOCAB 모델은 처음부터 훈련을 받지 않기 때문에 훈련 기간이 2일 더 적다.    

모든 사전 훈련된 BERT 모델은 [PyTorch transformer 라이브러리](https://github.com/huggingface/transformers)를 사용하여 PyTorch와 호환되도록 변환된다.    

우리의 모든 모델은 AllenNLP를 사용하여 PyTorch에서 구현된다


### Casing 
우리는 NER에 cased models을 사용하고,    
다른 모든 작업에 cased models을 사용하는 데 [BERT](https://yerimoh.github.io/Lan2/)를 따른다.          
우리는 또한 parsing을 위해 cased models을 사용한다.    

일부 light experimentation은 uncased model이 cased model보다 (때로는 NER에서도) 약간 더 나은 성능을 보인다는 것을 보여주었다.    




<details>
<summary>📜 uncased vs cased </summary>
<div markdown="1">

BERT uncased와 BERT cased는 WordPiece 토큰화 단계에서 텍스트의 **대소문자 사용** 여부, **악센트 마커의 존재 여부**에서 다릅니다.   
 
```
# BERT uncased
OpenGenus -> opengenus
OpènGènus -> opengenus 

# BERT cased
OpenGenus
OpènGènus
```

</div>
</details>   
 



---



## 3.4 Finetuning BERT

<span style="background-color:#F5F5F5">**[base BERT와 같은 점]**</span>           
* 우리는 오리지널 BERT에서 사용된 same architecture, optimization, and hyperparameter을 따른다.   
* **텍스트 분류**(즉, CLS및 REL)의 경우 ```[CLS] 토큰```에 대한 최종 BERT 벡터를 **linear classification layer**에 공급한다.    
* **sequence labeling**(즉, NER 및 PICO)의 경우 각 토큰에 대한 최종 BERT 벡터를 **softmax output이 있는  linear classification layer**으로 공급한다.        

<span style="background-color:#F5F5F5">**[base BERT와른다른 점]**</span>           
* additional conditional random field를 사용하는 것에 약간 차이가 있는데, 이는 well-formed entities를 보장함으로써 평가를 더 쉽게 만들었다.     
* DEP의 경우, stacked BiLSTMs 대신, BERT size 100의  dependency tag, arc embeddings과 BERT 벡터에 대한 이진 행렬(biaffine matrix) attention를 가진 [모델](https://arxiv.org/abs/1611.01734)을 사용한다.   




---


## 3.5 Frozen BERT Embeddings
또한 동결된 BERT embeddings 위에서 간단한 task별 모델을 교육하여 ELMo와 같은 pretrained contextualized word embeddings으로 BERT의 사용을 탐구한다      

* text classification를 위해, BERT 벡터의 각 문장을 크기 200의 2층 BiLSTM에 공급한 후,    
연결된 첫 번째와 마지막 BiLSTM 벡터에 a multilayer perceptron (with hidden size 200)을 적용     
* sequence labeling의 경우 동일한 BiLSTM 레이어를 사용하고  conditional random field를 사용하여 well-formed predictions을 보장한다.      
* DEP의 경우, 우리는 크기가 100인 tag and arc embeddings과 다른 작업과 동일한 BiLSTM 설정을 모델을 사용한다.      
* BiLSTM의 깊이나 크기 변경은 결과에 큰 영향을 미치지는 않았다(Reimers and Gurevych, 2017).             
* Adam을 사용하여 cross entropy loss을 최적화하지만 BERT 가중치를 동결하고 0.5의 dropout을 적용한다.     
* 우리는 배치 크기 32와 학습률 0.001을 사용하여 development se(10명의 환자)에서 early stopping를 사용하여 train한다.    
* 우리는 광범위한 hyperparameter search을 수행하지 않았지만,   
 최적의 하이퍼 파라미터는 작업에 따라 달라질 것이다.     



----
----


# 4 Results
Table 1은 실험 결과를 요약한 것이다.      
우리는 SCIBERT가 scientific tasks에서 BERT-Base를 능가한다는 것을 관찰한다. 
(+2.11 F1 with finetuning and +2.43 F1 without).      

우리는 또한 SCIBERT를 사용하여 이러한 tasks 중 많은 tasks에서 새로운 SOTA 결과를 얻는다.      
![image](https://user-images.githubusercontent.com/76824611/225640313-d95a9b8f-bc70-45a2-80a9-967c79b0800d.png)


---

## 4.1 Biomedical Domain
우리는 SCIBERT가 biomedical tasks에서 BERTBase를 능가한다는 것을 관찰한다 (+1.92 F1 with finetuning and +3.59 F1 without).    
또한, SCIBERT는 BC5CDR과 [ChemProt](https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf) 및 [EBMNLP](https://aclanthology.org/P18-1019.pdf)에서 새로운 SOTA 결과를 달성한다. 



SCIBERT는 3개의 데이터 세트에서 SOTA보다 성능이 약간 떨어진다.      
* JNLPBA에 대한 SOTA 모델은 JNLPBA뿐만 아니라 여러 NER 데이터 세트에 대해 훈련된 BiLSTM-CRF 앙상블이다.       
* NCBI-disease의 SOTA 모델은 BIOBERT로, 생물의학 논문의 18B 토큰에 finetuned BERTBase이다.      
* GENIA에 대한 SOTA 결과는 Nguyen과 Verspoor(2019)에 있으며, 이는 우리가 사용하지 않는 음성 부분(POS) 기능과 함께 Dozat과 Manning(2017)의 모델을 사용한다.


표 2에서, 우리는 SCIBERT 결과를 에 포함된 데이터 세트의 하위 집합에 대한 보고된 BIOBERT 결과와 비교한다(Lee et al., 2019). 흥미롭게도, SCIBERT는 BC5CDR 및 ChemProt에서 BIBERT 결과를 능가하며, 상당히 작은 생물 의학 말뭉치에서 훈련되었음에도 불구하고 JNLPBA에서 유사한 성능을 발휘한다.





**[Table 2]** Comparing SCIBERT with the reported   
![image](https://user-images.githubusercontent.com/76824611/225625651-79fc0bfa-08f5-4b3f-afe0-9016e1444b63.png)


----

## 4.2 Computer Science Domain
* SCIBERT가 Computer Science 작업에서 BERTBaase보다 성능이 좋다.    
* SCIBERT는 ACLARC(Cohan et al., 2019)와 SCIERC의 NER 부분에서 새로운 SOTA 결과를 달성한다(Luan et al., 2018).    
* SIERC의 relations의 경우, 우리의 결과는 루안 외 연구진(2018)의 결과와 비교할 수 없다. 왜냐하면 우리는 given gold entitie가 있는 관계 분류를 수행하는 반면 그들은 oint entity and relation extraction을 수행하기 때문이다


---


## 4.3 Multiple Domains
* SCIBERT가 다중 도메인 작업에서 BERTBase(+0.49F1)를 능가한다는 것을 관찰한다.     
* 또한, SCIBERT는 Sci Cite에서 SOTA를 능가한다(Cohan et al., 2019). 

---
----




#  5 Discussion

## 5.1 Effect of Finetuning
* frozen embeddings 위의 task별 아키텍처(+3.25 F1 with SCIBERT and +3.58 with BERT-Base, on average)보다는 **BERT finetuning을 통해 개선**된 결과를 관찰한다.     
* 각 과학 영역에 대해,    
     * computer science (+5.59 F1 with SCIBERT and +3.17 F1 with BERT-Base      
     * biomedical tasks (+2.94 F1 with SCIBERT and +4.61 F1 with BERT-Base)에 대한 finetuning의 가장 큰 영향을 관찰하고,       
     * multidomain tasks (+0.7 F1 with SCIBERT and +1.14F1 with BERT-Base)에 대한 가장 작은 영향을 관찰한다.     
* BC5CDR 및 SciCite를 제외한 모든 데이터 세트에서 finetuning이 적용된 BERT-Base는 동결된 SCIBERT 임베딩을 사용하는 모델을 능가한다(또는 유사한 성능)     


---

## 5.2 Effect of SCIVOCAB
우리는 BASE VOCAB을 사용하여 SCIBERT에 대한 finetuning 실험을 반복함으로써 **도메인 내 과학 어휘의 중요성**을 평가한다.     
우리는 SCIBERT BASE VOCAB에 대한 최적의 hyperparameters가 SCIBERT-SCIVOCAB의 초 매개 변수와 종종 일치한다는 것을 발견한다.     


데이터 세트에 걸쳐 평균화된 SCIVOCAB를 사용할 때 +0.60 F1을 관찰한다. 각 과학 영역에 대해 생물 의학 작업의 경우 +0.76 F1, 컴퓨터 과학 작업의 경우 +0.61 F1, 다중 도메인 작업의 경우 +0.11 F1을 관찰한다.

disjoint vocabularies (Section 2)와 BERT-Base(Section 4)에 대한 개선의 크기를 고려할 때, 우리는 **도메인 내 어휘가 도움**이 되지만, **SCIBERT가 과학적 scientific corpus pretraining으로부터 가장 많은 이점**을 얻을 수 있다고 의심한다.


---
----

# Related Work
BERT의 도메인 적응에 대한 최근 연구에는,   
* **BIOBERT**(Lee et al., 2019): BIOBERT는 PubMed 요약 및 PMC 전체 텍스트 기사에 대해 train된 것이다.   
* **CLINICBERT**: MIMIC-III 데이터베이스의 임상 텍스트에 대해 교육을 받는다(Johnson et al., 2016).    
* 대조적으로, **SCIBERT**는 Semantic Scholar corpus(Ammar et al., 2018)의 <span style="background-color:#FFE6E6">**114만 개의 생물 의학 및 컴퓨터 과학 논문의 전문에 대해 train**</span>된다. 또한, SCIBERT는 <span style="background-color:#FFE6E6">**도메인 내 어휘(SCIVOCAB)를 사용**</span>하는 반면, 위에 언급된 다른 모델은 원래 BERT 어휘(BASEVOCAB)를 사용한다.



---
--


# 7 Conclusion and Future Work
<span style="background-color:#F5F5F5">**[Conclusion]**</span>         
* 우리는 BERT를 기반으로 한 **scientific 텍스트에 대한 pretrained 언어 모델인 SCIBERT를 출시**했다.     
* 우리는 scientific 영역의 일련의 작업과 데이터 세트에 대해 **SCIBERT를 평가**했다.    
* SCIBERT는 BERT-Base를 크게 능가하고 생물 의학 작업에 대한 일부 보고된 BIOBERT(Lee et al., 2019) 결과와 비교* 해도 이러한 작업 중 몇 가지에서 새로운 SOTA 결과를 달성한다.     

<span style="background-color:#F5F5F5">**[Future Work]**</span>         
* 향후 작업을 위해 BERT-Large와 유사한 SCIBERT 버전을 출시하고, 각 도메인의 다양한 비율의 논문을 실험할 예정이다.     
* 이러한 언어 모델은 교육 비용이 많이 들기 때문에 여러 도메인에 걸쳐 유용한 단일 리소스를 구축하는 것을 목표로 한다.













