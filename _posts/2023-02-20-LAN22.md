---
title: "SCIBERT: A Pretrained Language Model for Scientific Text ì •ë¦¬"
date:   2023-02-20
excerpt: "SCIBERT: A Pretrained Language Model for Scientific Text"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# Abstract

<span style="background-color:#F5F5F5">**[ë¬¸ì œ]**</span>         
ê³¼í•™ ì˜ì—­ì—ì„œ NLP ì‘ì—…ì— ëŒ€í•œ large-scale annotated dataë¥¼ ì–»ëŠ” ê²ƒì€ ì–´ë µê³  ë¹„ìš©ì´ ë§ì´ ë“ ë‹¤.     

<span style="background-color:#F5F5F5">**[í•´ê²°: SCIBERT]**</span>         
* ìš°ë¦¬ëŠ” high-qualityì˜  large-scale labeled scientific dataì˜ ë¶€ì¡±ì„ í•´ê²°í•˜ê¸° ìœ„í•´,     
 [BERT](https://yerimoh.github.io/Lan2/)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ pre-trainëœ SCIBERTë¥¼ ë§Œë“¤ì—ˆ    
 * <span style="background-color:#FFE6E6">SCIBERTëŠ” **scientific publications**ì˜  **large multi-domain corpusì—ì„œ unsupervised pretraining ì„ í™œìš©**í•˜ì—¬ downstream scientific NLP tasksì˜ ì„±ëŠ¥ì„ í–¥ìƒ</span>ì‹œí‚¨ë‹¤.     
 * ì´ëŠ” BERTì— ë¹„í•´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤.    
 * ì½”ë“œ ë° ì‚¬ì „ êµìœ¡ëœ ëª¨ë¸ì€ [ë§í¬](https://github.com/allenai/scibert/)ì—ì„œ í™•ì¸ ê°€ëŠ¥í•˜ë‹¤.     



----
----


# 1 Introduction
scientific publicationsì˜ ì–‘ì´ ë§¤ìš° ë§ì•„, NLPëŠ” ì´ëŸ¬í•œ ë¬¸ì„œì˜ ëŒ€ê·œëª¨ ì§€ì‹ ì¶”ì¶œ ë° ê¸°ê³„ íŒë…ì„ ìœ„í•œ í•„ìˆ˜ ë„êµ¬ê°€ ë¨.   

<span style="background-color:#F5F5F5">**[ë¬¸ì œ]**</span>         
* NLPì˜ ëª¨ë¸ë“¤ì„ í›ˆë ¨í•˜ë ¤ë©´ ë§ì€ ì–‘ì˜ labeled dataê°€ í•„ìš”í•œ ê²½ìš°ê°€ ë§ë‹¤.     
â¡ ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒì€ ì–´ë µê³  ë¹„ìš©ì´ ë§ì´ ë“¦     
* [ELMo](https://wikidocs.net/33930), [GPT](https://yerimoh.github.io/Lan4/) ë° [BERT](https://yerimoh.github.io/Lan2/) í†µí•´ ì•Œ ìˆ˜ ìˆë“¯ì´,  large corporaì—ì„œ ì–¸ì–´ ëª¨ë¸ì˜ **unsupervised pretraining of language modelsì€ ë§ì€ NLP ì‘ì—…ì—ì„œ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒ**ì‹œí‚¨ë‹¤.     
ì´ëŸ¬í•œ ëª¨ë¸ì€ ê° í† í°ì— ëŒ€í•œ **contextualized embeddingsì„ ë°˜í™˜**í•œë‹¤.(ì¦‰, ë¬¸ë§¥ íŒŒì•…ì´ ê°€ëŠ¥í•œ í† í°ì„ ì¤€ë‹¤.)     
â¡ ì¦‰, ì´ëŸ¬í•œ <span style="background-color:#fff5b1">**unsupervised pretrainingì„ í™œìš©**í•˜ëŠ” ê²ƒì€ ê³¼í•™ì  NLPì—ì„œì™€ ê°™ì´ **task-specific annotationsì„ ì–»ê¸° ì–´ë ¤ìš´ ê²½ìš°ì— íŠ¹íˆ ì¤‘ìš”**</span>í•´ì¡Œë‹¤.      
* âš ï¸ í•˜ì§€ë§Œ BERTì™€ ELMoëŠ” task-specificê³¼ ê°™ì€ domainì´ ì•„ë‹Œ **general domain corpora**(such as news articles and Wikipedia.)**ë¡œ í›ˆë ¨ì„ ë°›ì€ ëª¨ë¸**ì´ë‹¤. (scienceì™€ ê°™ì€ task-specific corporaë¡œ í›ˆë ¨ì„ í•˜ì§€ ì•ŠìŒ)     


<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ í•´ê²°]**</span>         
ì´ ì‘ì—…ì—ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ì—¬ë¥¼ í•œë‹¤:
* **(i)** ìš°ë¦¬ëŠ” <span style="background-color:#FFE6E6">**scientific domain**ì—ì„œ NLP tasksì˜ ë²”ìœ„ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒ</span>ì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ì…ì¦ëœ ìƒˆë¡œìš´ ë¦¬ì†ŒìŠ¤ì¸ SCIBERTë¥¼ ì¶œì‹œí•œë‹¤.    
â¡ SCIBERTëŠ” BERTë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ pretrained language modelì´ì§€ë§Œ **large corpus of scientific text**ì— ëŒ€í•´ í›ˆë ¨ë˜ì—ˆë‹¤.         
* **(ii)** ìš°ë¦¬ëŠ” frozen embeddings ìœ„ì˜  <span style="background-color:#FFE6E6">**fine tuning VS task-specific architecture**ì˜ ì„±ëŠ¥ê³¼  in-domain vocabularyì˜ ì˜í–¥ì„ ì¡°ì‚¬</span>í•˜ê¸° ìœ„í•´ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•œë‹¤       
* **(iii)** ìš°ë¦¬ëŠ” scientific domainì˜ ì¼ë ¨ì˜ ì‘ì—…ì— ëŒ€í•´ SCIBERTë¥¼ evaluateí•˜ê³ , SOTA ê²°ê³¼ë¥¼ ë‹¬ì„±í•œë‹¤.        




---
----


# 2 Methods
## Background 
* [BERT](https://yerimoh.github.io/Lan2/)ëŠ” [multilayer bidirectional Transformer](https://yerimoh.github.io/Lan2/#1-pre-training-bert)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤.      
* ê¸°ì¡´ì˜ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ë§ í•˜ëŠ” ëŒ€ì‹ ,     
BERTëŠ” [ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡(Masked Language Model, MLM)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)í•˜ê³ ,    
[ë‘ ë¬¸ì¥ì´ ì„œë¡œ ë”°ë¥´ëŠ”ì§€ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡(Next sentence prediction, NSP)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)í•˜ëŠ” ë‘ ê°€ì§€ ì‘ì—…ì— ëŒ€í•´ í›ˆë ¨ëœë‹¤.    

SCIBERTëŠ” BERTì™€ **ë™ì¼í•œ ì•„í‚¤í…ì²˜**ë¥¼ ë”°ë¥´ì§€ë§Œ ëŒ€ì‹  **scientific textì— ëŒ€í•´ pretrained**ì„ ë°›ëŠ”ë‹¤.     

---

## Vocabulary
BERTëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ì˜ unsupervised tokenizationë¥¼ ìœ„í•´ [WordPiece](https://yerimoh.github.io/Lan2/#token-embeddings)ë¥¼ ì‚¬ìš©í•œë‹¤.      
vocabularyëŠ” ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” words or subword unitsë¥¼ í¬í•¨í•˜ë„ë¡ êµ¬ì„±ëœë‹¤.    

ìš°ë¦¬ëŠ” BERTì™€ í•¨ê»˜ ë°œí‘œëœ ì›ë˜ì˜ ì–´íœ˜ë¥¼ **BASEVOCAB**ë¼ê³  ë¶€ë¥¸ë‹¤.      


ìš°ë¦¬ëŠ” [SentencePiece ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/google/sentencepiece)ë¥¼ ì‚¬ìš©í•˜ì—¬,    
**scientific corpusì— ìƒˆë¡œìš´ WordPiece ì–´íœ˜ì¸ SCIVOCAB**ë¥¼ êµ¬ì„±í•œë‹¤.     
* ë³¸ ë…¼ë¬¸ì€ ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìê°€ ì—†ëŠ” ì–´íœ˜ë¥¼ ëª¨ë‘ ìƒì„±í•˜ê³  BASE VOCABì˜ í¬ê¸°ì— ë§ê²Œ ì–´íœ˜ í¬ê¸°ë¥¼ 30Kë¡œ ì„¤ì •í•œë‹¤.   
* BASEVOCABì™€ SCIVOCAB ì‚¬ì´ì˜ ê²°ê³¼ ì¼ë°˜ VOCABì˜ í† í°ê³¼ SCIVOACBì˜ ê°™ì€ í† í°ì€ 42%ë¡œ **scientific and general domain textsì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ì˜ ìƒë‹¹í•œ ì°¨ì´**ë¥¼ ë³´ì—¬ì¤€ë‹¤    


<details>
<summary>ğŸ“œ SentencePieceë€? </summary>
<div markdown="1">

ë…¼ë¬¸ : https://arxiv.org/pdf/1808.06226.pdf
êµ¬ê¸€ì´ BPE ì•Œê³ ë¦¬ì¦˜ê³¼ Unigram Language Model Tokenizerë¥¼ êµ¬í˜„í•œ ê²ƒ.   

SentencePieceëŠ” **pre-tokenizationì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ” tokenizer**ì˜ í•˜ë‚˜ë¡œ, ì–´ë–¤ ì–¸ì–´ì—ë„ ììœ ë¡­ê²Œ ì ìš©ë  ìˆ˜ ìˆê³  ì†ë„ë„ êµ‰ì¥íˆ ë¹ ë¥´ê¸° ë•Œë¬¸ì— NLPì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” tokenizerì´ë‹¤. 

SetencePieceëŠ” ê¸°ì¡´ì— ì¡´ì¬í•˜ë˜ unigram, BPEì™€ ê°™ì€ tokenizerë“¤ì„ ëª¨ë“  ì–¸ì–´ì— ëŒ€í•´ ì ìš©ì´ ê°€ëŠ¥í•˜ë„ë¡ generalizeí•˜ê³  ì•½ê°„ì˜ ì¶”ê°€ì ì¸ ê¸°ëŠ¥ë“¤ì„ ë”í•´ì„œ êµ¬í˜„í•œ ê²ƒì´ë‹¤.    
íŠ¹ë³„íˆ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ì œì‹œí–ˆë‹¤ê¸°ë³´ë‹¤ëŠ”, ê¸°ì¡´ì˜ tokenizerë“¤ì„ ì¢€ ë” ì‚¬ìš©í•˜ê¸° í¸í•˜ê³  ì„±ëŠ¥ì´ ì¢‹ê²Œ ê°œì„ í–ˆë‹¤ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤. 
 
 
**[SentencePieceì˜ íŠ¹ì§•]**         
* **Pre-tokenizationì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ”ë‹¤.**    
 ê¸°ì¡´ì˜ tokenizerë“¤ì€ ì˜ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµë˜ì—ˆë‹¤ë©´ ì˜ì–´ê°€ ë„ì–´ì“°ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ì„œ ë‹¨ì–´ë“¤ì´ ëŒ€ë¶€ë¶„ ë¶„ë¦¬ë˜ê¸° ë•Œë¬¸ì—, **ë„ì–´ì“°ê¸°ê°€ ì—†ëŠ” ì¼ë³¸ì–´ë‚˜ ì¤‘êµ­ì–´ì™€ ê°™ì€ ì–¸ì–´ë“¤ì—ëŠ” ì ìš©ë  ìˆ˜ ì—†ë‹¤**ëŠ” í•œê³„ì ì„ ê°€ì§€ê³  ìˆì—ˆë‹¤.     
 í•˜ì§€ë§Œ SentencePieceëŠ” **ë„ì–´ì“°ê¸°ë¥¼ ë‹¤ë¥¸ ì•ŒíŒŒë²³ í˜¹ì€ ê¸€ìì²˜ëŸ¼ í•˜ë‚˜ì˜ characterë¡œ ì·¨ê¸‰**í•œë‹¤.      
 â¡ ê·¸ë ‡ê¸° ë•Œë¬¸ì— ëª¨ë“  ì¢…ë¥˜ì˜ ì–¸ì–´ì— ëŒ€í•´ generalí•˜ê²Œ ì ìš©ì´ ê°€ëŠ¥í•˜ê³ , pre-tokenizationì´ í•„ìš”í•˜ì§€ ì•Šë‹¤ëŠ” ì¥ì ì„ ê°€ì§„ë‹¤.    
* **Character-coverage ì„¤ì •ì´ ê°€ëŠ¥í•˜ë‹¤.**     
 ì˜ì–´ì˜ ê²½ìš°ì—ëŠ” ì•ŒíŒŒë²³ì˜ ê°œìˆ˜ê°€ 30ê°œ ë‚´ì™¸ë¡œ ì ê¸° ë•Œë¬¸ì— ê´œì±ƒì§€ë§Œ, ì¤‘êµ­ì–´ë‚˜ ì¼ë³¸ì–´ì™€ ê°™ì€ ë¬¸ìë“¤ì˜ ê²½ìš°ì—ëŠ” êµ‰ì¥íˆ ë§ì€ ì¢…ë¥˜ì˜ í•œìë¥¼ í¬í•¨í•˜ê³  ìˆê³ , ì´ì— ë”°ë¼ **ì—„ì²­ë‚˜ê²Œ ë§ì€ ê°œìˆ˜ì˜ characterë“¤ì„ ë‹¤ ë‹¤ë£¨ì–´ì•¼ í•´ì„œ tokenizerë¥¼ í•™ìŠµí•˜ëŠ” ë°ì— ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤**ëŠ” í•œê³„ì ì„ ê°€ì§„ë‹¤.     
í•˜ì§€ë§Œ SentencePieceëŠ” ëª¨ë“  characterë¥¼ ë‹¤ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, **ë¹ˆë„ì— ë”°ë¼ ìƒìœ„ 99%ì˜ characterë“¤ë§Œ ê³ ë ¤**í•˜ëŠ” ë“±ì˜ ì˜µì…˜ì„ ì£¼ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.
* **Subword generalzationì´ ê°€ëŠ¥í•˜ë‹¤.**      
 ìš°ë¦¬ê°€ í•™ìŠµí•œ tokenizerê°€ í•­ìƒ ì™„ë²½í•˜ë‹¤ë©´ ì¢‹ê² ì§€ë§Œ, ì´ê²ƒì´ **overfittingë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±**ì„ ë°°ì œí•  ìˆ˜ëŠ” ì—†ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— SentencePieceì—ì„œëŠ” ìš°ë¦¬ê°€ í•™ìŠµí•œ tokenizerì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” tokenë“¤ë§Œì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ê°€ë”ì€ ë‹¤ë¥¸ tokenë“¤ë„ ìƒ˜í”Œë§**í•˜ë©´ì„œ ì‚¬ìš©í•˜ì—¬ overfittingí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.       
* **ì†ë„ê°€ ë¹ ë¥´ë‹¤.**      
 ì§ì ‘ êµ¬í˜„í•´ë³´ë©´ ì•Œê² ì§€ë§Œ, ì•Œê³ ë¦¬ì¦˜ì´ êµ‰ì¥íˆ ìµœì í™”ë˜ì–´ ìˆì–´ì„œ ê°™ì€ BPEì™€ unigramì„ ì‹¤í–‰í•˜ë”ë¼ë„ êµ‰ì¥íˆ ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤.

</div>
</details>  

---

## Corpus 
ìš°ë¦¬ëŠ” Semantic Scholarì˜ 1.14M ë…¼ë¬¸ì˜ ë¬´ì‘ìœ„ ìƒ˜í”Œì— ëŒ€í•´ SCIBERTë¥¼ í›ˆë ¨í•œë‹¤.      
ì´ corpusì˜ êµ¬ì„±    
* ì»´í“¨í„° ê³¼í•™ ì˜ì—­ì˜ ë…¼ë¬¸ 18%           
* ê´‘ë²”ìœ„í•œ ìƒë¬¼ ì˜í•™ ì˜ì—­ì˜ ë…¼ë¬¸ 82%       

ìš°ë¦¬ëŠ” ë…¼ë¬¸ì˜ ì „ë¬¸ì„ ì‚¬ìš©í•œë‹¤. (ìš”ì•½ë³¸ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.)    

í‰ê·  ë…¼ë¬¸ ê¸¸ì´ëŠ” 154ê°œì˜ ë¬¸ì¥(2,769ê°œì˜ í† í°)ìœ¼ë¡œ, **BERTê°€ í›ˆë ¨ëœ 3.3B í† í°ê³¼ ìœ ì‚¬í•œ corpus í¬ê¸°**ê°€ ëœë‹¤.      
ìš°ë¦¬ëŠ” ê³¼í•™ì  í…ìŠ¤íŠ¸ì— ìµœì í™”ëœ **[ScispaCy](https://github.com/allenai/SciSpaCy)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ë¶„í• í•œë‹¤.    




<details>
<summary>ğŸ“œ Semantic Scholarë€?  </summary>
<div markdown="1">


**ì í•©í•œ ë…¼ë¬¸**ì„ ì‹ ì†í•˜ê²Œ ê²€ìƒ‰í•˜ì—¬ ì´ìš©ìë“¤ì´ ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ì´í•´í•˜ëŠ”ë° ë„ì›€ì´ ë˜ê³ ì PDFíŒŒì¼ë¡œ ê°„ëµí•œ ë°œí‘œ PPTí˜•ì‹ìœ¼ë¡œ **ìš”ì•½í•œ ë‚´ìš©ì„ ì—´ëŒí•  ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ë¥¼ ì œê³µ**í•˜ê³ ìˆëŠ” ê²ƒ.  
 
 ë˜í•œ íŠ¹ì •í•œ ì—°êµ¬ë¶„ì•¼ë¥¼ ì—°êµ¬í•˜ëŠ” í•™ìë¡œì„œ ìµœê·¼ ë°œí‘œëœ ë…¼ë¬¸ì„ Semantic Scholar ë…¼ë¬¸ ê²€ìƒ‰ ì—”ì§„ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ë…¼ë¬¸ì˜ ë‚´ìš©ì„ íŒŒì•…í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆë‹¤


ì´ íšŒì‚¬ëŠ” Allen Institute for Artificial Intelligenceì—ì„œ ì„¤ë¦½í•œ ë¹„ì˜ë¦¬ ê¸°ì—…ì´ë©° ë¬´ë£Œë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³  ìˆë‹¤. **ì¸ê³µì§€ëŠ¥ì€ ë…¼ë¬¸ì˜ ì¤„ê±°ë¦¬ ë‚´ìš©ì„ íŒŒì•…í•˜ì—¬ â€œAbstractiveâ€ê¸°ë²•ì„ í†µí•´ ìš”ì•½ëœ ë‚´ìš©ì„ ìƒì„±**í•˜ê³  ìˆë‹¤.
 
 
</div>
</details>  


---
---


# Experimental Setup
## 3.1 Tasks
ìš°ë¦¬ëŠ” ì•„ë˜ì˜ core NLP tasksë¡œ í‰ê°€í•œë‹¤:        
**1.** Named Entity Recognition (NER)   
**2.** PICO Extraction (PICO)   
**3**. Text Classification (CLS)   
**4.** Relation Classification (REL)  
**5.** Dependency Parsing (DEP)   


PICO, like NER, is a sequence labeling task where
the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes
in a clinical trial paper (Kim et al., 2011). REL
is a special case of text classification where the
model predicts the type of relation expressed between two entities, which are encapsulated in the
sentence by inserted special tokens.


PICOëŠ” NERì™€ ë§ˆì°¬ê°€ì§€ë¡œ ëª¨ë¸ì´ [Participants, Interventions, Comparisons, and Outcomesë¥¼ ì„¤ëª…í•˜ëŠ” ë²”ìœ„ë¥¼ ì¶”ì¶œ](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-S2-S5)í•˜ëŠ” sequence labeling taskì´ë‹¤.    
ê·¸ë¦¬ê³  RELì€ ëª¨ë¸ì´ ë‘ ì—”í‹°í‹° ê°„ì— í‘œí˜„ë˜ëŠ” ê´€ê³„ì˜ ìœ í˜•ì„ ì˜ˆì¸¡í•˜ëŠ” í…ìŠ¤íŠ¸ ë¶„ë¥˜ì˜ íŠ¹ë³„í•œ ê²½ìš°ì´ë©°, ì‚½ì…ëœ íŠ¹ìˆ˜ í† í°ì— ì˜í•´ ë¬¸ì¥ì— ìº¡ìŠí™”(encapsulated)ëœë‹¤.   


<details>
<summary>ğŸ“œ PICOë€?  </summary>
<div markdown="1">
 
**PICOë€?**        
ê³¼í•™ ì‹¤í—˜ì—ì„œ ì•„ë˜ ìš”ì¸ë“¤ì„ ë½‘ì•„ë‚´ëŠ” ê²ƒ    
PICO í”„ë ˆì„ì›Œí¬ëŠ” structuring clinical question ì§ˆë¬¸ì— í•„ìš”í•œ ê° í•µì‹¬ ìš”ì†Œë¥¼ í¬ì°©í•˜ê¸° ë•Œë¬¸ì— ì„ìƒ ì§ˆë¬¸ì„ êµ¬ì„±í•˜ëŠ” ë° ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. PICOëŠ” ë‹¤ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.    
* Participants/Problem (P)   
* Intervention (I)   
* Comparison (C)   
* Outcome (O)   

**evidence-based medicine (EBM) ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ëŒ€ê·œëª¨ ì˜í•™ ë¬¸í—Œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì„ìƒ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ì˜ì¡´**í•©ë‹ˆë‹¤.   

ì˜ ì •ì˜ë˜ê³  ì§‘ì¤‘ëœ ì„ìƒ ì§ˆë¬¸ì„ ê³µì‹í™”í•˜ê¸° ìœ„í•´ PICOë¼ëŠ” í”„ë ˆì„ì›Œí¬ê°€ ë„ë¦¬ ì‚¬ìš©ë˜ë©°, ì´ëŠ” ì°¸ê°€ì/ë¬¸ì œ(P), ê°œì…(I), ë¹„êµ(C) ë° ê²°ê³¼(O)ì˜ ë„¤ ê°€ì§€ êµ¬ì„± ìš”ì†Œì— ì†í•˜ëŠ” ì£¼ì–´ì§„ ì˜í•™ í…ìŠ¤íŠ¸ì˜ ë¬¸ì¥ì„ ì‹ë³„í•©ë‹ˆë‹¤. 
 
ì•„ë˜ í‘œëŠ” ì§ˆë¬¸ì˜ ìœ í˜•ì— ë”°ë¥¸ PICOì´ë‹¤.   

![image](https://user-images.githubusercontent.com/76824611/225283752-14153e36-7d6c-405f-8eeb-ca5347553eee.png)
![image](https://user-images.githubusercontent.com/76824611/225284104-08723df3-75bf-47b0-a336-133c3201b7f2.png)

 
 https://libguides.mssm.edu/ebm/ebp_pico
 
 
</div>
</details>  


<details>
<summary>ğŸ“œ NERì´ë€?  </summary>
<div markdown="1">
 
NER(Named Entity Recognition)ì€ ë§ ê·¸ëŒ€ë¡œ Named Entity(ì´ë¦„ì„ ê°€ì§„ ê°œì²´)ë¥¼ Recognition(ì¸ì‹)í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, ê°œì²´ëª… ì¸ì‹ì´ë¼ê³  í•©ë‹ˆë‹¤.
 
ì¦‰ ê°œì²´ë§ˆë‹¤ ë­”ì§€ íƒœê¹…ì„ í•´ì£¼ëŠ” ê²ƒë‹¤.

ì˜ˆë¥¼ ë“¤ë©´ ì•„ë˜ì™€ ê°™ë‹¤.   
![image](https://user-images.githubusercontent.com/76824611/225285580-0b1ac9d7-75d7-4412-b71a-fa9d60144b1c.png)

â€[ì¶œì²˜](https://www.letr.ai/blog/tech-20210723)   
 
</div>
</details>  

<details>
<summary>ğŸ“œ Automatic classification of sentences to support Evidence Based Medicine?  </summary>
<div markdown="1">
 
Automatic classification of sentences to support Evidence Based Medicine ë€?

**Aim**     
Evidence Based Medicineì—ì„œ ì‚¬ìš©ë˜ëŠ” ë¯¸ë¦¬ ì •ì˜ëœ ì˜ë£Œ ë²”ì£¼ ì„¸íŠ¸ê°€ ì£¼ì–´ì§€ë©´ **ì˜í•™ ì´ˆë¡ì˜ ë¬¸ì¥ì— ì´ëŸ¬í•œ ë ˆì´ë¸”ì„ ìë™ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ê²ƒ**ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. 
 
**ë°©ë²•**            
ìš°ë¦¬ëŠ” íŠ¹ì • **ì˜ë£Œ ë²”ì£¼(ì˜ˆ: ê°œì… , ê²°ê³¼) ë¡œ ì†ìœ¼ë¡œ ì£¼ì„**ì„ ë‹¨ 1,000ê°œì˜ ì˜ë£Œ ì´ˆë¡ ëª¨ìŒì„ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤ . ìš°ë¦¬ëŠ” ë¶„ë¥˜ë¥¼ ìœ„í•´ CRF(Conditional Random Fields)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ì–´íœ˜, ì˜ë¯¸, êµ¬ì¡° ë° ìˆœì°¨ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì˜ ì‚¬ìš©ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤.
 

**ê²°ê³¼**    
ëª¨ë“  ë ˆì´ë¸”ì— ëŒ€í•œ ë¶„ë¥˜ ì‘ì—…ì˜ ê²½ìš°, ìš°ë¦¬ ì‹œìŠ¤í…œì€ sequential featureì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™” ë° êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì´ˆë¡ì˜ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ ê°ê° 80.9% ë° 66.9%ì˜ micro-averaged f-scoresë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. í•µì‹¬ ë¬¸ì¥ì—ë§Œ ë ˆì´ë¸”ì„ ì§€ì •í•˜ëŠ” ê²½ìš°, ìš°ë¦¬ ì‹œìŠ¤í…œì€ ë™ì¼í•œ ìˆœì°¨ì  ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ì´ˆë¡ê³¼ êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì´ˆë¡ì— ëŒ€í•´ ê°ê° 89.3%ì™€ 74.0%ì˜  f-scoresë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì™¸ë¶€ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ê²°ê³¼ëŠ” ë” ë‚®ì•˜ìŠµë‹ˆë‹¤(ëª¨ë“  ë ˆì´ë¸”ì˜ ê²½ìš° f-ì ìˆ˜ 63.1%, ì£¼ìš” ë¬¸ì¥ì˜ ê²½ìš° 83.8%).

**ê²°ë¡ **         
ìš°ë¦¬ê°€ ì‚¬ìš©í•œ ê¸°ëŠ¥ ì¤‘ abstract ì—ì„œ ì£¼ì–´ì§„ ë¬¸ì¥ì„ ë¶„ë¥˜í•˜ëŠ” ë° ê°€ì¥ ì¢‹ì€ ê¸°ëŠ¥ì€ unigrams, section headings, and sequential informationì˜ ìˆœì°¨ì  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ëŠ¥ìœ¼ë¡œ ì¸í•´ simple bag-of-words ì ‘ê·¼ ë°©ì‹ì— ë¹„í•´ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìœ¼ë©° ì´ì „ ì‘ì—…ì—ì„œ ì‚¬ìš©ëœ feature setsë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¬ìŠµë‹ˆë‹¤.
 
</div>
</details>  





----




## 3.2 Datasets


**[Table 1]**     
* ëª¨ë“  tasks ë°  datasetsì—ì„œ ëª¨ë“  BERT ë³€í˜•ì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•œë‹¤.        
* êµµì€ ê¸€ì”¨ëŠ” SOTA ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. (95% bootstrap ì‹ ë¢° êµ¬ê°„ ë‚´ì—ì„œ ì°¨ì´ê°€ ìˆì„ ê²½ìš° ì—¬ëŸ¬ ê°œì˜ ê²°ê³¼ê°€ êµµì€ ê¸€ì”¨ë¡œ í‘œì‹œë¨).          
* ê³¼ê±° ì‘ì—…ê³¼ í•¨ê»˜, ìš°ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì€ ìš”ì†Œë¥¼ ë³´ê³ í•œë‹¤.        
    * NER (span-level)ì— ëŒ€í•œ macro F1 scores   
    * REL ë° CLS(ë¬¸ì¥ ë ˆë²¨, ë¬¸ì¥ì˜ ì‹œì‘ í† í°)ì— ëŒ€í•œ macro F1 scores    
    * PICO(í† í° ë ˆë²¨)ì— ëŒ€í•œ macro F1 scores     
    * ChemProì— ëŒ€í•œ micro F1 scores       
* ì ìˆ˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ë³´ê³ í•œë‹¤. DEPì˜ ê²½ìš°, ìš°ë¦¬ëŠ” LASì— ë§ê²Œ ì¡°ì •ëœ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ëª¨ë¸ì— ëŒ€í•œ ë ˆì´ë¸”ë§ëœ(LAS) ë° ë ˆì´ë¸”ë§ë˜ì§€ ì•Šì€(UAS) ì²¨ë¶€ ì ìˆ˜ë¥¼ ë³´ê³ í•œë‹¤. ëª¨ë“  ê²°ê³¼ëŠ” ì„œë¡œ ë‹¤ë¥¸ ëœë¤ ì‹œë“œë¥¼ ì‚¬ìš©í•œ ì—¬ëŸ¬ ëŸ°ì˜ í‰ê· ì…ë‹ˆë‹¤.



![image](https://user-images.githubusercontent.com/76824611/225508975-89b576ed-3457-433c-a7c6-7e91167e9e1f.png)

![image](https://user-images.githubusercontent.com/76824611/225509055-2a261c2e-bfb8-4fcf-83d1-04bd79b6a4ee.png)

ê° ë²”ì£¼ë³„ Precision, Recall, F1 Score
![image](https://user-images.githubusercontent.com/76824611/225509111-83dd797e-039c-460c-b734-84374d44ada6.png)


Macro Average F1 Score


Table 1: Test performances of all BERT variants on all tasks and datasets. Bold indicates the SOTA result (multiple
results bolded if difference within 95% bootstrap confidence interval). Keeping with past work, we report macro
F1 scores for NER (span-level), macro F1 scores for REL and CLS (sentence-level), and macro F1 for PICO
(token-level), and micro F1 for ChemProt specifically. For DEP, we report labeled (LAS) and unlabeled (UAS)
attachment scores (excluding punctuation) for the same model with hyperparameters tuned for LAS. All results
are the average of multiple runs with different random seeds.




ê°„ëµí™”ë¥¼ ìœ„í•´, ìš°ë¦¬ëŠ” ì—¬ê¸°ì„œ ìƒˆë¡œìš´ ë°ì´í„° ì„¸íŠ¸ë§Œ ì„¤ëª…í•˜ê³ , ì´ì „ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ í‘œ 1ì˜ ì°¸ì¡°ë¥¼ ë…ìì—ê²Œ ì°¸ì¡°í•˜ë„ë¡ í•œë‹¤. EBM-NLP(Nye et al., 2018)ëŠ” ì„ìƒ ì‹œí—˜ ìš”ì•½ì—ì„œ PICO ë²”ìœ„ì— ì£¼ì„ì„ ë‹¬ì•˜ë‹¤. SCIERC(Luan et al., 2018)ëŠ” ì»´í“¨í„° ê³¼í•™ ì¶”ìƒí™”ì˜ ì‹¤ì²´ì™€ ê´€ê³„ì— ì£¼ì„ì„ ë‹¬ì•˜ë‹¤. ACL-ARC(Jurgens et al., 2018)ì™€ SciCite(Cohan et al., 2019)ëŠ” ë‹¤ë¥¸ ë…¼ë¬¸ì„ ì¸ìš©í•˜ëŠ” ê³¼í•™ ë…¼ë¬¸ì˜ ë¬¸ì¥ì— ì˜ë„ ë ˆì´ë¸”(ì˜ˆ: ë¹„êµ, í™•ì¥ ë“±)ì„ í• ë‹¹í•œë‹¤. ì¢…ì´ í•„ë“œ ë°ì´í„° ì„¸íŠ¸ëŠ” Microsoft Academic Graph(Sinha et al., 2015) 3ì—ì„œ êµ¬ì¶•ë˜ì—ˆìœ¼ë©° ì¢…ì´ ì œëª©ì„ 7ê°œ ì—°êµ¬ ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì— ë§¤í•‘í•œë‹¤. ê° ì—°êµ¬ ë¶„ì•¼(ì§€ë¦¬, ì •ì¹˜, ê²½ì œ, ë¹„ì¦ˆë‹ˆìŠ¤, ì‚¬íšŒí•™, ì˜í•™ ë° ì‹¬ë¦¬í•™)ì—ëŠ” ì•½ 12Kì˜ êµìœ¡ ì‚¬ë¡€ê°€ ìˆìŠµë‹ˆë‹¤
For brevity, we only describe the newer datasets
here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al.,
2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab
stracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g.
Comparison, Extension, etc.) to sentences from
scientific papers that cite other papers. The Paper
Field dataset is built from the Microsoft Academic
Graph (Sinha et al., 2015)
3
and maps paper titles
to one of 7 fields of study. Each field of study
(i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples



---- 

## 3.3 Pretrained BERT Variants
### BERT-Base 
ì›ë˜ BERT ì½”ë“œì™€ í•¨ê»˜ ë¦´ë¦¬ìŠ¤ëœ BERT-Base(Devlin et al., 2019)ì— ëŒ€í•´ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•œë‹¤.4 ì–´íœ˜ëŠ” BASE VOCABì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ ëª¨ë¸ì˜ ì‚¬ë¡€ ë²„ì „ê³¼ ì‚¬ë¡€ê°€ ì—†ëŠ” ë²„ì „ì„ ëª¨ë‘ í‰ê°€í•œë‹¤.

We use the pretrained weights for
BERT-Base (Devlin et al., 2019) released with the
original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model.


### SCIBERT 
ìš°ë¦¬ëŠ” BERT-Baseì™€ ë™ì¼í•œ êµ¬ì„±ê³¼ í¬ê¸°ë¡œ ë§ë­‰ì¹˜ì—ì„œ SCIBERTë¥¼ í›ˆë ¨í•˜ê¸° ìœ„í•´ ì›ë˜ BERT ì½”ë“œë¥¼ ì‚¬ìš©í•œë‹¤. ìš°ë¦¬ëŠ” (i) ì‚¬ë¡€ ë˜ëŠ” ì‚¬ë¡€ê°€ ì—†ëŠ” SCIBERTì˜ 4ê°€ì§€ ë‹¤ë¥¸ ë²„ì „, (ii) BASE VOCAB ë˜ëŠ” SCIVOCABë¥¼ êµìœ¡í•œë‹¤. BASE VOCABì„ ì‚¬ìš©í•˜ëŠ” ë‘ ëª¨ë¸ì€ í•´ë‹¹ BERT-Base ëª¨ë¸ì—ì„œ ë¯¸ì„¸ ì¡°ì •ë©ë‹ˆë‹¤. ìƒˆë¡œìš´ SCIVOCABë¥¼ ì‚¬ìš©í•˜ëŠ” ë‹¤ë¥¸ ë‘ ëª¨ë¸ì€ ì²˜ìŒë¶€í„° í›ˆë ¨ëœë‹¤.

ê¸´ ë¬¸ì¥ì— ëŒ€í•œ BERT ì‚¬ì „ í›ˆë ¨ì€ ëŠë¦´ ìˆ˜ ìˆë‹¤. ì›ë˜ BERT ì½”ë“œì— ë”°ë¼ ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ë¥¼ 128ê°œ í† í°ìœ¼ë¡œ ì„¤ì •í•˜ê³  í›ˆë ¨ ì†ì‹¤ì´ ì¤„ì–´ë“¤ì§€ ì•Šì„ ë•Œê¹Œì§€ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¨ë‹¤. ê·¸ëŸ° ë‹¤ìŒ ìµœëŒ€ 512ê°œì˜ í† í°ê¹Œì§€ ë¬¸ì¥ ê¸¸ì´ë¥¼ í—ˆìš©í•˜ëŠ” ëª¨ë¸ì„ ê³„ì† í›ˆë ¨í•œë‹¤.

ìš°ë¦¬ëŠ” 8ê°œì˜ ì½”ì–´ê°€ ìˆëŠ” ë‹¨ì¼ TPU v3ë¥¼ ì‚¬ìš©í•œë‹¤. ë§ë­‰ì¹˜ì—ì„œ SCIVOCAB ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ëŠ” ë°ëŠ” 1ì£¼ 5ì¼(ìµœëŒ€ ê¸¸ì´ 128ì„ ì‚¬ìš©í•œ 5ì¼, ìµœëŒ€ ê¸¸ì´ 512ë¥¼ ì‚¬ìš©í•œ 2ì¼)ì´ ê±¸ë¦°ë‹¤. BASE VOCAB ëª¨ë¸ì€ ì²˜ìŒë¶€í„° í›ˆë ¨ì„ ë°›ì§€ ì•Šê¸° ë•Œë¬¸ì— í›ˆë ¨ ê¸°ê°„ì´ 2ì¼ ë” ì ìŠµë‹ˆë‹¤.

ëª¨ë“  ì‚¬ì „ í›ˆë ¨ëœ BERT ëª¨ë¸ì€ PyTorch íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ PyTorchì™€ í˜¸í™˜ë˜ë„ë¡ ë³€í™˜ë©ë‹ˆë‹¤.6 ìš°ë¦¬ì˜ ëª¨ë“  ëª¨ë¸(ì„¹ì…˜ 3.4 ë° 3.5)ì€ AllenNLPë¥¼ ì‚¬ìš©í•˜ì—¬ PyTorchì—ì„œ êµ¬í˜„ëœë‹¤(Gardner et al., 2017).

We use the original BERT code to
train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4
different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB. The
two models that use BASEVOCAB are finetuned
from the corresponding BERT-Base models. The
other two models that use the new SCIVOCAB are
trained from scratch.

Pretraining BERT for long sentences can be
slow. Following the original BERT code, we set a
maximum sentence length of 128 tokens, and train
the model until the training loss stops decreasing.
We then continue training the model allowing sentence lengths up to 512 tokens.


We use a single TPU v3 with 8 cores. Training
the SCIVOCAB models from scratch on our corpus
takes 1 week5
(5 days with max length 128, then
2 days with max length 512). The BASEVOCAB
models take 2 fewer days of training because they
arenâ€™t trained from scratch.


All pretrained BERT models are converted to
be compatible with PyTorch using the pytorchtransformers library.6 All our models (Sections 3.4 and 3.5) are implemented in PyTorch using AllenNLP (Gardner et al., 2017).


### Casing 
ìš°ë¦¬ëŠ” NERì— ì¼€ì´ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ë‹¤ë¥¸ ëª¨ë“  ì‘ì—…ì— ì¼€ì´ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë° ë°ë¸Œë¦° ë“±(2019)ì„ ë”°ë¥¸ë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ íŒŒì‹±ì„ ìœ„í•´ ì¼€ì´ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤. ì¼ë¶€ ë¹› ì‹¤í—˜ì€ ì¼€ì´ìŠ¤ê°€ ì—†ëŠ” ëª¨ë¸ì´ ì¼€ì´ìŠ¤ ëª¨ë¸ë³´ë‹¤ (ë•Œë¡œëŠ” NERì—ì„œë„) ì•½ê°„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.

We follow Devlin et al. (2019) in using
the cased models for NER and the uncased models
for all other tasks. We also use the cased models
for parsing. Some light experimentation showed
that the uncased models perform slightly better
(even sometimes on NER) than cased models.




---






















