---
title: "SCIBERT: A Pretrained Language Model for Scientific Text ì •ë¦¬"
date:   2023-02-20
excerpt: "SCIBERT: A Pretrained Language Model for Scientific Text"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# Abstract

<span style="background-color:#F5F5F5">**[ë¬¸ì œ]**</span>         
ê³¼í•™ ì˜ì—­ì—ì„œ NLP ì‘ì—…ì— ëŒ€í•œ large-scale annotated dataë¥¼ ì–»ëŠ” ê²ƒì€ ì–´ë µê³  ë¹„ìš©ì´ ë§ì´ ë“ ë‹¤.     

<span style="background-color:#F5F5F5">**[í•´ê²°: SCIBERT]**</span>         
* ìš°ë¦¬ëŠ” high-qualityì˜  large-scale labeled scientific dataì˜ ë¶€ì¡±ì„ í•´ê²°í•˜ê¸° ìœ„í•´,     
 [BERT](https://yerimoh.github.io/Lan2/)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ pre-trainëœ SCIBERTë¥¼ ë§Œë“¤ì—ˆ    
 * <span style="background-color:#FFE6E6">SCIBERTëŠ” **scientific publications**ì˜  **large multi-domain corpusì—ì„œ unsupervised pretraining ì„ í™œìš©**í•˜ì—¬ downstream scientific NLP tasksì˜ ì„±ëŠ¥ì„ í–¥ìƒ</span>ì‹œí‚¨ë‹¤.     
 * ì´ëŠ” BERTì— ë¹„í•´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤.    
 * ì½”ë“œ ë° ì‚¬ì „ êµìœ¡ëœ ëª¨ë¸ì€ [ë§í¬](https://github.com/allenai/scibert/)ì—ì„œ í™•ì¸ ê°€ëŠ¥í•˜ë‹¤.     



----
----


# 1 Introduction
scientific publicationsì˜ ì–‘ì´ ë§¤ìš° ë§ì•„, NLPëŠ” ì´ëŸ¬í•œ ë¬¸ì„œì˜ ëŒ€ê·œëª¨ ì§€ì‹ ì¶”ì¶œ ë° ê¸°ê³„ íŒë…ì„ ìœ„í•œ í•„ìˆ˜ ë„êµ¬ê°€ ë¨.   

<span style="background-color:#F5F5F5">**[ë¬¸ì œ]**</span>         
* NLPì˜ ëª¨ë¸ë“¤ì„ í›ˆë ¨í•˜ë ¤ë©´ ë§ì€ ì–‘ì˜ labeled dataê°€ í•„ìš”í•œ ê²½ìš°ê°€ ë§ë‹¤.     
â¡ ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒì€ ì–´ë µê³  ë¹„ìš©ì´ ë§ì´ ë“¦     
* [ELMo](https://wikidocs.net/33930), [GPT](https://yerimoh.github.io/Lan4/) ë° [BERT](https://yerimoh.github.io/Lan2/) í†µí•´ ì•Œ ìˆ˜ ìˆë“¯ì´,  large corporaì—ì„œ ì–¸ì–´ ëª¨ë¸ì˜ **unsupervised pretraining of language modelsì€ ë§ì€ NLP ì‘ì—…ì—ì„œ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒ**ì‹œí‚¨ë‹¤.     
ì´ëŸ¬í•œ ëª¨ë¸ì€ ê° í† í°ì— ëŒ€í•œ **contextualized embeddingsì„ ë°˜í™˜**í•œë‹¤.(ì¦‰, ë¬¸ë§¥ íŒŒì•…ì´ ê°€ëŠ¥í•œ í† í°ì„ ì¤€ë‹¤.)     
â¡ ì¦‰, ì´ëŸ¬í•œ <span style="background-color:#fff5b1">**unsupervised pretrainingì„ í™œìš©**í•˜ëŠ” ê²ƒì€ ê³¼í•™ì  NLPì—ì„œì™€ ê°™ì´ **task-specific annotationsì„ ì–»ê¸° ì–´ë ¤ìš´ ê²½ìš°ì— íŠ¹íˆ ì¤‘ìš”**</span>í•´ì¡Œë‹¤.      
* âš ï¸ í•˜ì§€ë§Œ BERTì™€ ELMoëŠ” task-specificê³¼ ê°™ì€ domainì´ ì•„ë‹Œ **general domain corpora**(such as news articles and Wikipedia.)**ë¡œ í›ˆë ¨ì„ ë°›ì€ ëª¨ë¸**ì´ë‹¤. (scienceì™€ ê°™ì€ task-specific corporaë¡œ í›ˆë ¨ì„ í•˜ì§€ ì•ŠìŒ)     


<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ í•´ê²°]**</span>         
ì´ ì‘ì—…ì—ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ì—¬ë¥¼ í•œë‹¤:
* **(i)** ìš°ë¦¬ëŠ” <span style="background-color:#FFE6E6">**scientific domain**ì—ì„œ NLP tasksì˜ ë²”ìœ„ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒ</span>ì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ì…ì¦ëœ ìƒˆë¡œìš´ ë¦¬ì†ŒìŠ¤ì¸ SCIBERTë¥¼ ì¶œì‹œí•œë‹¤.    
â¡ SCIBERTëŠ” BERTë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ pretrained language modelì´ì§€ë§Œ **large corpus of scientific text**ì— ëŒ€í•´ í›ˆë ¨ë˜ì—ˆë‹¤.         
* **(ii)** ìš°ë¦¬ëŠ” frozen embeddings ìœ„ì˜  <span style="background-color:#FFE6E6">**fine tuning VS task-specific architecture**ì˜ ì„±ëŠ¥ê³¼  in-domain vocabularyì˜ ì˜í–¥ì„ ì¡°ì‚¬</span>í•˜ê¸° ìœ„í•´ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•œë‹¤       
* **(iii)** ìš°ë¦¬ëŠ” scientific domainì˜ ì¼ë ¨ì˜ ì‘ì—…ì— ëŒ€í•´ SCIBERTë¥¼ evaluateí•˜ê³ , SOTA ê²°ê³¼ë¥¼ ë‹¬ì„±í•œë‹¤.        




---
----


# 2 Methods
## Background 
* [BERT](https://yerimoh.github.io/Lan2/)ëŠ” [multilayer bidirectional Transformer](https://yerimoh.github.io/Lan2/#1-pre-training-bert)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤.      
* ê¸°ì¡´ì˜ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ë§ í•˜ëŠ” ëŒ€ì‹ ,     
BERTëŠ” [ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡(Masked Language Model, MLM)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)í•˜ê³ ,    
[ë‘ ë¬¸ì¥ì´ ì„œë¡œ ë”°ë¥´ëŠ”ì§€ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡(Next sentence prediction, NSP)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)í•˜ëŠ” ë‘ ê°€ì§€ ì‘ì—…ì— ëŒ€í•´ í›ˆë ¨ëœë‹¤.    

SCIBERTëŠ” BERTì™€ **ë™ì¼í•œ ì•„í‚¤í…ì²˜**ë¥¼ ë”°ë¥´ì§€ë§Œ ëŒ€ì‹  **scientific textì— ëŒ€í•´ pretrained**ì„ ë°›ëŠ”ë‹¤.     

---

## Vocabulary
BERTëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ì˜ unsupervised tokenizationë¥¼ ìœ„í•´ [WordPiece](https://yerimoh.github.io/Lan2/#token-embeddings)ë¥¼ ì‚¬ìš©í•œë‹¤.      
vocabularyëŠ” ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” words or subword unitsë¥¼ í¬í•¨í•˜ë„ë¡ êµ¬ì„±ëœë‹¤.    

ìš°ë¦¬ëŠ” BERTì™€ í•¨ê»˜ ë°œí‘œëœ ì›ë˜ì˜ ì–´íœ˜ë¥¼ **BASEVOCAB**ë¼ê³  ë¶€ë¥¸ë‹¤.      


ìš°ë¦¬ëŠ” [SentencePiece ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/google/sentencepiece)ë¥¼ ì‚¬ìš©í•˜ì—¬,    
**scientific corpusì— ìƒˆë¡œìš´ WordPiece ì–´íœ˜ì¸ SCIVOCAB**ë¥¼ êµ¬ì„±í•œë‹¤.     
* ë³¸ ë…¼ë¬¸ì€ ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìê°€ ì—†ëŠ” ì–´íœ˜ë¥¼ ëª¨ë‘ ìƒì„±í•˜ê³  BASE VOCABì˜ í¬ê¸°ì— ë§ê²Œ ì–´íœ˜ í¬ê¸°ë¥¼ 30Kë¡œ ì„¤ì •í•œë‹¤.   
* BASEVOCABì™€ SCIVOCAB ì‚¬ì´ì˜ ê²°ê³¼ ì¼ë°˜ VOCABì˜ í† í°ê³¼ SCIVOACBì˜ ê°™ì€ í† í°ì€ 42%ë¡œ **scientific and general domain textsì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ì˜ ìƒë‹¹í•œ ì°¨ì´**ë¥¼ ë³´ì—¬ì¤€ë‹¤    


<details>
<summary>ğŸ“œ SentencePieceë€? </summary>
<div markdown="1">

ë…¼ë¬¸ : https://arxiv.org/pdf/1808.06226.pdf
êµ¬ê¸€ì´ BPE ì•Œê³ ë¦¬ì¦˜ê³¼ Unigram Language Model Tokenizerë¥¼ êµ¬í˜„í•œ ê²ƒ.   

SentencePieceëŠ” **pre-tokenizationì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ” tokenizer**ì˜ í•˜ë‚˜ë¡œ, ì–´ë–¤ ì–¸ì–´ì—ë„ ììœ ë¡­ê²Œ ì ìš©ë  ìˆ˜ ìˆê³  ì†ë„ë„ êµ‰ì¥íˆ ë¹ ë¥´ê¸° ë•Œë¬¸ì— NLPì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” tokenizerì´ë‹¤. 

SetencePieceëŠ” ê¸°ì¡´ì— ì¡´ì¬í•˜ë˜ unigram, BPEì™€ ê°™ì€ tokenizerë“¤ì„ ëª¨ë“  ì–¸ì–´ì— ëŒ€í•´ ì ìš©ì´ ê°€ëŠ¥í•˜ë„ë¡ generalizeí•˜ê³  ì•½ê°„ì˜ ì¶”ê°€ì ì¸ ê¸°ëŠ¥ë“¤ì„ ë”í•´ì„œ êµ¬í˜„í•œ ê²ƒì´ë‹¤.    
íŠ¹ë³„íˆ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ì œì‹œí–ˆë‹¤ê¸°ë³´ë‹¤ëŠ”, ê¸°ì¡´ì˜ tokenizerë“¤ì„ ì¢€ ë” ì‚¬ìš©í•˜ê¸° í¸í•˜ê³  ì„±ëŠ¥ì´ ì¢‹ê²Œ ê°œì„ í–ˆë‹¤ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤. 
 
 
**[SentencePieceì˜ íŠ¹ì§•]**         
* **Pre-tokenizationì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ”ë‹¤.**    
 ê¸°ì¡´ì˜ tokenizerë“¤ì€ ì˜ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµë˜ì—ˆë‹¤ë©´ ì˜ì–´ê°€ ë„ì–´ì“°ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ì„œ ë‹¨ì–´ë“¤ì´ ëŒ€ë¶€ë¶„ ë¶„ë¦¬ë˜ê¸° ë•Œë¬¸ì—, **ë„ì–´ì“°ê¸°ê°€ ì—†ëŠ” ì¼ë³¸ì–´ë‚˜ ì¤‘êµ­ì–´ì™€ ê°™ì€ ì–¸ì–´ë“¤ì—ëŠ” ì ìš©ë  ìˆ˜ ì—†ë‹¤**ëŠ” í•œê³„ì ì„ ê°€ì§€ê³  ìˆì—ˆë‹¤.     
 í•˜ì§€ë§Œ SentencePieceëŠ” **ë„ì–´ì“°ê¸°ë¥¼ ë‹¤ë¥¸ ì•ŒíŒŒë²³ í˜¹ì€ ê¸€ìì²˜ëŸ¼ í•˜ë‚˜ì˜ characterë¡œ ì·¨ê¸‰**í•œë‹¤.      
 â¡ ê·¸ë ‡ê¸° ë•Œë¬¸ì— ëª¨ë“  ì¢…ë¥˜ì˜ ì–¸ì–´ì— ëŒ€í•´ generalí•˜ê²Œ ì ìš©ì´ ê°€ëŠ¥í•˜ê³ , pre-tokenizationì´ í•„ìš”í•˜ì§€ ì•Šë‹¤ëŠ” ì¥ì ì„ ê°€ì§„ë‹¤.    
* **Character-coverage ì„¤ì •ì´ ê°€ëŠ¥í•˜ë‹¤.**     
 ì˜ì–´ì˜ ê²½ìš°ì—ëŠ” ì•ŒíŒŒë²³ì˜ ê°œìˆ˜ê°€ 30ê°œ ë‚´ì™¸ë¡œ ì ê¸° ë•Œë¬¸ì— ê´œì±ƒì§€ë§Œ, ì¤‘êµ­ì–´ë‚˜ ì¼ë³¸ì–´ì™€ ê°™ì€ ë¬¸ìë“¤ì˜ ê²½ìš°ì—ëŠ” êµ‰ì¥íˆ ë§ì€ ì¢…ë¥˜ì˜ í•œìë¥¼ í¬í•¨í•˜ê³  ìˆê³ , ì´ì— ë”°ë¼ **ì—„ì²­ë‚˜ê²Œ ë§ì€ ê°œìˆ˜ì˜ characterë“¤ì„ ë‹¤ ë‹¤ë£¨ì–´ì•¼ í•´ì„œ tokenizerë¥¼ í•™ìŠµí•˜ëŠ” ë°ì— ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤**ëŠ” í•œê³„ì ì„ ê°€ì§„ë‹¤.     
í•˜ì§€ë§Œ SentencePieceëŠ” ëª¨ë“  characterë¥¼ ë‹¤ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, **ë¹ˆë„ì— ë”°ë¼ ìƒìœ„ 99%ì˜ characterë“¤ë§Œ ê³ ë ¤**í•˜ëŠ” ë“±ì˜ ì˜µì…˜ì„ ì£¼ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.
* **Subword generalzationì´ ê°€ëŠ¥í•˜ë‹¤.**      
 ìš°ë¦¬ê°€ í•™ìŠµí•œ tokenizerê°€ í•­ìƒ ì™„ë²½í•˜ë‹¤ë©´ ì¢‹ê² ì§€ë§Œ, ì´ê²ƒì´ **overfittingë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±**ì„ ë°°ì œí•  ìˆ˜ëŠ” ì—†ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— SentencePieceì—ì„œëŠ” ìš°ë¦¬ê°€ í•™ìŠµí•œ tokenizerì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” tokenë“¤ë§Œì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ê°€ë”ì€ ë‹¤ë¥¸ tokenë“¤ë„ ìƒ˜í”Œë§**í•˜ë©´ì„œ ì‚¬ìš©í•˜ì—¬ overfittingí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.       
* **ì†ë„ê°€ ë¹ ë¥´ë‹¤.**      
 ì§ì ‘ êµ¬í˜„í•´ë³´ë©´ ì•Œê² ì§€ë§Œ, ì•Œê³ ë¦¬ì¦˜ì´ êµ‰ì¥íˆ ìµœì í™”ë˜ì–´ ìˆì–´ì„œ ê°™ì€ BPEì™€ unigramì„ ì‹¤í–‰í•˜ë”ë¼ë„ êµ‰ì¥íˆ ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤.

</div>
</details>  

---

## Corpus 
ìš°ë¦¬ëŠ” Semantic Scholarì˜ 1.14M ë…¼ë¬¸ì˜ ë¬´ì‘ìœ„ ìƒ˜í”Œì— ëŒ€í•´ SCIBERTë¥¼ í›ˆë ¨í•œë‹¤.      
ì´ corpusì˜ êµ¬ì„±    
* ì»´í“¨í„° ê³¼í•™ ì˜ì—­ì˜ ë…¼ë¬¸ 18%           
* ê´‘ë²”ìœ„í•œ ìƒë¬¼ ì˜í•™ ì˜ì—­ì˜ ë…¼ë¬¸ 82%       

ìš°ë¦¬ëŠ” ë…¼ë¬¸ì˜ ì „ë¬¸ì„ ì‚¬ìš©í•œë‹¤. (ìš”ì•½ë³¸ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.)    

í‰ê·  ë…¼ë¬¸ ê¸¸ì´ëŠ” 154ê°œì˜ ë¬¸ì¥(2,769ê°œì˜ í† í°)ìœ¼ë¡œ, **BERTê°€ í›ˆë ¨ëœ 3.3B í† í°ê³¼ ìœ ì‚¬í•œ corpus í¬ê¸°**ê°€ ëœë‹¤.      
ìš°ë¦¬ëŠ” ê³¼í•™ì  í…ìŠ¤íŠ¸ì— ìµœì í™”ëœ **[ScispaCy](https://github.com/allenai/SciSpaCy)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ë¶„í• í•œë‹¤.    




<details>
<summary>ğŸ“œ Semantic Scholarë€?  </summary>
<div markdown="1">


**ì í•©í•œ ë…¼ë¬¸**ì„ ì‹ ì†í•˜ê²Œ ê²€ìƒ‰í•˜ì—¬ ì´ìš©ìë“¤ì´ ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ì´í•´í•˜ëŠ”ë° ë„ì›€ì´ ë˜ê³ ì PDFíŒŒì¼ë¡œ ê°„ëµí•œ ë°œí‘œ PPTí˜•ì‹ìœ¼ë¡œ **ìš”ì•½í•œ ë‚´ìš©ì„ ì—´ëŒí•  ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ë¥¼ ì œê³µ**í•˜ê³ ìˆëŠ” ê²ƒ.  
 
 ë˜í•œ íŠ¹ì •í•œ ì—°êµ¬ë¶„ì•¼ë¥¼ ì—°êµ¬í•˜ëŠ” í•™ìë¡œì„œ ìµœê·¼ ë°œí‘œëœ ë…¼ë¬¸ì„ Semantic Scholar ë…¼ë¬¸ ê²€ìƒ‰ ì—”ì§„ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ë…¼ë¬¸ì˜ ë‚´ìš©ì„ íŒŒì•…í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆë‹¤


ì´ íšŒì‚¬ëŠ” Allen Institute for Artificial Intelligenceì—ì„œ ì„¤ë¦½í•œ ë¹„ì˜ë¦¬ ê¸°ì—…ì´ë©° ë¬´ë£Œë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³  ìˆë‹¤. **ì¸ê³µì§€ëŠ¥ì€ ë…¼ë¬¸ì˜ ì¤„ê±°ë¦¬ ë‚´ìš©ì„ íŒŒì•…í•˜ì—¬ â€œAbstractiveâ€ê¸°ë²•ì„ í†µí•´ ìš”ì•½ëœ ë‚´ìš©ì„ ìƒì„±**í•˜ê³  ìˆë‹¤.
 
 
</div>
</details>  


---
---


# Experimental Setup
## 3.1 Tasks
ìš°ë¦¬ëŠ” ì•„ë˜ì˜ core NLP tasksë¡œ í‰ê°€í•œë‹¤:        
**1.** Named Entity Recognition (NER)   
**2.** PICO Extraction (PICO)   
**3**. Text Classification (CLS)   
**4.** Relation Classification (REL)  
**5.** Dependency Parsing (DEP)   


PICO, like NER, is a sequence labeling task where
the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes
in a clinical trial paper (Kim et al., 2011). REL
is a special case of text classification where the
model predicts the type of relation expressed between two entities, which are encapsulated in the
sentence by inserted special tokens.


PICOëŠ” NERì™€ ë§ˆì°¬ê°€ì§€ë¡œ ëª¨ë¸ì´ [Participants, Interventions, Comparisons, and Outcomesë¥¼ ì„¤ëª…í•˜ëŠ” ë²”ìœ„ë¥¼ ì¶”ì¶œ](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-S2-S5)í•˜ëŠ” sequence labeling taskì´ë‹¤.    
ê·¸ë¦¬ê³  RELì€ ëª¨ë¸ì´ ë‘ ì—”í‹°í‹° ê°„ì— í‘œí˜„ë˜ëŠ” ê´€ê³„ì˜ ìœ í˜•ì„ ì˜ˆì¸¡í•˜ëŠ” í…ìŠ¤íŠ¸ ë¶„ë¥˜ì˜ íŠ¹ë³„í•œ ê²½ìš°ì´ë©°, ì‚½ì…ëœ íŠ¹ìˆ˜ í† í°ì— ì˜í•´ ë¬¸ì¥ì— ìº¡ìŠí™”(encapsulated)ëœë‹¤.   


<details>
<summary>ğŸ“œ PICOë€?  </summary>
<div markdown="1">
 
**PICOë€?**        
ê³¼í•™ ì‹¤í—˜ì—ì„œ ì•„ë˜ ìš”ì¸ë“¤ì„ ë½‘ì•„ë‚´ëŠ” ê²ƒ    
PICO í”„ë ˆì„ì›Œí¬ëŠ” structuring clinical question ì§ˆë¬¸ì— í•„ìš”í•œ ê° í•µì‹¬ ìš”ì†Œë¥¼ í¬ì°©í•˜ê¸° ë•Œë¬¸ì— ì„ìƒ ì§ˆë¬¸ì„ êµ¬ì„±í•˜ëŠ” ë° ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. PICOëŠ” ë‹¤ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.    
* Participants/Problem (P)   
* Intervention (I)   
* Comparison (C)   
* Outcome (O)   

**evidence-based medicine (EBM) ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ëŒ€ê·œëª¨ ì˜í•™ ë¬¸í—Œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì„ìƒ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ì˜ì¡´**í•©ë‹ˆë‹¤.   

ì˜ ì •ì˜ë˜ê³  ì§‘ì¤‘ëœ ì„ìƒ ì§ˆë¬¸ì„ ê³µì‹í™”í•˜ê¸° ìœ„í•´ PICOë¼ëŠ” í”„ë ˆì„ì›Œí¬ê°€ ë„ë¦¬ ì‚¬ìš©ë˜ë©°, ì´ëŠ” ì°¸ê°€ì/ë¬¸ì œ(P), ê°œì…(I), ë¹„êµ(C) ë° ê²°ê³¼(O)ì˜ ë„¤ ê°€ì§€ êµ¬ì„± ìš”ì†Œì— ì†í•˜ëŠ” ì£¼ì–´ì§„ ì˜í•™ í…ìŠ¤íŠ¸ì˜ ë¬¸ì¥ì„ ì‹ë³„í•©ë‹ˆë‹¤. 
 
ì•„ë˜ í‘œëŠ” ì§ˆë¬¸ì˜ ìœ í˜•ì— ë”°ë¥¸ PICOì´ë‹¤.   

![image](https://user-images.githubusercontent.com/76824611/225283752-14153e36-7d6c-405f-8eeb-ca5347553eee.png)
![image](https://user-images.githubusercontent.com/76824611/225284104-08723df3-75bf-47b0-a336-133c3201b7f2.png)

 
 https://libguides.mssm.edu/ebm/ebp_pico
 
 
</div>
</details>  


<details>
<summary>ğŸ“œ NERì´ë€?  </summary>
<div markdown="1">
 
NER(Named Entity Recognition)ì€ ë§ ê·¸ëŒ€ë¡œ Named Entity(ì´ë¦„ì„ ê°€ì§„ ê°œì²´)ë¥¼ Recognition(ì¸ì‹)í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, ê°œì²´ëª… ì¸ì‹ì´ë¼ê³  í•©ë‹ˆë‹¤.
 
ì¦‰ ê°œì²´ë§ˆë‹¤ ë­”ì§€ íƒœê¹…ì„ í•´ì£¼ëŠ” ê²ƒë‹¤.

ì˜ˆë¥¼ ë“¤ë©´ ì•„ë˜ì™€ ê°™ë‹¤.   
![image](https://user-images.githubusercontent.com/76824611/225285580-0b1ac9d7-75d7-4412-b71a-fa9d60144b1c.png)

â€[ì¶œì²˜](https://www.letr.ai/blog/tech-20210723)   
 
</div>
</details>  

<details>
<summary>ğŸ“œ Automatic classification of sentences to support Evidence Based Medicine?  </summary>
<div markdown="1">
 
Automatic classification of sentences to support Evidence Based Medicine ë€?

**Aim**     
Evidence Based Medicineì—ì„œ ì‚¬ìš©ë˜ëŠ” ë¯¸ë¦¬ ì •ì˜ëœ ì˜ë£Œ ë²”ì£¼ ì„¸íŠ¸ê°€ ì£¼ì–´ì§€ë©´ **ì˜í•™ ì´ˆë¡ì˜ ë¬¸ì¥ì— ì´ëŸ¬í•œ ë ˆì´ë¸”ì„ ìë™ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ê²ƒ**ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. 
 
**ë°©ë²•**            
ìš°ë¦¬ëŠ” íŠ¹ì • **ì˜ë£Œ ë²”ì£¼(ì˜ˆ: ê°œì… , ê²°ê³¼) ë¡œ ì†ìœ¼ë¡œ ì£¼ì„**ì„ ë‹¨ 1,000ê°œì˜ ì˜ë£Œ ì´ˆë¡ ëª¨ìŒì„ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤ . ìš°ë¦¬ëŠ” ë¶„ë¥˜ë¥¼ ìœ„í•´ CRF(Conditional Random Fields)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ì–´íœ˜, ì˜ë¯¸, êµ¬ì¡° ë° ìˆœì°¨ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì˜ ì‚¬ìš©ì„ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤.
 

**ê²°ê³¼**    
ëª¨ë“  ë ˆì´ë¸”ì— ëŒ€í•œ ë¶„ë¥˜ ì‘ì—…ì˜ ê²½ìš°, ìš°ë¦¬ ì‹œìŠ¤í…œì€ sequential featureì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™” ë° êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì´ˆë¡ì˜ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ ê°ê° 80.9% ë° 66.9%ì˜ micro-averaged f-scoresë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. í•µì‹¬ ë¬¸ì¥ì—ë§Œ ë ˆì´ë¸”ì„ ì§€ì •í•˜ëŠ” ê²½ìš°, ìš°ë¦¬ ì‹œìŠ¤í…œì€ ë™ì¼í•œ ìˆœì°¨ì  ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ì´ˆë¡ê³¼ êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì´ˆë¡ì— ëŒ€í•´ ê°ê° 89.3%ì™€ 74.0%ì˜  f-scoresë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì™¸ë¶€ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ê²°ê³¼ëŠ” ë” ë‚®ì•˜ìŠµë‹ˆë‹¤(ëª¨ë“  ë ˆì´ë¸”ì˜ ê²½ìš° f-ì ìˆ˜ 63.1%, ì£¼ìš” ë¬¸ì¥ì˜ ê²½ìš° 83.8%).

**ê²°ë¡ **         
ìš°ë¦¬ê°€ ì‚¬ìš©í•œ ê¸°ëŠ¥ ì¤‘ abstract ì—ì„œ ì£¼ì–´ì§„ ë¬¸ì¥ì„ ë¶„ë¥˜í•˜ëŠ” ë° ê°€ì¥ ì¢‹ì€ ê¸°ëŠ¥ì€ unigrams, section headings, and sequential informationì˜ ìˆœì°¨ì  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ëŠ¥ìœ¼ë¡œ ì¸í•´ simple bag-of-words ì ‘ê·¼ ë°©ì‹ì— ë¹„í•´ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìœ¼ë©° ì´ì „ ì‘ì—…ì—ì„œ ì‚¬ìš©ëœ feature setsë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¬ìŠµë‹ˆë‹¤.
 
</div>
</details>  





----




## 3.2 Datasets


**[Table 1]**     
![image](https://user-images.githubusercontent.com/76824611/225572942-78914c0c-d6e1-4437-a1cb-b4bd6a95cc3e.png)
* ëª¨ë“  tasks ë°  datasetsì—ì„œ ëª¨ë“  BERT ë³€í˜•ì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•œë‹¤.        
* êµµì€ ê¸€ì”¨ëŠ” SOTA ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. (95% bootstrap ì‹ ë¢° êµ¬ê°„ ë‚´ì—ì„œ ì°¨ì´ê°€ ìˆì„ ê²½ìš° ì—¬ëŸ¬ ê°œì˜ ê²°ê³¼ê°€ êµµì€ ê¸€ì”¨ë¡œ í‘œì‹œë¨).          
* ê³¼ê±° ì‘ì—…ê³¼ í•¨ê»˜, ìš°ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ì€ ìš”ì†Œë¥¼ ë³´ê³ í•œë‹¤.        
    * NER (span-level)ì— ëŒ€í•œ macro F1 scores   
    * REL ë° CLS(ë¬¸ì¥ ë ˆë²¨, ë¬¸ì¥ì˜ ì‹œì‘ í† í°)ì— ëŒ€í•œ macro F1 scores    
    * PICO(í† í° ë ˆë²¨)ì— ëŒ€í•œ macro F1 scores     
    * ChemProì— ëŒ€í•œ micro F1 scores       
    * DEPì˜ ê²½ìš°, ìš°ë¦¬ëŠ” LASì— ë§ê²Œ ì¡°ì •ëœ hyperparametersë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ëª¨ë¸ì— ëŒ€í•œ ë ˆì´ë¸”ë§ëœ(LAS) ë° ë ˆì´ë¸”ë§ë˜ì§€ ì•Šì€(UAS) attachment scoresë¥¼ ë³´ê³ í•œë‹¤.     
* ëª¨ë“  ê²°ê³¼ëŠ” ì„œë¡œ ë‹¤ë¥¸ random seedsë¥¼ ì‚¬ìš©í•œ ì—¬ëŸ¬ runsì˜ í‰ê· ì´ë‹¤.         




<details>
<summary>ğŸ“œ F1 scores  </summary>
<div markdown="1">
 
 
![image](https://user-images.githubusercontent.com/76824611/225508975-89b576ed-3457-433c-a7c6-7e91167e9e1f.png)

![image](https://user-images.githubusercontent.com/76824611/225509055-2a261c2e-bfb8-4fcf-83d1-04bd79b6a4ee.png)

ê° ë²”ì£¼ë³„ Precision, Recall, F1 Score
![image](https://user-images.githubusercontent.com/76824611/225509111-83dd797e-039c-460c-b734-84374d44ada6.png)

Macro Average F1 Score
![image](https://user-images.githubusercontent.com/76824611/225510862-71947c9c-2e64-451d-b627-ae50d17d9e51.png)

Micro Average F1 Score = accuracy = micro-precision = micro-recall
![image](https://user-images.githubusercontent.com/76824611/225510902-105ad68e-a594-424c-b7a4-2f6c4d86fe2e.png)

</div>
</details>  




ìœ„ í‘œ ì™¸ì˜ ë°ì´í„°ë¥¼ ì„¤ëª…í•˜ìë©´,    
* [EBM-NLP](https://aclanthology.org/P18-1019/)ëŠ” ì„ìƒ ì‹œí—˜ ìš”ì•½ì—ì„œ PICO ë²”ìœ„ì— ì£¼ì„ì„ ë‹¬ì•˜ë‹¤.     
* [SCIERC](https://aclanthology.org/D18-1360.pdf)ëŠ” relations from computer science abstractsì˜ entitiesì™€  relationsì— ì£¼ì„ì„ ë‹¬ì•˜ë‹¤.     
* [ACL-ARC](https://aclanthology.org/Q18-1028.pdf)ì™€ [SciCite](https://aclanthology.org/N19-1361.pdf)ëŠ” ë‹¤ë¥¸ ë…¼ë¬¸ì„ ì¸ìš©í•˜ëŠ” ê³¼í•™ ë…¼ë¬¸ì˜ ë¬¸ì¥ì—  intent labels(ì˜ˆ: Comparison, Extension ë“±)ì„ í• ë‹¹í•œë‹¤.     
* ë…¼ë¬¸ ë¶„ì•¼ datasetëŠ” [Microsoft Academic Graph](https://www.microsoft.com/en-us/research/publication/overview-microsoft-academic-service-mas-applications/)ì—ì„œ êµ¬ì¶•ë˜ì—ˆìœ¼ë©° ë…¼ë¬¸ ì œëª©ì„ 7ê°œ ì—°êµ¬ ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì— ë§¤í•‘í•œë‹¤.     
ê° ì—°êµ¬ ë¶„ì•¼( geography, politics, economics, business, sociology, medicine, and psychology)ì—ëŠ” ì•½ 12Kì˜ training exampleì´ ìˆë‹¤.    


---- 


## 3.3 Pretrained BERT Variants
### BERT-Base 
ì›ë˜ [BERT ì½”ë“œ](https://github.com/google-research/bert)ì™€ í•¨ê»˜ ë¦´ë¦¬ìŠ¤ëœ [BERT-Base](https://yerimoh.github.io/Lan2/)ì— ëŒ€í•´ pretrained weightsë¥¼ ì‚¬ìš©í•œë‹¤.      

ì–´íœ˜ëŠ” BASE VOCABì´ë‹¤.   

ìš°ë¦¬ëŠ” ì´ ëª¨ë¸ì˜ cased ë²„ì „ê³¼ uncased ë²„ì „ì„ ëª¨ë‘ í‰ê°€í•œë‹¤.




### SCIBERT 
ìš°ë¦¬ëŠ” **BERT-Baseì™€ ë™ì¼í•œ êµ¬ì„±ê³¼ í¬ê¸°ë¡œ ë§ë­‰ì¹˜ì—ì„œ SCIBERTë¥¼ í›ˆë ¨**í•˜ê¸° ìœ„í•´ ì›ë˜ BERT ì½”ë“œë¥¼ ì‚¬ìš©í•œë‹¤.    

ìš°ë¦¬ëŠ” ì•„ë˜ 4ê°€ì§€ different SCIBERT caseì— ëŒ€í•´ trainí•œë‹¤.(iì™€ iiì—ì„œ í•˜ë‚˜ì”© ì¡°í•©)      
**(i)** cased or uncased      
**(ii)** BASEVOCAB or SCIVOCAB         

**BASEVOCAB**ì„ ì‚¬ìš©í•˜ëŠ” ë‘ ëª¨ë¸ì€ í•´ë‹¹ **BERT-Base ëª¨ë¸ì—ì„œ finetuning**ëœë‹¤.       
**SCIVOCAB**ë¥¼ ì‚¬ìš©í•˜ëŠ” ë‘ ëª¨ë¸ì€ **ì²˜ìŒ(scratch)ë¶€í„° train**ëœë‹¤.      

<span style="background-color:#F5F5F5">**[train ë°©ë²•]**</span>              
* **1)** original BERT ì½”ë“œì— ë”°ë¼ ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ë¥¼ 128ê°œ í† í°ìœ¼ë¡œ ì„¤ì •í•¨     
long sentencesì— ëŒ€í•œ BERT Pretrainingì€ ëŠë¦´ ìˆ˜ ìˆê¸° ë•Œë¬¸      
* **2)** training lossê°€ ì¤„ì–´ë“¤ì§€ ì•Šì„ ë•Œê¹Œì§€ ëª¨ë¸ì„ trainì‹œí‚¨ë‹¤.  
* **3)** ê·¸ëŸ° ë‹¤ìŒ ìµœëŒ€ 512ê°œì˜ í† í°ê¹Œì§€ ë¬¸ì¥ ê¸¸ì´ë¥¼ í—ˆìš©í•˜ëŠ” ëª¨ë¸ì„ ê³„ì† í›ˆë ¨í•œë‹¤.   

<span style="background-color:#F5F5F5">**[train time]**</span>              
* ìš°ë¦¬ëŠ” 8ê°œì˜ ì½”ì–´ê°€ ìˆëŠ” single TPU v3ë¥¼ ì‚¬ìš©í•œë‹¤.    
* corpusì—ì„œ SCIVOCAB ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í›ˆë ¨í•˜ëŠ” ë°ëŠ” 1ì£¼ì¼ í•˜ê³  5ì¼ì´ ê±¸ë ¸ë‹¤.   
 ìµœëŒ€ ê¸¸ì´ 128ì„ ì‚¬ìš©í•˜ë©´ 5ì¼, ìµœëŒ€ ê¸¸ì´ 512ë¥¼ ì‚¬ìš©í•˜ë©´ 2ì¼ì´ ê±¸ë¦°ë‹¤.      
* BASE VOCAB ëª¨ë¸ì€ ì²˜ìŒë¶€í„° í›ˆë ¨ì„ ë°›ì§€ ì•Šê¸° ë•Œë¬¸ì— í›ˆë ¨ ê¸°ê°„ì´ 2ì¼ ë” ì ë‹¤.    

ëª¨ë“  ì‚¬ì „ í›ˆë ¨ëœ BERT ëª¨ë¸ì€ [PyTorch transformer ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/huggingface/transformers)ë¥¼ ì‚¬ìš©í•˜ì—¬ PyTorchì™€ í˜¸í™˜ë˜ë„ë¡ ë³€í™˜ëœë‹¤.    

ìš°ë¦¬ì˜ ëª¨ë“  ëª¨ë¸ì€ AllenNLPë¥¼ ì‚¬ìš©í•˜ì—¬ PyTorchì—ì„œ êµ¬í˜„ëœë‹¤


### Casing 
ìš°ë¦¬ëŠ” NERì— cased modelsì„ ì‚¬ìš©í•˜ê³ ,    
ë‹¤ë¥¸ ëª¨ë“  ì‘ì—…ì— cased modelsì„ ì‚¬ìš©í•˜ëŠ” ë° [BERT](https://yerimoh.github.io/Lan2/)ë¥¼ ë”°ë¥¸ë‹¤.          
ìš°ë¦¬ëŠ” ë˜í•œ parsingì„ ìœ„í•´ cased modelsì„ ì‚¬ìš©í•œë‹¤.    

ì¼ë¶€ light experimentationì€ uncased modelì´ cased modelë³´ë‹¤ (ë•Œë¡œëŠ” NERì—ì„œë„) ì•½ê°„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.    




<details>
<summary>ğŸ“œ uncased vs cased </summary>
<div markdown="1">

BERT uncasedì™€ BERT casedëŠ” WordPiece í† í°í™” ë‹¨ê³„ì—ì„œ í…ìŠ¤íŠ¸ì˜ **ëŒ€ì†Œë¬¸ì ì‚¬ìš©** ì—¬ë¶€, **ì•…ì„¼íŠ¸ ë§ˆì»¤ì˜ ì¡´ì¬ ì—¬ë¶€**ì—ì„œ ë‹¤ë¦…ë‹ˆë‹¤.   
 
```
# BERT uncased
OpenGenus -> opengenus
OpÃ¨nGÃ¨nus -> opengenus 

# BERT cased
OpenGenus
OpÃ¨nGÃ¨nus
```

</div>
</details>   
 



---



## 3.4 Finetuning BERT

[base BERTì™€ ê°™ì€ ì ]
* ìš°ë¦¬ëŠ” ì˜¤ë¦¬ì§€ë„ BERTì—ì„œ ì‚¬ìš©ëœ same architecture, optimization, and hyperparameterì„ ë”°ë¥¸ë‹¤.   
* **í…ìŠ¤íŠ¸ ë¶„ë¥˜**(ì¦‰, CLSë° REL)ì˜ ê²½ìš° ```[CLS] í† í°```ì— ëŒ€í•œ ìµœì¢… BERT ë²¡í„°ë¥¼ **linear classification layer**ì— ê³µê¸‰í•œë‹¤.    
* **sequence labeling**(ì¦‰, NER ë° PICO)ì˜ ê²½ìš° ê° í† í°ì— ëŒ€í•œ ìµœì¢… BERT ë²¡í„°ë¥¼ **softmax outputì´ ìˆëŠ”  linear classification layer**ìœ¼ë¡œ ê³µê¸‰í•œë‹¤.        

[base BERTì™€ ë‹¤ë¥¸ ì ]
* additional conditional random fieldë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì— ì•½ê°„ ì°¨ì´ê°€ ìˆëŠ”ë°, ì´ëŠ” well-formed entitiesë¥¼ ë³´ì¥í•¨ìœ¼ë¡œì¨ í‰ê°€ë¥¼ ë” ì‰½ê²Œ ë§Œë“¤ì—ˆë‹¤.     
* DEPì˜ ê²½ìš°, stacked BiLSTMs ëŒ€ì‹ , BERT size 100ì˜  dependency tag, arc embeddingsê³¼ BERT ë²¡í„°ì— ëŒ€í•œ ì´ì§„ í–‰ë ¬(biaffine matrix) attentionë¥¼ ê°€ì§„ [ëª¨ë¸](https://arxiv.org/abs/1611.01734)ì„ ì‚¬ìš©í•œë‹¤.




---


## 3.5 Frozen BERT Embeddings
ë˜í•œ ë™ê²°ëœ BERT embeddings ìœ„ì—ì„œ ê°„ë‹¨í•œ taskë³„ ëª¨ë¸ì„ êµìœ¡í•˜ì—¬ ELMoì™€ ê°™ì€ pretrained contextualized word embeddingsìœ¼ë¡œ BERTì˜ ì‚¬ìš©ì„ íƒêµ¬í•œë‹¤      

* text classificationë¥¼ ìœ„í•´, BERT ë²¡í„°ì˜ ê° ë¬¸ì¥ì„ í¬ê¸° 200ì˜ 2ì¸µ BiLSTMì— ê³µê¸‰í•œ í›„,    
ì—°ê²°ëœ ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ BiLSTM ë²¡í„°ì— a multilayer perceptron (with hidden size 200)ì„ ì ìš©     
* sequence labelingì˜ ê²½ìš° ë™ì¼í•œ BiLSTM ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ê³   conditional random fieldë¥¼ ì‚¬ìš©í•˜ì—¬ well-formed predictionsì„ ë³´ì¥í•œë‹¤.      
* DEPì˜ ê²½ìš°, ìš°ë¦¬ëŠ” í¬ê¸°ê°€ 100ì¸ tag and arc embeddingsê³¼ ë‹¤ë¥¸ ì‘ì—…ê³¼ ë™ì¼í•œ BiLSTM ì„¤ì •ì„ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤.      
* BiLSTMì˜ ê¹Šì´ë‚˜ í¬ê¸° ë³€ê²½ì€ ê²°ê³¼ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ëŠ” ì•Šì•˜ë‹¤(Reimers and Gurevych, 2017).             
* Adamì„ ì‚¬ìš©í•˜ì—¬ cross entropy lossì„ ìµœì í™”í•˜ì§€ë§Œ BERT ê°€ì¤‘ì¹˜ë¥¼ ë™ê²°í•˜ê³  0.5ì˜ dropoutì„ ì ìš©í•œë‹¤.     
* ìš°ë¦¬ëŠ” ë°°ì¹˜ í¬ê¸° 32ì™€ í•™ìŠµë¥  0.001ì„ ì‚¬ìš©í•˜ì—¬ development se(10ëª…ì˜ í™˜ì)ì—ì„œ early stoppingë¥¼ ì‚¬ìš©í•˜ì—¬ trainí•œë‹¤.    
* ìš°ë¦¬ëŠ” ê´‘ë²”ìœ„í•œ hyperparameter searchì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ì§€ë§Œ,   
 ìµœì ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ëŠ” ì‘ì—…ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ê²ƒì´ë‹¤.     



----
----


# 4 Results
Table 1 summarizes the experimental results. We
observe that SCIBERT outperforms BERT-Base
on scientific tasks (+2.11 F1 with finetuning and
+2.43 F1 without)8
. We also achieve new SOTA
results on many of these tasks using SCIBERT.



í‘œ 1ì€ ì‹¤í—˜ ê²°ê³¼ë¥¼ ìš”ì•½í•œ ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” SCIBERTê°€ ê³¼í•™ì  ì‘ì—…ì—ì„œ BERT-Baseë¥¼ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•œë‹¤. (+2.11 F1ì€ ë¯¸ì„¸ ì¡°ì •ì„, +2.43 F1ì€ +2.43 F1ì€ +). ìš°ë¦¬ëŠ” ë˜í•œ SCIBERTë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ì‘ì—… ì¤‘ ë§ì€ ì‘ì—…ì—ì„œ ìƒˆë¡œìš´ SOTA ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤.


---

## 4.1 Biomedical Domain

ìš°ë¦¬ëŠ” SCIBERTê°€ ìƒë¬¼ ì˜í•™ ì‘ì—…ì—ì„œ BERTBase(+1.92F1)ë¥¼ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•œë‹¤. ë˜í•œ, SCIBERTëŠ” BC5CDRê³¼ ChemProt(Lee et al., 2019) ë° EBMNLP(Nye et al., 2018)ì—ì„œ ìƒˆë¡œìš´ SOTA ê²°ê³¼ë¥¼ ë‹¬ì„±í•œë‹¤.

SCIBERTëŠ” 3ê°œì˜ ë°ì´í„° ì„¸íŠ¸ì—ì„œ SOTAë³´ë‹¤ ì„±ëŠ¥ì´ ì•½ê°„ ë–¨ì–´ì§„ë‹¤. JNLPBAì— ëŒ€í•œ SOTA ëª¨ë¸ì€ JNLPBAë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ NER ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ í›ˆë ¨ëœ BiLSTM-CRF ì•™ìƒë¸”ì´ë‹¤(Yoon et al., 2018). NCBI ì§ˆë³‘ì˜ SOTA ëª¨ë¸ì€ ë°”ì´ì˜¤ë²„íŠ¸(Lee et al., 2019)ë¡œ, ìƒë¬¼ì˜í•™ ë…¼ë¬¸ì˜ 18B í† í°ì— ë¯¸ì„¸ ì¡°ì •ëœ BERTBaseì´ë‹¤. GENIAì— ëŒ€í•œ SOTA ê²°ê³¼ëŠ” Nguyenê³¼ Verspoor(2019)ì— ìˆìœ¼ë©°, ì´ëŠ” ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ìŒì„± ë¶€ë¶„(POS) ê¸°ëŠ¥ê³¼ í•¨ê»˜ Dozatê³¼ Manning(2017)ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤.

í‘œ 2ì—ì„œ, ìš°ë¦¬ëŠ” SCIBERT ê²°ê³¼ë¥¼ ì— í¬í•¨ëœ ë°ì´í„° ì„¸íŠ¸ì˜ í•˜ìœ„ ì§‘í•©ì— ëŒ€í•œ ë³´ê³ ëœ BIOBERT ê²°ê³¼ì™€ ë¹„êµí•œë‹¤(Lee et al., 2019). í¥ë¯¸ë¡­ê²Œë„, SCIBERTëŠ” BC5CDR ë° ChemProtì—ì„œ BIBERT ê²°ê³¼ë¥¼ ëŠ¥ê°€í•˜ë©°, ìƒë‹¹íˆ ì‘ì€ ìƒë¬¼ ì˜í•™ ë§ë­‰ì¹˜ì—ì„œ í›ˆë ¨ë˜ì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  JNLPBAì—ì„œ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤.


We observe that SCIBERT outperforms BERTBase on biomedical tasks (+1.92 F1 with finetuning and +3.59 F1 without). In addition, SCIBERT achieves new SOTA results on BC5CDR
and ChemProt (Lee et al., 2019), and EBMNLP (Nye et al., 2018).

SCIBERT performs slightly worse than SOTA
on 3 datasets. The SOTA model for JNLPBA
is a BiLSTM-CRF ensemble trained on multiple NER datasets not just JNLPBA (Yoon et al.,
2018). The SOTA model for NCBI-disease
is BIOBERT (Lee et al., 2019), which is BERTBase finetuned on 18B tokens from biomedical papers. The SOTA result for GENIA is
in Nguyen and Verspoor (2019) which uses the
model from Dozat and Manning (2017) with partof-speech (POS) features, which we do not use.



In Table 2, we compare SCIBERT results
with reported BIOBERT results on the subset of
datasets included in (Lee et al., 2019). Interesting, SCIBERT outperforms BIOBERT results on
BC5CDR and ChemProt, and performs similarly
on JNLPBA despite being trained on a substantially smaller biomedical corpus.




![image](https://user-images.githubusercontent.com/76824611/225625651-79fc0bfa-08f5-4b3f-afe0-9016e1444b63.png)

Table 2: Comparing SCIBERT with the reported
BIOBERT results on biomedical datasets.
í‘œ 2: SCIBERTì™€ ìƒë¬¼ì˜í•™ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ë³´ê³ ëœ BIOBERT ê²°ê³¼ë¥¼ ë¹„êµí•œë‹¤.


----

## 4.2 Computer Science Domain
ìš°ë¦¬ëŠ” SCIBERTê°€ ì»´í“¨í„° ê³¼í•™ ì‘ì—…ì—ì„œ BERTBase(+ë¯¸ì„¸ ì¡°ì •ì´ ìˆëŠ” ê²½ìš° +3.55F1, ì—†ëŠ” ê²½ìš° +1.13 F1)ë¥¼ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•œë‹¤. ë˜í•œ SCIBERTëŠ” ACLARC(Cohan et al., 2019)ì™€ SCIERCì˜ NER ë¶€ë¶„ì—ì„œ ìƒˆë¡œìš´ SOTA ê²°ê³¼ë¥¼ ë‹¬ì„±í•œë‹¤(Luan et al., 2018). SIERCì˜ ê´€ê³„ì˜ ê²½ìš°, ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ë£¨ì•ˆ ì™¸ ì—°êµ¬ì§„(2018)ì˜ ê²°ê³¼ì™€ ë¹„êµí•  ìˆ˜ ì—†ë‹¤. ì™œëƒí•˜ë©´ ìš°ë¦¬ëŠ” ì£¼ì–´ì§„ ê¸ˆ ì‹¤ì²´ê°€ ìˆëŠ” ê´€ê³„ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°˜ë©´ ê·¸ë“¤ì€ ê³µë™ ì‹¤ì²´ì™€ ê´€ê³„ ì¶”ì¶œì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì´ë‹¤
We observe that SCIBERT outperforms BERTBase on computer science tasks (+3.55 F1 with
finetuning and +1.13 F1 without). In addition,
SCIBERT achieves new SOTA results on ACLARC (Cohan et al., 2019), and the NER part of
SciERC (Luan et al., 2018). For relations in SciERC, our results are not comparable with those in
Luan et al. (2018) because we are performing relation classification given gold entities, while they
perform joint entity and relation extraction


---


## 4.3 Multiple Domains
ìš°ë¦¬ëŠ” SCIBERTê°€ ë‹¤ì¤‘ ë„ë©”ì¸ ì‘ì—…ì—ì„œ BERTBase(+0.49F1)ë¥¼ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•œë‹¤. ë˜í•œ, SCIBERTëŠ” Sci Citeì—ì„œ SOTAë¥¼ ëŠ¥ê°€í•œë‹¤(Cohan et al., 2019). ì¢…ì´ í•„ë“œ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ ì´ì „ì— ê²Œì‹œëœ SOTA ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.

We observe that SCIBERT outperforms BERTBase on the multidomain tasks (+0.49 F1 with
finetuning and +0.93 F1 without). In addition, SCIBERT outperforms the SOTA on Sci
Cite (Cohan et al., 2019). No prior published
SOTA results exist for the Paper Field dataset.



---
----




#  5 Discussion

## 5.1 Effect of Finetuning
We observe improved results via BERT finetuning
rather than task-specific architectures atop frozen
embeddings (+3.25 F1 with SCIBERT and +3.58
with BERT-Base, on average). For each scientific
domain, we observe the largest effects of finetuning on the computer science (+5.59 F1 with SCIBERT and +3.17 F1 with BERT-Base) and biomedical tasks (+2.94 F1 with SCIBERT and +4.61 F1
with BERT-Base), and the smallest effect on multidomain tasks (+0.7 F1 with SCIBERT and +1.14
F1 with BERT-Base). On every dataset except
BC5CDR and SciCite, BERT-Base with finetuning
outperforms (or performs similarly to) a model using frozen SCIBERT embedding

---

## 5.2 Effect of SCIVOCAB
We assess the importance of an in-domain scientific vocabulary by repeating the finetuning experiments for SCIBERT with BASEVOCAB. We
find the optimal hyperparameters for SCIBERTBASEVOCAB often coincide with those of SCIBERT-SCIVOCAB.

Averaged across datasets, we observe +0.60 F1
when using SCIVOCAB. For each scientific do-
main, we observe +0.76 F1 for biomedical tasks,
+0.61 F1 for computer science tasks, and +0.11 F1
for multidomain tasks.

Given the disjoint vocabularies (Section 2) and
the magnitude of improvement over BERT-Base
(Section 4), we suspect that while an in-domain
vocabulary is helpful, SCIBERT benefits most
from the scientific corpus pretraining.


---
----

# Related Work
Recent work on domain adaptation of BERT includes BIOBERT (Lee et al., 2019) and CLINICALBERT (Alsentzer et al., 2019; Huang et al.,
2019). BIOBERT is trained on PubMed abstracts and PMC full text articles, and CLINICALBERT is trained on clinical text from the
MIMIC-III database (Johnson et al., 2016). In
contrast, SCIBERT is trained on the full text of
1.14M biomedical and computer science papers
from the Semantic Scholar corpus (Ammar et al.,
2018). Furthermore, SCIBERT uses an in-domain
vocabulary (SCIVOCAB) while the other abovementioned models use the original BERT vocabulary (BASEVOCAB).




---
--


# 7 Conclusion and Future Work
We released SCIBERT, a pretrained language
model for scientific text based on BERT. We evaluated SCIBERT on a suite of tasks and datasets from
scientific domains. SCIBERT significantly outperformed BERT-Base and achieves new SOTA results on several of these tasks, even compared to
some reported BIOBERT (Lee et al., 2019) results
on biomedical tasks.

For future work, we will release a version of
SCIBERT analogous to BERT-Large, as well as experiment with different proportions of papers from
each domain. Because these language models are
costly to train, we aim to build a single resource
thatâ€™s useful across multiple domains.




---
---

# Acknowledgment
We thank the anonymous reviewers for their comments and suggestions. We also thank Waleed
Ammar, Noah Smith, Yoav Goldberg, Daniel
King, Doug Downey, and Dan Weld for their helpful discussions and feedback. All experiments
were performed on beaker.org and supported
in part by credits from Google Cloud








