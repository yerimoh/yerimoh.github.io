---
title: "SCIBERT: A Pretrained Language Model for Scientific Text ì •ë¦¬"
date:   2023-02-20
excerpt: "SCIBERT: A Pretrained Language Model for Scientific Text"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# Abstract

<span style="background-color:#F5F5F5">**[ë¬¸ì œ]**</span>         
ê³¼í•™ ì˜ì—­ì—ì„œ NLP ì‘ì—…ì— ëŒ€í•œ large-scale annotated dataë¥¼ ì–»ëŠ” ê²ƒì€ ì–´ë µê³  ë¹„ìš©ì´ ë§ì´ ë“ ë‹¤.     

<span style="background-color:#F5F5F5">**[í•´ê²°: SCIBERT]**</span>         
* ìš°ë¦¬ëŠ” high-qualityì˜  large-scale labeled scientific dataì˜ ë¶€ì¡±ì„ í•´ê²°í•˜ê¸° ìœ„í•´,     
 [BERT](https://yerimoh.github.io/Lan2/)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ pre-trainëœ SCIBERTë¥¼ ë§Œë“¤ì—ˆ    
 * <span style="background-color:#FFE6E6">SCIBERTëŠ” **scientific publications**ì˜  **large multi-domain corpusì—ì„œ unsupervised pretraining ì„ í™œìš©**í•˜ì—¬ downstream scientific NLP tasksì˜ ì„±ëŠ¥ì„ í–¥ìƒ</span>ì‹œí‚¨ë‹¤.     
 * ì´ëŠ” BERTì— ë¹„í•´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤.    
 * ì½”ë“œ ë° ì‚¬ì „ êµìœ¡ëœ ëª¨ë¸ì€ [ë§í¬](https://github.com/allenai/scibert/)ì—ì„œ í™•ì¸ ê°€ëŠ¥í•˜ë‹¤.     



----
----


# 1 Introduction
scientific publicationsì˜ ì–‘ì´ ë§¤ìš° ë§ì•„, NLPëŠ” ì´ëŸ¬í•œ ë¬¸ì„œì˜ ëŒ€ê·œëª¨ ì§€ì‹ ì¶”ì¶œ ë° ê¸°ê³„ íŒë…ì„ ìœ„í•œ í•„ìˆ˜ ë„êµ¬ê°€ ë¨.   

<span style="background-color:#F5F5F5">**[ë¬¸ì œ]**</span>         
* NLPì˜ ëª¨ë¸ë“¤ì„ í›ˆë ¨í•˜ë ¤ë©´ ë§ì€ ì–‘ì˜ labeled dataê°€ í•„ìš”í•œ ê²½ìš°ê°€ ë§ë‹¤.     
â¡ ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒì€ ì–´ë µê³  ë¹„ìš©ì´ ë§ì´ ë“¦     
* [ELMo](https://wikidocs.net/33930), [GPT](https://yerimoh.github.io/Lan4/) ë° [BERT](https://yerimoh.github.io/Lan2/) í†µí•´ ì•Œ ìˆ˜ ìˆë“¯ì´,  large corporaì—ì„œ ì–¸ì–´ ëª¨ë¸ì˜ **unsupervised pretraining of language modelsì€ ë§ì€ NLP ì‘ì—…ì—ì„œ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒ**ì‹œí‚¨ë‹¤.     
ì´ëŸ¬í•œ ëª¨ë¸ì€ ê° í† í°ì— ëŒ€í•œ **contextualized embeddingsì„ ë°˜í™˜**í•œë‹¤.(ì¦‰, ë¬¸ë§¥ íŒŒì•…ì´ ê°€ëŠ¥í•œ í† í°ì„ ì¤€ë‹¤.)     
â¡ ì¦‰, ì´ëŸ¬í•œ <span style="background-color:#fff5b1">**unsupervised pretrainingì„ í™œìš©**í•˜ëŠ” ê²ƒì€ ê³¼í•™ì  NLPì—ì„œì™€ ê°™ì´ **task-specific annotationsì„ ì–»ê¸° ì–´ë ¤ìš´ ê²½ìš°ì— íŠ¹íˆ ì¤‘ìš”**</span>í•´ì¡Œë‹¤.      
* âš ï¸ í•˜ì§€ë§Œ BERTì™€ ELMoëŠ” task-specificê³¼ ê°™ì€ domainì´ ì•„ë‹Œ **general domain corpora**(such as news articles and Wikipedia.)**ë¡œ í›ˆë ¨ì„ ë°›ì€ ëª¨ë¸**ì´ë‹¤. (scienceì™€ ê°™ì€ task-specific corporaë¡œ í›ˆë ¨ì„ í•˜ì§€ ì•ŠìŒ)     


<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ í•´ê²°]**</span>         
ì´ ì‘ì—…ì—ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ì—¬ë¥¼ í•œë‹¤:
* **(i)** ìš°ë¦¬ëŠ” <span style="background-color:#FFE6E6">**scientific domain**ì—ì„œ NLP tasksì˜ ë²”ìœ„ì—ì„œ ì„±ëŠ¥ì„ í–¥ìƒ</span>ì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ì…ì¦ëœ ìƒˆë¡œìš´ ë¦¬ì†ŒìŠ¤ì¸ SCIBERTë¥¼ ì¶œì‹œí•œë‹¤.    
â¡ SCIBERTëŠ” BERTë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ pretrained language modelì´ì§€ë§Œ **large corpus of scientific text**ì— ëŒ€í•´ í›ˆë ¨ë˜ì—ˆë‹¤.         
* **(ii)** ìš°ë¦¬ëŠ” frozen embeddings ìœ„ì˜  <span style="background-color:#FFE6E6">**fine tuning VS task-specific architecture**ì˜ ì„±ëŠ¥ê³¼  in-domain vocabularyì˜ ì˜í–¥ì„ ì¡°ì‚¬</span>í•˜ê¸° ìœ„í•´ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•œë‹¤       
* **(iii)** ìš°ë¦¬ëŠ” scientific domainì˜ ì¼ë ¨ì˜ ì‘ì—…ì— ëŒ€í•´ SCIBERTë¥¼ evaluateí•˜ê³ , SOTA ê²°ê³¼ë¥¼ ë‹¬ì„±í•œë‹¤.        




---
----


# 2 Methods
## Background 
* [BERT](https://yerimoh.github.io/Lan2/)ëŠ” [multilayer bidirectional Transformer](https://yerimoh.github.io/Lan2/#1-pre-training-bert)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤.      
* ê¸°ì¡´ì˜ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ë§ í•˜ëŠ” ëŒ€ì‹ ,     
BERTëŠ” [ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡(Masked Language Model, MLM)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)í•˜ê³ ,    
[ë‘ ë¬¸ì¥ì´ ì„œë¡œ ë”°ë¥´ëŠ”ì§€ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡(Next sentence prediction, NSP)](https://yerimoh.github.io/Lan2/#1-pre-training-bert)í•˜ëŠ” ë‘ ê°€ì§€ ì‘ì—…ì— ëŒ€í•´ í›ˆë ¨ëœë‹¤.    

SCIBERTëŠ” BERTì™€ **ë™ì¼í•œ ì•„í‚¤í…ì²˜**ë¥¼ ë”°ë¥´ì§€ë§Œ ëŒ€ì‹  **scientific textì— ëŒ€í•´ pretrained**ì„ ë°›ëŠ”ë‹¤.     

---

## Vocabulary
BERTëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ì˜ unsupervised tokenizationë¥¼ ìœ„í•´ [WordPiece](https://yerimoh.github.io/Lan2/#token-embeddings)ë¥¼ ì‚¬ìš©í•œë‹¤.      
vocabularyëŠ” ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” words or subword unitsë¥¼ í¬í•¨í•˜ë„ë¡ êµ¬ì„±ëœë‹¤.    

ìš°ë¦¬ëŠ” BERTì™€ í•¨ê»˜ ë°œí‘œëœ ì›ë˜ì˜ ì–´íœ˜ë¥¼ **BASEVOCAB**ë¼ê³  ë¶€ë¥¸ë‹¤.      


ìš°ë¦¬ëŠ” [SentencePiece ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/google/sentencepiece)ë¥¼ ì‚¬ìš©í•˜ì—¬,    
**scientific corpusì— ìƒˆë¡œìš´ WordPiece ì–´íœ˜ì¸ SCIVOCAB**ë¥¼ êµ¬ì„±í•œë‹¤.     
* ë³¸ ë…¼ë¬¸ì€ ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìê°€ ì—†ëŠ” ì–´íœ˜ë¥¼ ëª¨ë‘ ìƒì„±í•˜ê³  BASE VOCABì˜ í¬ê¸°ì— ë§ê²Œ ì–´íœ˜ í¬ê¸°ë¥¼ 30Kë¡œ ì„¤ì •í•œë‹¤.   
* BASEVOCABì™€ SCIVOCAB ì‚¬ì´ì˜ ê²°ê³¼ ì¼ë°˜ VOCABì˜ í† í°ê³¼ SCIVOACBì˜ ê°™ì€ í† í°ì€ 42%ë¡œ **scientific and general domain textsì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ì˜ ìƒë‹¹í•œ ì°¨ì´**ë¥¼ ë³´ì—¬ì¤€ë‹¤    


<details>
<summary>ğŸ“œ SentencePieceë€? </summary>
<div markdown="1">

ë…¼ë¬¸ : https://arxiv.org/pdf/1808.06226.pdf
êµ¬ê¸€ì´ BPE ì•Œê³ ë¦¬ì¦˜ê³¼ Unigram Language Model Tokenizerë¥¼ êµ¬í˜„í•œ ê²ƒ.   

SentencePieceëŠ” **pre-tokenizationì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ” tokenizer**ì˜ í•˜ë‚˜ë¡œ, ì–´ë–¤ ì–¸ì–´ì—ë„ ììœ ë¡­ê²Œ ì ìš©ë  ìˆ˜ ìˆê³  ì†ë„ë„ êµ‰ì¥íˆ ë¹ ë¥´ê¸° ë•Œë¬¸ì— NLPì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” tokenizerì´ë‹¤. 

SetencePieceëŠ” ê¸°ì¡´ì— ì¡´ì¬í•˜ë˜ unigram, BPEì™€ ê°™ì€ tokenizerë“¤ì„ ëª¨ë“  ì–¸ì–´ì— ëŒ€í•´ ì ìš©ì´ ê°€ëŠ¥í•˜ë„ë¡ generalizeí•˜ê³  ì•½ê°„ì˜ ì¶”ê°€ì ì¸ ê¸°ëŠ¥ë“¤ì„ ë”í•´ì„œ êµ¬í˜„í•œ ê²ƒì´ë‹¤.    
íŠ¹ë³„íˆ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ì œì‹œí–ˆë‹¤ê¸°ë³´ë‹¤ëŠ”, ê¸°ì¡´ì˜ tokenizerë“¤ì„ ì¢€ ë” ì‚¬ìš©í•˜ê¸° í¸í•˜ê³  ì„±ëŠ¥ì´ ì¢‹ê²Œ ê°œì„ í–ˆë‹¤ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤. 
 
 
**[SentencePieceì˜ íŠ¹ì§•]**         
* **Pre-tokenizationì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ”ë‹¤.**    
 ê¸°ì¡´ì˜ tokenizerë“¤ì€ ì˜ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµë˜ì—ˆë‹¤ë©´ ì˜ì–´ê°€ ë„ì–´ì“°ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ì„œ ë‹¨ì–´ë“¤ì´ ëŒ€ë¶€ë¶„ ë¶„ë¦¬ë˜ê¸° ë•Œë¬¸ì—, **ë„ì–´ì“°ê¸°ê°€ ì—†ëŠ” ì¼ë³¸ì–´ë‚˜ ì¤‘êµ­ì–´ì™€ ê°™ì€ ì–¸ì–´ë“¤ì—ëŠ” ì ìš©ë  ìˆ˜ ì—†ë‹¤**ëŠ” í•œê³„ì ì„ ê°€ì§€ê³  ìˆì—ˆë‹¤.     
 í•˜ì§€ë§Œ SentencePieceëŠ” **ë„ì–´ì“°ê¸°ë¥¼ ë‹¤ë¥¸ ì•ŒíŒŒë²³ í˜¹ì€ ê¸€ìì²˜ëŸ¼ í•˜ë‚˜ì˜ characterë¡œ ì·¨ê¸‰**í•œë‹¤.      
 â¡ ê·¸ë ‡ê¸° ë•Œë¬¸ì— ëª¨ë“  ì¢…ë¥˜ì˜ ì–¸ì–´ì— ëŒ€í•´ generalí•˜ê²Œ ì ìš©ì´ ê°€ëŠ¥í•˜ê³ , pre-tokenizationì´ í•„ìš”í•˜ì§€ ì•Šë‹¤ëŠ” ì¥ì ì„ ê°€ì§„ë‹¤.    
* **Character-coverage ì„¤ì •ì´ ê°€ëŠ¥í•˜ë‹¤.**     
 ì˜ì–´ì˜ ê²½ìš°ì—ëŠ” ì•ŒíŒŒë²³ì˜ ê°œìˆ˜ê°€ 30ê°œ ë‚´ì™¸ë¡œ ì ê¸° ë•Œë¬¸ì— ê´œì±ƒì§€ë§Œ, ì¤‘êµ­ì–´ë‚˜ ì¼ë³¸ì–´ì™€ ê°™ì€ ë¬¸ìë“¤ì˜ ê²½ìš°ì—ëŠ” êµ‰ì¥íˆ ë§ì€ ì¢…ë¥˜ì˜ í•œìë¥¼ í¬í•¨í•˜ê³  ìˆê³ , ì´ì— ë”°ë¼ **ì—„ì²­ë‚˜ê²Œ ë§ì€ ê°œìˆ˜ì˜ characterë“¤ì„ ë‹¤ ë‹¤ë£¨ì–´ì•¼ í•´ì„œ tokenizerë¥¼ í•™ìŠµí•˜ëŠ” ë°ì— ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤**ëŠ” í•œê³„ì ì„ ê°€ì§„ë‹¤.     
í•˜ì§€ë§Œ SentencePieceëŠ” ëª¨ë“  characterë¥¼ ë‹¤ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, **ë¹ˆë„ì— ë”°ë¼ ìƒìœ„ 99%ì˜ characterë“¤ë§Œ ê³ ë ¤**í•˜ëŠ” ë“±ì˜ ì˜µì…˜ì„ ì£¼ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.
* **Subword generalzationì´ ê°€ëŠ¥í•˜ë‹¤.**      
 ìš°ë¦¬ê°€ í•™ìŠµí•œ tokenizerê°€ í•­ìƒ ì™„ë²½í•˜ë‹¤ë©´ ì¢‹ê² ì§€ë§Œ, ì´ê²ƒì´ **overfittingë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±**ì„ ë°°ì œí•  ìˆ˜ëŠ” ì—†ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— SentencePieceì—ì„œëŠ” ìš°ë¦¬ê°€ í•™ìŠµí•œ tokenizerì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” tokenë“¤ë§Œì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ê°€ë”ì€ ë‹¤ë¥¸ tokenë“¤ë„ ìƒ˜í”Œë§**í•˜ë©´ì„œ ì‚¬ìš©í•˜ì—¬ overfittingí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.       
* **ì†ë„ê°€ ë¹ ë¥´ë‹¤.**      
 ì§ì ‘ êµ¬í˜„í•´ë³´ë©´ ì•Œê² ì§€ë§Œ, ì•Œê³ ë¦¬ì¦˜ì´ êµ‰ì¥íˆ ìµœì í™”ë˜ì–´ ìˆì–´ì„œ ê°™ì€ BPEì™€ unigramì„ ì‹¤í–‰í•˜ë”ë¼ë„ êµ‰ì¥íˆ ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤.

</div>
</details>  

---

## Corpus 
ìš°ë¦¬ëŠ” Semantic Scholarì˜ 1.14M ë…¼ë¬¸ì˜ ë¬´ì‘ìœ„ ìƒ˜í”Œì— ëŒ€í•´ SCIBERTë¥¼ í›ˆë ¨í•œë‹¤.      
ì´ corpusì˜ êµ¬ì„±    
* ì»´í“¨í„° ê³¼í•™ ì˜ì—­ì˜ ë…¼ë¬¸ 18%           
* ê´‘ë²”ìœ„í•œ ìƒë¬¼ ì˜í•™ ì˜ì—­ì˜ ë…¼ë¬¸ 82%       

ìš°ë¦¬ëŠ” ë…¼ë¬¸ì˜ ì „ë¬¸ì„ ì‚¬ìš©í•œë‹¤. (ìš”ì•½ë³¸ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.)    

í‰ê·  ë…¼ë¬¸ ê¸¸ì´ëŠ” 154ê°œì˜ ë¬¸ì¥(2,769ê°œì˜ í† í°)ìœ¼ë¡œ, **BERTê°€ í›ˆë ¨ëœ 3.3B í† í°ê³¼ ìœ ì‚¬í•œ corpus í¬ê¸°**ê°€ ëœë‹¤.      
ìš°ë¦¬ëŠ” ê³¼í•™ì  í…ìŠ¤íŠ¸ì— ìµœì í™”ëœ **[ScispaCy](https://github.com/allenai/SciSpaCy)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ë¶„í• í•œë‹¤.    




<details>
<summary>ğŸ“œ Semantic Scholarë€?  </summary>
<div markdown="1">


**ì í•©í•œ ë…¼ë¬¸**ì„ ì‹ ì†í•˜ê²Œ ê²€ìƒ‰í•˜ì—¬ ì´ìš©ìë“¤ì´ ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ì´í•´í•˜ëŠ”ë° ë„ì›€ì´ ë˜ê³ ì PDFíŒŒì¼ë¡œ ê°„ëµí•œ ë°œí‘œ PPTí˜•ì‹ìœ¼ë¡œ **ìš”ì•½í•œ ë‚´ìš©ì„ ì—´ëŒí•  ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ë¥¼ ì œê³µ**í•˜ê³ ìˆëŠ” ê²ƒ.  
 
 ë˜í•œ íŠ¹ì •í•œ ì—°êµ¬ë¶„ì•¼ë¥¼ ì—°êµ¬í•˜ëŠ” í•™ìë¡œì„œ ìµœê·¼ ë°œí‘œëœ ë…¼ë¬¸ì„ Semantic Scholar ë…¼ë¬¸ ê²€ìƒ‰ ì—”ì§„ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ë…¼ë¬¸ì˜ ë‚´ìš©ì„ íŒŒì•…í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆë‹¤


ì´ íšŒì‚¬ëŠ” Allen Institute for Artificial Intelligenceì—ì„œ ì„¤ë¦½í•œ ë¹„ì˜ë¦¬ ê¸°ì—…ì´ë©° ë¬´ë£Œë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³  ìˆë‹¤. **ì¸ê³µì§€ëŠ¥ì€ ë…¼ë¬¸ì˜ ì¤„ê±°ë¦¬ ë‚´ìš©ì„ íŒŒì•…í•˜ì—¬ â€œAbstractiveâ€ê¸°ë²•ì„ í†µí•´ ìš”ì•½ëœ ë‚´ìš©ì„ ìƒì„±**í•˜ê³  ìˆë‹¤.
 
 
</div>
</details>  


---
---


# Experimental Setup
## 3.1 Tasks
ìš°ë¦¬ëŠ” ì•„ë˜ì˜ core NLP tasksë¡œ í‰ê°€í•œë‹¤:        
**1.** Named Entity Recognition (NER)   
**2.** PICO Extraction (PICO)   
**3**. Text Classification (CLS)   
**4.** Relation Classification (REL)  
**5.** Dependency Parsing (DEP)   


PICO, like NER, is a sequence labeling task where
the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes
in a clinical trial paper (Kim et al., 2011). REL
is a special case of text classification where the
model predicts the type of relation expressed between two entities, which are encapsulated in the
sentence by inserted special tokens.


----




## 3.2 Datasets
For brevity, we only describe the newer datasets
here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al.,
2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab
stracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g.
Comparison, Extension, etc.) to sentences from
scientific papers that cite other papers. The Paper
Field dataset is built from the Microsoft Academic
Graph (Sinha et al., 2015)
3
and maps paper titles
to one of 7 fields of study. Each field of study
(i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples



---- 

## 3.3 Pretrained BERT Variants
### BERT-Base 
We use the pretrained weights for
BERT-Base (Devlin et al., 2019) released with the
original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model.


### SCIBERT 
We use the original BERT code to
train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4
different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB. The
two models that use BASEVOCAB are finetuned
from the corresponding BERT-Base models. The
other two models that use the new SCIVOCAB are
trained from scratch.

Pretraining BERT for long sentences can be
slow. Following the original BERT code, we set a
maximum sentence length of 128 tokens, and train
the model until the training loss stops decreasing.
We then continue training the model allowing sentence lengths up to 512 tokens.


We use a single TPU v3 with 8 cores. Training
the SCIVOCAB models from scratch on our corpus
takes 1 week5
(5 days with max length 128, then
2 days with max length 512). The BASEVOCAB
models take 2 fewer days of training because they
arenâ€™t trained from scratch.


All pretrained BERT models are converted to
be compatible with PyTorch using the pytorchtransformers library.6 All our models (Sections 3.4 and 3.5) are implemented in PyTorch using AllenNLP (Gardner et al., 2017).


### Casing 
We follow Devlin et al. (2019) in using
the cased models for NER and the uncased models
for all other tasks. We also use the cased models
for parsing. Some light experimentation showed
that the uncased models perform slightly better
(even sometimes on NER) than cased models.




---






















