---
title: "Symbolic Chain-of-Thought Distillation: Small Models Can Also â€œThinkâ€ Step-by-Step ì •ë¦¬" 
date:   2023-08-15
excerpt: "Symbolic Chain-of-Thought Distillation: Small Models Can Also â€œThinkâ€ Step-by-Step"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

   **[ì› ë…¼ë¬¸]**     
[Symbolic Chain-of-Thought Distillation: Small Models Can Also â€œThinkâ€ Step-by-Step
](https://aclanthology.org/2023.acl-long.150.pdf)


-----



# **ABSTRACT**

**[ë¬¸ì œ]**      
* Chain-of-thought prompting (e.g., â€œLetâ€™s think step-by-step")ëŠ” í° ì–¸ì–´ ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ìœ„í•œ í•©ë¦¬í™”ë¥¼ verbalizeí•˜ëŠ” ë° ê°€ì¥ ê¸°ì´ˆê°€ ëœë‹¤.     
* chain-of-thoughtëŠ” ì¶©ë¶„íˆ í° ëª¨ë¸(beyond 50B parameters)ì—ì„œë§Œ ê·¹ì ì¸ ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤.     


**[ë³¸ ë…¼ë¬¸]**    
* ê·œëª¨ê°€ ì‘ì€ ëª¨ë¸(125Mâ€”1.3B parameters)ë„ ì„±ëŠ¥í–¥ìƒì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒì„ ì¦ëª…
* ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´, SCOTD(Symbolmic Chain-of-Thought Disrelation) ì œì•ˆí•¨
* ì—¬ëŸ¬ ìƒì‹ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ì‹¤í—˜ì€ ì•„ë˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ
    * **1)** SCOTDê°€ supervised ë° few-shot settings ëª¨ë‘ì—ì„œ student ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´         
    * **2)** íŠ¹íˆ challenge ì„¸íŠ¸ì˜ ê²½ìš° teacherë¡œë¶€í„° ë§ì€ reasoning chainsì„ ìƒ˜í”Œë§í•˜ëŠ” ê²ƒì´ ë¬´ì—‡ë³´ë‹¤ ì¤‘ìš”í•¨ì„ ë³´ì—¬ì¤Œ.
    * **3)** distillation í›„, studentì˜ chain-of-thoughtsì€ í¬ê¸°ê°€ ì‘ì€ parametersì—ë„ ë¶ˆêµ¬í•˜ê³  ì¸ê°„ì— ì˜í•´ teacherì— í•„ì  ê°€ëŠ¥
* ë³¸ ë…¼ë¬¸ì€ ì•„ë˜ì™€ ê°™ì€ ì—¬ëŸ¬ ê°€ì„¤ì„ í…ŒìŠ¤íŠ¸í•œë‹¤.       
    * diversity vs. teacher likelihood vs. open-endedness       
* ìš°ë¦¬ëŠ” chain-of-thought ìƒ˜í”Œ ë° ì½”ë“œì˜ ì½”í¼ìŠ¤ë¥¼ ê³µê°œí•¨        


<details>
<summary>ğŸ“ ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation)ë€?</summary>
<div markdown="1">
  
 
ì§€ì‹ ì¦ë¥˜(knowledge distillation)ë€, ì´ë¯¸ ì‚¬ì „í•™ìŠµ ë˜ì–´ìˆëŠ” ëŒ€ê·œëª¨ ëª¨ë¸ì¸ teacherë¡œë¶€í„° ê²½ëŸ‰í™”ëœ ì••ì¶• ëª¨ë¸ì¸ studentë¡œ AIì˜ ì§€ì‹ì„ ë‚˜ëˆ„ì–´ ì£¼ëŠ” ê°œë…ì´ë‹¤.

ì¦ë¥˜(distillation)ë¼ëŠ” ë‹¨ì–´ê°€ ì•¡ì²´ í˜¼í•©ë¬¼ì„ ê°€ì—´í•˜ì—¬ ì•¡ì²´ í˜¼í•©ë¬¼ì„ ë¶„ë¦¬í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•œë‹¤ëŠ” ì ì„ ìƒê°í•˜ë©´ ê·¸ ëœ»ì´ ì‰½ê²Œ ì™€ë‹¿ëŠ”ë‹¤.



<img width="391" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/64c7eec1-b973-4ad4-95ef-3ad429ae64b8">

ì¦‰ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ NNì—ì„œ ì§€ì‹ ì¦ë¥˜ëŠ” í° ëª¨ë¸(teacher network)ë¡œë¶€í„° ì¦ë¥˜í•œ ì§€ì‹ì„ ì‘ì€ ëª¨ë¸(student netwrok)ë¡œ transferí•˜ëŠ” ê³¼ì •ì´ë‹¤.
  
</div>
</details>  

---



# 1 Introduction

**[chain-of-thought]**      
* Empirical scaling lawsì€ benchmark taskì— ëŒ€í•œ LLM(Large Language Models)ì˜ accuracyë¥¼ **ëª¨ë¸ í¬ê¸°**ì™€ **pre-training data volume**ì„ **ì¦ê°€**ì‹œì¼œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤.              
* ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ êµìœ¡ ì‹œê°„ ê°œì„ ì„ ë„˜ì–´ â€œchain-of-thought" (CoT)ì´ë¼ëŠ” inference-time ì „ëµì€, â€œLetâ€™s think step-by-step" ê³¼ ê°™ì€ í•µì‹¬ ë¬¸êµ¬ë¥¼ í†µí•´ **ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ì˜ ì–¸ì–´í™”ë¥¼ ìœ ë„**í•˜ê³  ì„±ëŠ¥ì„ ê°œì„ í•¨.      



<details>
<summary>ğŸ“ â€œchain-of-thought" (CoT)ë€ ìš©ì–´ ì •ë¦¬?</summary>
<div markdown="1">
  
Sometimes called â€œself-rationalization" or     
â€œprompting with explanations.â€


We will use these terms interchangeably
in this paper.

  
</div>
</details>  


**[chain-of-thoughtì˜ í•œê³„]**      
* ê·¸ëŸ¬ë‚˜, ìƒê°ì˜ chain-of-thought promptingëŠ” <span style="background-color:#FFE6E6">**ì¶©ë¶„í•œ ê·œëª¨ì˜ ëª¨ë¸**(ì˜ˆ: 60B ì´ìƒì˜ ë§¤ê°œ ë³€ìˆ˜(Wei et al., 2022bë¥¼ ê°€ì§„ ëª¨ë¸)**ì—ë§Œ ìœ ìš©**</span>í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŒ       



**[Solution]**     
* ì´ ì‘ì—…ì—ì„œ, ìš°ë¦¬ëŠ” <span style="background-color:#fff5b1">**ì‘ì€ ì–¸ì–´ ëª¨ë¸**ì´ **ë” í° ì–¸ì–´ ëª¨ë¸ì— ì˜í•´ chain-of-thoughtì„ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥**ì„ "taught"í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ì—°êµ¬</span>í•¨.      
* ì´ë¥¼ìœ„í•´ **SCOTD(Symbolic Chain of Thoughtemplicled)**ê°œë°œ
    * ë³¸ ë…¼ë¬¸ì€ SCoTDë¥¼ í†µí•´ ì†Œê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ í•©ë¦¬í™”ë˜ì§€ ì•Šì€ í•™ìŠµì— ë¹„í•´ 3ê°€ì§€ ìƒì‹ QA ê³¼ì œì—ì„œ ìê¸° í•©ë¦¬í™”ë¥¼ ë°°ìš°ê³  í›¨ì”¬ ë” ì˜ ìˆ˜í–‰í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•¨
    * ì´ ê²°ê³¼ëŠ” ê°ë… ë° í“¨ìƒ· ì„¤ì • ëª¨ë‘ì— ì ìš©ë˜ë©° ë‹¤ì–‘í•œ ê·œëª¨ì˜ í•™ìƒ ëª¨ë¸(1ì–µ2500ë§Œ~1.3B ë§¤ê°œ ë³€ìˆ˜)ì— ê±¸ì³ ì ìš©ë¨
    * íŠ¹íˆ contrast sets(Gardner et al. 2020), fully held-out tasks(Â§3.5)ì™€ ê°™ì€ ì–´ë ¤ìš´ ì‹œë‚˜ë¦¬ì˜¤ì— distilled chain-of-thought models(SCO3.4.5)ì„ ì ìš©í•  ë•Œ ì„±ëŠ¥ í–¥ìƒì´ ë‘ë“œëŸ¬ì§
* **ì´ í”„ë¡œì„¸ìŠ¤ì˜ ì„±ê³µ ë¹„ê²°**
    * teacher ëª¨ë¸(ì˜ˆ: 30ê°€ì§€ ì´ìœ /ì˜ˆ: ê·¸ë¦¼ 2)ì—ì„œ **ë¹„êµì  ë§ì€ ìˆ˜ì˜ rationalesë¥¼ ìƒ˜í”Œë§**í•˜ëŠ” ê²ƒ       
    * ì´ëŠ” ì˜ˆë‹¹ í•˜ë‚˜ì˜ rationalesìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” ë§ì€ ì´ì „ ê´€í–‰ê³¼ ë‹¤ë¦„. (Camburu et al., 2018; Li et al., 2022a)     
    * ì ˆì œ ì—°êµ¬ì—ì„œ, ìš°ë¦¬ëŠ” ê°œë°©í˜• í™•ë¥  ëŒ€ ë‹¤ì–‘ì„± ì¸ìŠ¤í„´ìŠ¤ì— ì˜í•´ í• ë‹¹ëœ ì½”í¼ìŠ¤ì— ëŒ€í•´ ì½”í¼ìŠ¤ë¥¼ í•„í„°ë§í•¨             

ìš°ë¦¬ëŠ” ì½”ë“œì™€ ìƒ˜í”Œë§ëœ ì‚¬ìŠ¬ì˜ ì½”í¼ìŠ¤ë¥¼ https://github.com/ allenai/cot_communicationì—ì„œ ê³µê°œí•  ê²ƒì…ë‹ˆë‹¤



Key to the success of this process is sampling a relatively large number of rationales per example from the teacher model (e.g., 30 rationales/example) (Figure 2). This is different from many prior practices that train with one rationale per example (Camburu et al., 2018; Li et al., 2022a). In ablation studies, we investigate several competing hypotheses for what are the most important factors within the corpus: we filter the corpus to CoTs that are assigned high probability by GPT-3 vs. filtering to CoTs that are diverse vs. filtering to CoTs that explain more open-ended input instances. While diversity and high probability are reasonable filters that on average perform well, the â€œnull hypothesisâ€ of random downsampling performs well, suggesting that the sheer volume of the rationales is also a key contributing factor

We will release code and the corpus of sampled chain-of-thoughts at https://github.com/ allenai/cot_distillation













