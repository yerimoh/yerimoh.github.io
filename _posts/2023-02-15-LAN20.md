---
title: "Specializing Multi-domain NMT via Penalizing Low Mutual Information ì •ë¦¬"
date:   2023-02-15
excerpt: "Specializing Multi-domain NMT via Penalizing Low Mutual Information paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



# Abstract

<span style="background-color:#F5F5F5">**[multi-domain NMTì˜ ì¤‘ìš”ì„±]**</span>         
* Multi-domain Neural Machine Translation(NMT)ì€ **ì—¬ëŸ¬ domains**ìœ¼ë¡œ **single modelì„ í›ˆë ¨**ì‹œí‚¨ë‹¤.              
ì´ì™€ ê°™ì´ í›ˆë ¨ì‹œí‚¤ë©´, í•œ model ë‚´ì—ì„œ ì—¬ëŸ¬ domainsì„ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ ë§¤ìš° íš¨ê³¼ì ì´ë‹¤.     

<span style="background-color:#F5F5F5">**[multi-domain NMTì˜ í•œê³„]**</span>      
* ì´ìƒì ì¸ Multi-domain NMTëŠ” ê³ ìœ í•œ domain íŠ¹ì„±ë“¤ì„ ë™ì‹œì— í•™ìŠµí•´ì•¼ í•˜ì§€ë§Œ,   
**domain íŠ¹ì„±ì„ íŒŒì•…í•˜ëŠ” ê²ƒ**ì€ ë§¤ìš° ì¤‘ìš”í•˜ë©°, ì–´ë ¤ìš´ ì‘ì—…(non-trivial task)ì´ë‹¤.      


<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ ëª©í‘œ]**</span>      
* ë³¸ ë…¼ë¬¸ì—ì„œëŠ” <span style="background-color:#FFE6E6">**mutual information(MI)ì˜ ë Œì¦ˆ**</span>ë¥¼ í†µí•´ **domainë³„ ì •ë³´ë¥¼ ì¡°ì‚¬**í•˜ê³ ,    
<span style="background-color:#FFE6E6">**ë‚®ì€ MIê°€ ë” ë†’ì•„ì§€ë„ë¡ íŒ¨ë„í‹°ë¥¼ ì£¼ëŠ”**</span> ìƒˆë¡œìš´ ëª©í‘œë¥¼ ì œì•ˆí•œë‹¤.      


ë³¸ ë…¼ë¬¸ì˜ ë°©ë²•ì€ í˜„ì¬ ê²½ìŸë ¥ ìˆëŠ” multi-domain NMT ëª¨ë¸ ì¤‘ì—ì„œ SOTAë¥¼ ë‹¬ì„±í–ˆë‹¤.     
ë˜í•œ, ë³¸ ë…¼ë¬¸ì€ **low MIë¥¼ ë” ë†’ê²Œ promote**í•˜ì—¬ **domain-specialized multi-domain NMTë¥¼ ì´ˆë˜**í•œë‹¤ëŠ” ê²ƒì„ ê²½í—˜ì ìœ¼ë¡œ ë³´ì—¬ì¤€ë‹¤.          

---
---

# 1 Introduction
<span style="background-color:#F5F5F5">**[multi-domain NMT]**</span>        
Multi-domain Neural Machine Translation (NMT)ì€ **single ëª¨ë¸ë¡œ ì—¬ëŸ¬ domainì„ ì²˜ë¦¬**í•  ìˆ˜ ìˆì–´ ì¸í•´ ë§¤ë ¥ì ì¸ ì£¼ì œì˜€ë‹¤.      
ì´ìƒì ìœ¼ë¡œëŠ” multi-domain NMTëŠ” ì•„ë˜ì˜ ë‘ê°€ì§€ ì§€ì‹ì„ ëª¨ë‘ ì•Œì•„ì•¼ í•œë‹¤.     
* **general knowledge (e.g., sentence structure, common words)**: ì—¬ëŸ¬ domainì˜ ê³µí†µ ì§€ì‹ (domain ê°„ parameterë¥¼ ê³µìœ í•˜ë©´ ì–»ê¸° **ì‰¬ì›€**)             
* **domain-specific knowledge (e.g., domain terminology)**: ê° ë„ë©”ì¸ì—ì„œì˜ specificí•œ ì§€ì‹ (**ì–´ë µ**)           

---

<span style="background-color:#F5F5F5">**[multi-domain NMT í•™ìŠµ í•œê³„]**</span>        
* Haddowì™€ Koehn(2012)ì€ multi-domain NMTì´ **single-domain NMTë³´ë‹¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²½ìš°**ê°€ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.         
* Pham et al.(2021)ì€ separate domain-specific adaptation modulesì´ **ì „ë¬¸ ì§€ì‹ì„ ì™„ì „íˆ ì–»ê¸°ì— ì¶©ë¶„í•˜ì§€ ì•ŠìŒ**ì„ ë³´ì—¬ì¤€ë‹¤.             


---
     
<span style="background-color:#F5F5F5">**[ê¸°ì¡´ domain-specialized ì •ë„ ì¸¡ì • ë°©ë²•]**</span>        
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” domain ì „ë¬¸ ì§€ì‹ì„ **mutual information(MI) ê´€ì ì—ì„œ ì¬í•´ì„**í•˜ê³ , ì´ë¥¼ **ê°•í™”í•˜ëŠ” ë°©ë²•**ì„ ì œì•ˆí•œë‹¤.       
<center>**$$MI(D; Y \|X)$$**</center>        
* **ì—­í• :** domainê³¼ ë²ˆì—­ëœ ë¬¸ì¥ ì‚¬ì´ì˜ ì˜ì¡´ì„±ì„ ì¸¡ì •í•œë‹¤.                 
* **X:** source sentence          
* **Y:** target sentence      
* **D:** corresponding domain     
* **$$MI(D; Y \|X)$$**: D ì™€ translation $$Y\|X$$ ì‚¬ì´ì˜ MI   

ì—¬ê¸°ì„œ, ìš°ë¦¬ëŠ” <span style="background-color:#fff5b1">**$$MI(D; Y \|X)$$** ê°€ **í´ìˆ˜ë¡** **ë²ˆì—­ì´ domain ì§€ì‹ì„ ë” ë§ì´ í†µí•©**í•œë‹¤(**domain-specific knowledgeë¥¼ ì˜ í•™ìŠµ**í–ˆë‹¤.)ê³  ê°€ì •</span>í•œë‹¤.      

**low MIëŠ” ëª¨ë¸**ì´ ë²ˆì—­ì—ì„œ **domain íŠ¹ì„±ì„ ì¶©ë¶„íˆ í™œìš©í•˜ê³  ìˆì§€ ì•Šë‹¤**ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì— **ë°”ëŒì§í•˜ì§€ ì•Šë‹¤**.     
â¡ ì¦‰, low MIëŠ” ëª¨ë¸ì€ domain-specificë¥¼ ì˜ ì•Œì§€ ëª»í•œë‹¤ê³  í•´ì„ë  ìˆ˜ ìˆë‹¤.       

ì˜ˆë¥¼ ë“¤ì–´ IT ìš©ì–´ â€˜computing totalsâ€™ ë²ˆì—­ì„ ë³´ì,           
* **low MI model(ë„ë©”ì¸ì—  ì§€ì‹ ìŠµë“ë¥  ë‚®ìŒ):** "calculation"ìœ¼ë¡œ ë²ˆì—­(ITë¶„ì•¼ì— ë§ì§€ì•ŠëŠ” ë„ˆë¬´ generalí•œ ë‹¨ì–´ì´ë‹¤)        
* **high MI(ë„ë©”ì¸ ë³„ ì§€ì‹ ìŠµë“ë¥  ë‚®ìŒ):** â€˜computing totalsâ€™ëŠ” ë²ˆì—­ì—ì„œ ì˜¬ë°”ë¥´ê²Œ ìœ ì§€ëœë‹¤ (ITë¶„ì•¼ì— ë§ëŠ” í•´ì„ì„).     

âœ¨ ë”°ë¼ì„œ **MIë¥¼ ìµœëŒ€í™”**í•˜ë©´, multi-domain NMTì´ **domain-specialized** ëœë‹¤.     

---

<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ ëª©í‘œ]**</span>      
ìœ„ì˜ ì•„ì´ë””ì–´ì— ìê·¹ì„ ë°›ì•„ <span style="background-color:#fff5b1">**low MIë¥¼ í˜ë„í‹°í™”**í•˜ì—¬ multi-domain NMTë¥¼ **specializes**í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì†Œê°œ</span>í•œë‹¤.       

ë¨¼ì € **$$MI(D; Y \|X)$$ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë„ì¶œ**í•˜ê³ ,   
**low MIë¥¼ ê°€ì§„ subword-tokensì— ë” ë§ì€ í˜ë„í‹°**ë¥¼ ë¶€ì—¬í•˜ëŠ” **ìƒˆë¡œìš´ ëª©í‘œë¥¼ ê³µì‹í™”**í•œë‹¤.       


ë³¸ ë…¼ë¬¸ì˜ ê²°ê³¼ëŠ”,   
* ì œì•ˆëœ ë°©ë²•ì´ ëª¨ë“  ë„ë©”ì¸ì—ì„œ ë²ˆì—­ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.     
* MI ì‹œê°í™”ëŠ” ìš°ë¦¬ì˜ ë°©ë²•ì´ MIë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì¥í•œë‹¤.    
* ë³¸ ë…¼ë¬¸ì€ ë³¸ ë…¼ë¬¸ì˜ ëª¨ë¸ì´ ê°•í•œ ë„ë©”ì¸ íŠ¹ì„±ì„ ê°€ì§„ ìƒ˜í”Œì—ì„œ íŠ¹íˆ ë” ì˜ ìˆ˜í–‰ëœë‹¤ëŠ” ê²ƒì„ ê´€ì°°í–ˆë‹¤.       

----

<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ main contributions]**</span>      
* ë³¸ ë…¼ë¬¸ì€ multi-domain NMTì—ì„œ MIë¥¼ ì¡°ì‚¬í•˜ê³  **low MIê°€ ë” ë†’ì€ ê°€ì¹˜ë¥¼ ê°–ë„ë¡ íŒ¨ë„í‹°ë¥¼ ì£¼ëŠ”** ìƒˆë¡œìš´ ëª©í‘œë¥¼ ì œì‹œí•œë‹¤.    
* ê´‘ë²”ìœ„í•œ ì‹¤í—˜ ê²°ê³¼ëŠ” ìš°ë¦¬ì˜ ë°©ë²•ì´ ì‹¤ì œë¡œ **high MIë¥¼ ì‚°ì¶œ**í•˜ì—¬ multi-domainì„ ìƒì„±í•œë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•œë‹¤.





----
----


# 2 Related Works

<span style="background-color:#F5F5F5">**[Multi-Domain Neural Machine Translation]**</span>   
Multi-Domain NMTëŠ” ë²ˆì—­ì„ ê°œì„ í•˜ê¸° ìœ„í•´ **domain ì •ë³´ì˜ ì ì ˆí•œ ì‚¬ìš©ì„ ê°œë°œ**í•˜ëŠ” ë° ì¤‘ì ì„ ë‘”ë‹¤.    
ì´ˆê¸° ì—°êµ¬ì—ì„œëŠ” **source domain ì •ë³´**ë¥¼ ì£¼ì…í•˜ê³ , **domain ë¶„ë¥˜ê¸°ë¥¼ ì¶”ê°€**í•˜ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ì ‘ê·¼ ë°©ì‹ì´ ìˆì—ˆë‹¤.        
* source domain ì •ë³´ë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´, Kobus ë“±(2017)ì€ ì†ŒìŠ¤ domain labelì„ ì…ë ¥ì´ ìˆëŠ” **additional tag** ë˜ëŠ” **complementary feature**ìœ¼ë¡œ ì‚½ì…í•œë‹¤.           
* Britz ë“±(2017)ì€ **domain ë¶„ë¥˜ê¸°ì˜ ê·¸ë ˆì´ë””ì–¸íŠ¸**ë¥¼ ì‚¬ìš©í•˜ì—¬ **domainë³„ ë¬¸ì¥ ì„ë² ë”©**ì„ trainí•œë‹¤


**[previous workê³¼ ë³¸ ë…¼ë¬¸ì˜ ì°¨ì´]**         
* **previous work:**     
**auxiliary classifier**ë¥¼ ì£¼ì…í•˜ê±°ë‚˜ êµ¬í˜„í•˜ì—¬ domain ì •ë³´ë¥¼ í™œìš©í•˜ì§€ë§Œ,      
* **ë³¸ ë…¼ë¬¸:**    
MI ê´€ì ì—ì„œ domain ì •ë³´ë¥¼ ë³´ê³ , domainë³„ ì§€ì‹ì„ íƒìƒ‰í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ promotesí•˜ëŠ” **ì†ì‹¤**ì„ ì œì•ˆí•œë‹¤.       


-----


<span style="background-color:#F5F5F5">**[Information-Theoretic Approaches in NMT]**</span>   
NMTì˜ Mutual informationëŠ” ì£¼ë¡œ **ë©”íŠ¸ë¦­** ë˜ëŠ” **ì†ì‹¤ í•¨ìˆ˜**ë¡œ ì‚¬ìš©ëœë‹¤.       
* ë©”íŠ¸ë¦­ì˜ ê²½ìš°,        
     * Bugliarello et al.(2020)ì€ ì–¸ì–´ ê°„ ë²ˆì—­ì˜ ì–´ë ¤ì›€ì„ ì •ëŸ‰í™”í•˜ê¸° ìœ„í•´ **cross-mutual information(XMI**)ë¥¼ ì œì•ˆí•œë‹¤.      
     * Fernandes et al. (2021)ì€ ë²ˆì—­ ì¤‘ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì˜ ì‚¬ìš©ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ **XMIë¥¼ ìˆ˜ì •**í•œë‹¤.      
* ì†ì‹¤ í•¨ìˆ˜ì˜ ê²½ìš°,         
     * Xu et al.(2021)ì€ **ë‹¨ì–´ ë§¤í•‘ ë‹¤ì–‘ì„±ì„ ê³„ì‚°**í•˜ëŠ”  **cross-mutual information(BMI)ë¥¼ ì œì•ˆ**í•˜ë©°, NMT í›ˆë ¨ì— ì¶”ê°€ë¡œ ì ìš©ëœë‹¤.          
     *  Zhang et al.(2022)ì€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **target í† í°ê³¼ source sentence ê°„ì˜ MIë¥¼ ìµœëŒ€í™”**í•˜ì—¬ ëª¨ë¸ ë²ˆì—­ì„ ê°œì„ í•œë‹¤.     


**[previous workê³¼ ë³¸ ë…¼ë¬¸ì˜ ì°¨ì´]**         
* **previous work:**         
ì¼ë°˜ì ì¸ ê¸°ê³„ ë²ˆì—­ ì‹œë‚˜ë¦¬ì˜¤ë§Œ ê³ ë ¤í•œë‹¤.     
* **ë³¸ ë…¼ë¬¸:**    
multi-domain NMTì—ì„œ **ìƒí˜¸ ì •ë³´ë¥¼ í†µí•©**í•˜ì—¬ **domain-specific ì •ë³´ë¥¼ í•™ìŠµ**í•œë‹¤ëŠ” ì ì—ì„œ ë‹¤ë¥´ë‹¤.         ì¶”ê°€ ëª¨ë¸ì˜ í›ˆë ¨ì´ í•„ìš”í•œ ë‹¤ë¥¸ ë°©ë²•ê³¼ ë‹¬ë¦¬, ìš°ë¦¬ì˜ ë°©ë²•ì€ **ë‹¨ì¼ ëª¨ë¸ ë‚´ì—ì„œ MIë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆì–´** ê³„ì‚° íš¨ìœ¨ì´ ë” ë†’ë‹¤.          

----
-----

# 3 Proposed Method

ì´ ì„¹ì…˜ì—ì„œëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì†Œê°œí•˜ì—¬ **domain-specialized model**ì„ ë§Œë“ ë‹¤.       
ìƒˆë¡œìš´ ë°©ë²•ì€ ì „ì²´ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ë‹¤.     
**1)** ë¨¼ì € multi-domain NMTì—ì„œ **MIë¥¼ ë„ì¶œ**í•œë‹¤.    
**2)** ê·¸ ë‹¤ìŒ **low MI**ê°€ ë†’ì€ ê°’ì„ ê°–ë„ë¡ **íŒ¨ë„í‹°**ì„ ì¤€ë‹¤.     



---


## 3.1 Mutual Information in Multi-Domain

<span style="background-color:#F5F5F5">**[MI]**</span>     
**Mutual Information(MI)** ëŠ” ë‘ ëœë¤ variables ê°„ì˜ **ìƒí˜¸ ì˜ì¡´ì„±ì„ ì¸¡ì •**í•œë‹¤.      
* $$MI(D; Y \| X)$$: ë‹¤ì¤‘ domain NMTì—ì„œì˜ ì˜ë¯¸ëŠ”, domain($$D$$)ê³¼ $$translation(Y \| X)$$ ì‚¬ì´ì˜ MIëŠ” **ë²ˆì—­ì— í¬í•¨ëœ ë„ë©”ì¸ë³„ ì •ë³´ì˜ ì–‘**ì„ ë‚˜íƒ€ë‚¸ë‹¤.      
* $$MI(D; Y \| X)$$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤:
![image](https://user-images.githubusercontent.com/76824611/224192782-a68bfc2f-23f5-4025-a473-456131bbe283.png)


<details>
<summary>ğŸ“œ Full Derivation of Domain-Aware Mutual Information</summary>
<div markdown="1">
  
ìŒ... ê·¸ëŸ¬ë‹ˆê¹Œ ë¶„ìëŠ” grneralí•˜ê²Œ í•´ì„ë  í™•ë£°,    
ë¶„ëª¨ëŠ” domainìª¼ê¸ë¡œ í•´ì„ ë  í™•ë¥ ì´ë¼ê³  ê°„ë‹¨í•˜ê²Œ í•´ì„œí•˜ë©´ ë  ê²ƒ ì´ë‹¤.     
     
![image](https://user-images.githubusercontent.com/76824611/224193609-a9862f33-71cc-46ae-b6c2-4ce267939b1f.png)
     
The proof from the first to the second line is
provided below.
     
     
![image](https://user-images.githubusercontent.com/76824611/224193647-1edb77ff-978b-4c75-b473-348c77edec30.png)
     
     
</div>
</details> 


ì¦‰, $$MI(D; Y \| X)$$ì˜ ìµœì¢… í˜•íƒœëŠ” **domainê³¼ domainì´ ì—†ëŠ” ë²ˆì—­ì„ ê³ ë ¤í•œ ë²ˆì—­ì˜ ë¡œê·¸ ì§€ìˆ˜**ì´ë‹¤.     


<span style="background-color:#F5F5F5">**[XMI]**</span>     
ê·¸ëŸ°ë° ì‹¤ì œ ë¶„í¬ë¥¼ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—,    
ë§¤ê°œ ë³€ìˆ˜í™”ëœ ëª¨ë¸, ì¦‰ **cross-MI (XMI)** ë¡œ ê·¼ì‚¬í•œë‹¤.       
$$XMI(D; Y \| X)$$ëŠ” ì•„ë˜ì˜ ê° ëª¨ë¸ ì¶œë ¥ê³¼ í•¨ê»˜ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.
* **generic domain-agnostic model**(generic ë„ë©”ì¸ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ëª¨ë¸(*general*ìœ¼ë¡œ ë” ì–¸ê¸‰ë˜ê³  **G**ë¡œ ì•½ì¹­ë¨)ì˜ ì¶œë ¥ì€ **$$p(Y \| X)$$** ì˜ ì ì ˆí•œ ê·¼ì‚¬ì¹˜ê°€ ë  ê²ƒì´ë‹¤.       
* **domain-adapted(*DA*)** ëª¨ë¸ ì¶œë ¥ì€ **$$p(Y \| X, D)$$** ì— ì í•©í•  ê²ƒì´ë‹¤.     
![image](https://user-images.githubusercontent.com/76824611/224195320-b3892d25-c9f4-4d82-8582-2f7b28bc975f.png)

----

# 3.2 MI-based Token Weighted Loss
To calculate XMI, we need outputs from both general and domain-adapted models. Motivated by
the success of adapters (Houlsby et al., 2019) in
multi-domain NMT (Pham et al., 2021), we assign
adapters Ï†1, Â· Â· Â· Ï†N for each domain (N number of domains) and have an extra adapter Ï†G
for general. We will denote the shared parameter
(e.g., self-attention and feed-forward layer) as Î¸.
For a source sentence x from domain d, x passes
the model twice, once through the corresponding
domain adapter, Ï†d, and the other through the general adapter, Ï†G. Then, we treat the output probability from domain adapter as pDA and from general
adapter as pG. For the i
th target token, yi
,we calculate XMI as in Eq. (3),


ì‹   


, where y<i is the target subword-tokens up to,
but excluding yi
. For simplicity, we will denote
Eq. (3) as XMI(i). Low XMI(i) means that our
domain adapted model is not thoroughly utilizing
domain information during translation. Therefore,
we weight more on the tokens with low XMI(i),
resulting in minimizing Eq. (4),


ì‹    
, where nT is the number of subword-tokens in the
target sentence.


The final loss of our method is in Eq. (7), where
Î»1 and Î»2 are hyperparameters


ì‹
