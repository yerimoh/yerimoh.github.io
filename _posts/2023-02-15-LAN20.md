---
title: "Specializing Multi-domain NMT via Penalizing Low Mutual Information 정리"
date:   2023-02-15
excerpt: "Specializing Multi-domain NMT via Penalizing Low Mutual Information paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



# Abstract

<span style="background-color:#F5F5F5">**[multi-domain NMT의 중요성]**</span>         
* Multi-domain Neural Machine Translation(NMT)은 **여러 domains**으로 **single model을 훈련**시킨다.              
이와 같이 훈련시키면, 한 model 내에서 여러 domains을 처리할 수 있어 매우 효과적이다.     

<span style="background-color:#F5F5F5">**[multi-domain NMT의 한계]**</span>      
* 이상적인 Multi-domain NMT는 고유한 domain 특성들을 동시에 학습해야 하지만,   
**domain 특성을 파악하는 것**은 매우 중요하며, 어려운 작업(non-trivial task)이다.      


<span style="background-color:#F5F5F5">**[본 논문의 목표]**</span>      
* 본 논문에서는 <span style="background-color:#FFE6E6">**mutual information(MI)의 렌즈**</span>를 통해 **domain별 정보를 조사**하고,    
<span style="background-color:#FFE6E6">**낮은 MI가 더 높아지도록 패널티를 주는**</span> 새로운 목표를 제안한다.      


본 논문의 방법은 현재 경쟁력 있는 multi-domain NMT 모델 중에서 SOTA를 달성했다.     
또한, 본 논문은 **low MI를 더 높게 promote**하여 **domain-specialized multi-domain NMT를 초래**한다는 것을 경험적으로 보여준다.          

---
---

# 1 Introduction
<span style="background-color:#F5F5F5">**[multi-domain NMT]**</span>        
Multi-domain Neural Machine Translation (NMT)은 **single 모델로 여러 domain을 처리**할 수 있어 인해 매력적인 주제였다.      
이상적으로는 multi-domain NMT는 아래의 두가지 지식을 모두 알아야 한다.     
* **general knowledge (e.g., sentence structure, common words)**: 여러 domain의 공통 지식 (domain 간 parameter를 공유하면 얻기 **쉬움**)             
* **domain-specific knowledge (e.g., domain terminology)**: 각 도메인에서의 specific한 지식 (**어렵**)           

---

<span style="background-color:#F5F5F5">**[multi-domain NMT 학습 한계]**</span>        
* Haddow와 Koehn(2012)은 multi-domain NMT이 **single-domain NMT보다 성능이 떨어지는 경우**가 있음을 보여준다.         
* Pham et al.(2021)은 separate domain-specific adaptation modules이 **전문 지식을 완전히 얻기에 충분하지 않음**을 보여준다.             


---
     
<span style="background-color:#F5F5F5">**[기존 domain-specialized 정도 측정 방법]**</span>        
본 논문에서는 domain 전문 지식을 **mutual information(MI) 관점에서 재해석**하고, 이를 **강화하는 방법**을 제안한다.       
<center>**$$MI(D; Y \|X)$$**</center>        
* **역할:** domain과 번역된 문장 사이의 의존성을 측정한다.                 
* **X:** source sentence          
* **Y:** target sentence      
* **D:** corresponding domain     
* **$$MI(D; Y \|X)$$**: D 와 translation $$Y\|X$$ 사이의 MI   

여기서, 우리는 <span style="background-color:#fff5b1">**$$MI(D; Y \|X)$$** 가 **클수록** **번역이 domain 지식을 더 많이 통합**한다(**domain-specific knowledge를 잘 학습**했다.)고 가정</span>한다.      

**low MI는 모델**이 번역에서 **domain 특성을 충분히 활용하고 있지 않다**는 것을 나타내기 때문에 **바람직하지 않다**.     
➡ 즉, low MI는 모델은 domain-specific를 잘 알지 못한다고 해석될 수 있다.       

예를 들어 IT 용어 ‘computing totals’ 번역을 보자,           
* **low MI model(도메인에  지식 습득률 낮음):** "calculation"으로 번역(IT분야에 맞지않는 너무 general한 단어이다)        
* **high MI(도메인 별 지식 습득률 낮음):** ‘computing totals’는 번역에서 올바르게 유지된다 (IT분야에 맞는 해석임).     

✨ 따라서 **MI를 최대화**하면, multi-domain NMT이 **domain-specialized** 된다.     

---

<span style="background-color:#F5F5F5">**[본 논문의 목표]**</span>      
위의 아이디어에 자극을 받아 <span style="background-color:#fff5b1">**low MI를 페널티화**하여 multi-domain NMT를 **specializes**하는 새로운 방법을 소개</span>한다.       

먼저 **$$MI(D; Y \|X)$$를 이론적으로 도출**하고,   
**low MI를 가진 subword-tokens에 더 많은 페널티**를 부여하는 **새로운 목표를 공식화**한다.       


본 논문의 결과는,   
* 제안된 방법이 모든 도메인에서 번역 품질을 향상시킨다는 것을 보여준다.     
* MI 시각화는 우리의 방법이 MI를 최대화하는 데 효과적임을 보장한다.    
* 본 논문은 본 논문의 모델이 강한 도메인 특성을 가진 샘플에서 특히 더 잘 수행된다는 것을 관찰했다.       

----

<span style="background-color:#F5F5F5">**[본 논문의 main contributions]**</span>      
* 본 논문은 multi-domain NMT에서 MI를 조사하고 **low MI가 더 높은 가치를 갖도록 패널티를 주는** 새로운 목표를 제시한다.    
* 광범위한 실험 결과는 우리의 방법이 실제로 **high MI를 산출**하여 multi-domain을 생성한다는 것을 증명한다.





----
----


# 2 Related Works

<span style="background-color:#F5F5F5">**[Multi-Domain Neural Machine Translation]**</span>   
Multi-Domain NMT는 번역을 개선하기 위해 **domain 정보의 적절한 사용을 개발**하는 데 중점을 둔다.    
초기 연구에서는 **source domain 정보**를 주입하고, **domain 분류기를 추가**하는 두 가지 주요 접근 방식이 있었다.        
* source domain 정보를 추가하기 위해, Kobus 등(2017)은 소스 domain label을 입력이 있는 **additional tag** 또는 **complementary feature**으로 삽입한다.           
* Britz 등(2017)은 **domain 분류기의 그레이디언트**를 사용하여 **domain별 문장 임베딩**을 train한다


**[previous work과 본 논문의 차이]**         
* **previous work:**     
**auxiliary classifier**를 주입하거나 구현하여 domain 정보를 활용하지만,      
* **본 논문:**    
MI 관점에서 domain 정보를 보고, domain별 지식을 탐색하기 위해 모델을 promotes하는 **손실**을 제안한다.       


-----


<span style="background-color:#F5F5F5">**[Information-Theoretic Approaches in NMT]**</span>   
NMT의 Mutual information는 주로 **메트릭** 또는 **손실 함수**로 사용된다.       
* 메트릭의 경우,        
     * Bugliarello et al.(2020)은 언어 간 번역의 어려움을 정량화하기 위해 **cross-mutual information(XMI**)를 제안한다.      
     * Fernandes et al. (2021)은 번역 중 주어진 컨텍스트의 사용을 측정하기 위해 **XMI를 수정**한다.      
* 손실 함수의 경우,         
     * Xu et al.(2021)은 **단어 매핑 다양성을 계산**하는  **cross-mutual information(BMI)를 제안**하며, NMT 훈련에 추가로 적용된다.          
     *  Zhang et al.(2022)은 컨텍스트를 기반으로 **target 토큰과 source sentence 간의 MI를 최대화**하여 모델 번역을 개선한다.     


**[previous work과 본 논문의 차이]**         
* **previous work:**         
일반적인 기계 번역 시나리오만 고려한다.     
* **본 논문:**    
multi-domain NMT에서 **상호 정보를 통합**하여 **domain-specific 정보를 학습**한다는 점에서 다르다.         추가 모델의 훈련이 필요한 다른 방법과 달리, 우리의 방법은 **단일 모델 내에서 MI를 계산할 수 있어** 계산 효율이 더 높다.          

----
-----

# 3 Proposed Method

이 섹션에서는 새로운 방법을 소개하여 **domain-specialized model**을 만든다.       
새로운 방법은 전체적으로 아래와 같다.     
**1)** 먼저 multi-domain NMT에서 **MI를 도출**한다.    
**2)** 그 다음 **low MI**가 높은 값을 갖도록 **패널티**을 준다.     



---


## 3.1 Mutual Information in Multi-Domain
NMT
Mutual Information (MI) measures a mutual dependency between two random variables. In multi domain NMT, the MI between the domain (D) and
translation (Y |X), expressed as MI(D; Y |X), represents how much domain-specific information is
contained in the translation. MI(D; Y |X) can be
written as follows:


**Mutual Information(MI)** 는 두 랜덤 variables 간의 **상호 의존성을 측정**한다.      
* MI(D; Y | X): 다중 domain NMT에서의 의미는, domain(D)과 translation(Y | X) 사이의 MI는 **번역에 포함된 도메인별 정보의 양**을 나타낸다.      
* MI(D; Y | X)는 다음과 같이 쓸 수 있다:
![image](https://user-images.githubusercontent.com/76824611/224192782-a68bfc2f-23f5-4025-a473-456131bbe283.png)


식


The full derivation can be found in Appendix B.
Note that the final form of MI(D; Y |X) is a log
quotient of the translation considering domain and
translation without domain


Since the true distributions are unknown,
we approximate them with a parameterized
model (Bugliarello et al., 2020; Fernandes et al.,
2021), namely the cross-MI (XMI). Naturally, a
generic domain-agnostic model (further referred
to as general and abbreviated as G) output would
be the appropriate approximation of p(Y |X). A
domain-adapted (further shortened as DA) model
output would be suitable for p(Y |X, D). Hence,
XMI(D; Y |X) can be expressed as Eq. (2) with
each model output.

식
