---
title: "TRPO: Trust Region Policy Optimization ì •ë¦¬"
date:   2023-02-5
excerpt: "Effective Approaches to Attention-based Neural Machine Translation paper review"
#category: [Paper]
#layout: post
#tag:
#- Paper
order: 0

comments: true
---

# ëª©ì°¨



----


# Abstract
ë³¸ ë…¼ë¬¸ì€ ë‹¨ì¡°ë¡œìš´ ê°œì„ ì´ ë³´ì¥ëœ **policies ìµœì í™”**ë¥¼ ìœ„í•œ **ë°˜ë³µì ì¸ ì ˆì°¨ë¥¼ ì„¤ëª…**í•œë‹¤.          
**ì´ë¡ ì **ìœ¼ë¡œ ì •ë‹¹í™”ëœ ì ˆì°¨ì— ëŒ€í•œ ëª‡ ê°€ì§€ ê·¼ì‚¬ì¹˜ë¥¼ ë§Œë“¤ì–´ Trust Region Policy Optimization (TRPO)ë¼ëŠ” ì‹¤ìš©ì ì¸ algorithmì„ ê°œë°œí•œë‹¤.          
ì´ algorithmì€ natural policy gradient ë°©ë²•ê³¼ ìœ ì‚¬í•˜ë©°, ì‹ ê²½ë§ê³¼ ê°™ì€ ëŒ€ê·œëª¨ nonlinear policiesì„ ìµœì í™”í•˜ëŠ” ë° íš¨ê³¼ì ì´ë‹¤.       

**[ì„±ê³¼]**     
* ìš°ë¦¬ì˜ ì‹¤í—˜ì€ ì‹œë®¬ë ˆì´ì…˜ëœ ë¡œë´‡ ìˆ˜ì˜, ê¹¡ì¶©ê¹¡ì¶© ë›°ê¸°, ê±·ê¸° ìš´ë™ í•™ìŠµ, í™”ë©´ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì•„íƒ€ë¦¬ ê²Œì„ì„ í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤.          
* ì´ë¡ ì—ì„œ ë²—ì–´ë‚œ ê·¼ì‚¬ì¹˜ì—ë„ ë¶ˆêµ¬í•˜ê³ , TRPOëŠ” hyperparametersì˜ ì¡°ì •ì´ ê±°ì˜ ì—†ì´ monotonicí•œ ê°œì„ ì„ ì œê³µí•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤.


----

# 1 Introduction
policy optimizationë¥¼ ìœ„í•œ ëŒ€ë¶€ë¶„ì˜ algorithmsì€ í¬ê²Œ ì„¸ ê°€ì§€ ë²”ì£¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤:     
**(1) policy iteration methods:** í˜„ì¬ policyì— ë”°ë¥¸ **ê°€ì¹˜ í•¨ìˆ˜ ì¶”ì •**ê³¼ **policy ê°œì„ **ì´ êµëŒ€ë¡œ ì´ë¤„ì§(Bertsekas, 2005)     
**(2) policy gradient methods:** ê¸°ëŒ€ ìˆ˜ìµë¥  (ì´ ë³´ìƒ) ëª©í‘œì˜ estimator of the gradientë¥¼ ì‚¬ìš© (Peters & Schaal, 2008a) (ê·¸ë¦¬ê³  ì´í›„ ë³¸ ë…¼ë¬¸ì— ë‚˜ì˜¤ëŠ” policy iterationê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŒ),         
**(3) derivative-free optimization methods**: cross-entropy method (CEM) ë° covariance matrix adaptation (CMA)ê³¼ ìµœì í™” ë°©ë²•ì—ì„œ ë„ì¶œë˜ë©°, ì´ëŠ” ë°˜í™˜ì„ ì •ì±… ë§¤ê°œë³€ìˆ˜ ì¸¡ë©´ì—ì„œ ìµœì í™”ë  ë¸”ë™ë°•ìŠ¤ í•¨ìˆ˜ë¡œ ì·¨ê¸‰(Szita & Lorincz, 2006). Â¨


CEM ë° CMAì™€ ê°™ì€ ì¼ë°˜ì ì¸ **ë¯¸ë¶„ ì—†ëŠ” í™•ë¥ ì  ìµœì í™” ë°©ë²•**ì€ ì´í•´í•˜ê³  êµ¬í˜„í•˜ê¸° **ê°„ë‹¨**í•˜ë©´ì„œë„ **ì¢‹ì€ ê²°ê³¼**ë¥¼ ì–»ê¸° ë•Œë¬¸ì— ë§ì€ ë¬¸ì œì—ì„œ ì„ í˜¸ëœë‹¤.    
ex) ì˜ˆë¥¼ ë“¤ì–´, í…ŒíŠ¸ë¦¬ìŠ¤ëŠ” approximate dynamic programming (ADP) ë°©ë²•ì— ëŒ€í•œ ê³ ì „ì ì¸ ë²¤ì¹˜ë§ˆí¬ ë¬¸ì œì´ì§€ë§Œ, í™•ë¥ ì  ìµœì í™” ë°©ë²•ì€ ì´ ì‘ì—…ì—ì„œ ì´ê¸°ê¸° ì–´ë µë‹¤(Gabillon et al., 2013).    

* **ì§€ì†ì ì¸ ì œì–´ ë¬¸ì œ**ì˜ ê²½ìš°, CMAì™€ ê°™ì€ ë°©ë²•ì€ ì €ì°¨ì› ë§¤ê°œ ë³€ìˆ˜í™”ê°€ ìˆëŠ” ìˆ˜ë™ ì—”ì§€ë‹ˆì–´ë§ ì •ì±… í´ë˜ìŠ¤ë¥¼ ì œê³µí•  ë•Œ ì´ë™ê³¼ ê°™ì€ ì–´ë ¤ìš´ ì‘ì—…ì— ëŒ€í•œ **ì œì–´ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë° ì„±ê³µ**í–ˆë‹¤(Wampler & Popovic, 2009).       
* ì§€ì†ì ì¸ **ê·¸ë ˆì´ë””ì–¸íŠ¸ ê¸°ë°˜ ìµœì í™”**ëŠ” ì—„ì²­ë‚œ ìˆ˜ì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê°€ì§„ ì§€ë„ í•™ìŠµ ì‘ì—…ì— ëŒ€í•œ í•™ìŠµ í•¨ìˆ˜ ê·¼ì‚¬ì¹˜ì—ì„œ ë§¤ìš° ì„±ê³µì ì´ì—ˆìœ¼ë©°, ê·¸ ì„±ê³µì„ **ê°•í™” í•™ìŠµìœ¼ë¡œ í™•ì¥í•˜ë©´ ë³µì¡í•˜ê³  ê°•ë ¥í•œ ì •ì±…ì„ íš¨ìœ¨ì ìœ¼ë¡œ í›ˆë ¨í•  ìˆ˜ ìˆë‹¤**   
â¡ ì¦‰, ê°•í™”í•™ìŠµì— **ê·¸ë ˆì´ë””ì–¸íŠ¸ ê¸°ë°˜ ìµœì í™”**ë¥¼ ì ìš©í•˜ê² ë‹¤.     


**[ë…¼ë¬¸ì˜ êµ¬ì„±]**      
* **1)** íŠ¹ì • **surrogate objective functionë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒ**ì´ non-trivial step sizesë¡œ **policy ê°œì„ ì„ ë³´ì¥**í•œë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•œë‹¤.               
* **2)** ì´ë¡ ì ìœ¼ë¡œ ì¦ëª…ëœ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ **ì¼ë ¨ì˜ ê·¼ì‚¬ì¹˜ë¥¼ ë§Œë“¤ì–´ ì‹¤ì œ ì•Œê³ ë¦¬ì¦˜ì„ ì‚°ì¶œ**í•˜ë©°, ì´ë¥¼ trust region policy optimization (TRPO)ë¼ê³  í•œë‹¤.       
* **3)** ìš°ë¦¬ëŠ” ì´ ì•Œê³ ë¦¬ë“¬ì˜ ë‘ ê°€ì§€ ë³€í˜•ì„ ì„¤ëª…í•œë‹¤.       
   * **ì²«ì§¸,** model-freeì—ì„œ ì ìš©í•  ìˆ˜ ìˆëŠ” **single-path method**    
   * **ë‘˜ì§¸,** ì‹œìŠ¤í…œì„ ì¼ë°˜ì ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ì—ì„œë§Œ ê°€ëŠ¥í•œ íŠ¹ì • ìƒíƒœë¡œ ë³µì›í•´ì•¼ í•˜ëŠ” **vine method**.        
 
ìš°ë¦¬ì˜ ì‹¤í—˜ì—ì„œ, ìš°ë¦¬ëŠ” ë™ì¼í•œ TRPO ë°©ë²•ì´ ì›ì‹œ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ ì•„íƒ€ë¦¬ ê²Œì„ì„ í•  ë¿ë§Œ ì•„ë‹ˆë¼ ìˆ˜ì˜, ê¹¡ì¶©ê¹¡ì¶© ë›°ê¸°, ê±·ê¸°ì— ëŒ€í•œ ë³µì¡í•œ ì •ì±…ì„ ë°°ìš¸ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.      



----
----

# 2 Preliminaries


## MDP

**[MDP: Markov Decision Process]**    
$$MDP: (S,A,P,r, ğœŒ_0, ğ›¾)$$   
* **$$S$$**: stateë“¤ì˜ ìœ í•œí•œ set     
* **$$A$$**: actionë“¤ì˜ ìœ í•œí•œ set      
* **$$P$$**: $$S * A * S$$ â¡ $$â„$$ì€ ì „í†µì ì¸ í™•ë¥       
 ì–´ë–¤ stateì—ì„œ ì–´ë–¤ actë¥¼ í•˜ë©´ì€ ë‹¤ìŒ stateë¡œ ê°ˆ í™•ë¥ ì´ ì–¼ë§Œì§€     
* **$$r$$**: $$S$$ â¡ $$â„$$, reward í•¨ìˆ˜    
* **$$ğœŒ_0$$**: $$S$$ â¡ $$â„$$, ì²˜ìŒ state($$s_0$$)ì˜ ë¶„í¬       
ì‹œì‘í•˜ëŠ” stateê°€ ì—¬ëŸ¬ê°œê°€ ìˆì„ ìˆ˜ ìˆëŠ”ë° ì´ ì‹œì‘ ìœ„ì¹˜ì˜ ë¶„í¬
* **$$ğ›¾ âˆˆ (0,1)$$**: discount factor
0~1ì‚¬ì´ì˜ ê°’ì„ ê°–ëŠ” Discount Factor (ì¡°ê¸ˆ ë” íš¨ìœ¨ì ì¸ pathë¥¼ ì°¾ê²Œ í•´ì¤Œ)    
í˜„ì¬ ì–»ëŠ” ë³´ìƒì´ ë¯¸ë˜ì— ì–»ëŠ” ë³´ìƒë³´ë‹¤ ì–¼ë§ˆë‚˜ ë” ì¤‘ìš”í•œì§€ ì˜ë¯¸í•˜ëŠ” ê°’ (ë¯¸ë˜ì™€ ë¹„êµí•œ í˜„ì¬ ë³´ìƒì˜ ê°€ì¹˜)         

**[Policy]**      
$$ğœ‹ : S * A â¡ [0,1]$$   


**[Î·(Ï€) expected discounted reward]**:     
* Î·(Ï€)ëŠ” policyë¥¼ í•˜ë‚˜ ì£¼ë©´ ì´ policyê°€ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ ì•Œë ¤ì¤Œ(**policyì˜ ì„±ëŠ¥**ì„ ì•Œë ¤ì¤Œ)  
* ì¦‰ ìš°ë¦¬ëŠ” ì´ë¥¼ ìµœì í™” í•´ì•¼í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤       
![image](https://user-images.githubusercontent.com/76824611/219015698-515fb90e-7a3a-45ef-937f-7eba0787b406.png)     
   * tëŠ” 0ë¶€í„° ë¬´í•œëŒ€ê¹Œì§€ ê³„ì† reward * discount factorë¥¼ ì‹œê°„ ë‹¨ìœ„ë¡œ ë”í•œë‹¤.    
     ì¦‰, ì´ ê°’ì´ **ë†’ì•„ì§ˆìˆ˜ë¡ ì¢‹ì€ ê²ƒ**ì´ë‹¤.     
   * policyì— ëŒ€í•œê±°ë‹ˆê¹Œ ì‹œì‘($$s_0$$)ë¶€í„° ìˆìŒ    
   * $$s_0$$ëŠ” $$Ï_0(s_0)$$       
   * $$a_t$$ëŠ” $$Ï€(a_t \|s_t)$$    
   * $$s_{t+1}$$ëŠ” $$P(s_{t+1} \|s_t, a_t)$$   

**action value function $$Q_Ï€$$**)    
stateì—ì„œ ì–´ë–¤ stateì—ì„œ **action**ì— ì¼ì„ í•  ë•Œ, ê·¸ë•Œë¶€í„° ì¶”ê°€ë¡œ ë°›ì„ rewardë“¤ì˜ í•©     
ì–´ëŠ íŠ¹ì • stateë¡œë¶€í„° ì •ì˜ë˜ëŠ” ê²ƒì´ë¼ $$s_{t+1}$$ë¶€í„° ì‹œì‘ë¨    
![image](https://user-images.githubusercontent.com/76824611/219017464-1cb805fa-7093-4e99-9288-02ae9e09b567.png)

**value function $$V_Ï€$$**)    
ê·¸ stateë¡œë¶€í„° ê²Œì„ì´ ëë‚ ë•Œ ê¹Œì§€ ë°›ëŠ” rewardë“¤ì˜ ê¸°ëŒ€ê°’      
ì–´ëŠ íŠ¹ì • stateë¡œë¶€í„° ì •ì˜ë˜ëŠ” ê²ƒì´ë¼ $$s_{t+1}$$ë¶€í„° ì‹œì‘ë¨    
![image](https://user-images.githubusercontent.com/76824611/219017565-85ecfd94-c0a1-49bd-963c-bbdb73afe8c4.png)

**advantage function $$A_Ï€$$**)    
adventageì´ë‹¤.      
ì˜ˆìƒí–ˆë˜ ê²ƒ(V(s))ë³´ë‹¤ ì–¼ë§ˆë‚˜ ë” ì¢‹ì€ ê°’ì¸ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê°’    
![image](https://user-images.githubusercontent.com/76824611/219017616-c3574929-587e-4de1-a0cf-fcd824da0ef1.png)

![image](https://user-images.githubusercontent.com/76824611/219017695-39a6b362-95ca-40cd-a292-4aa12f29bae1.png)

<details>
<summary>ğŸ“œ Advantage </summary>
<div markdown="1">
  
**[Advantage Actor-Critic, A2C]**    
Actor-Critic ì˜ Actor ì˜ ê¸°ëŒ€ì¶œë ¥ìœ¼ë¡œ Advantage ë¥¼ ì‚¬ìš©    

Advantage ëŠ” ì˜ˆìƒí–ˆë˜ ê²ƒ(V(s))ë³´ë‹¤ ì–¼ë§ˆë‚˜ ë” ì¢‹ì€ ê°’ì¸ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê°’   
Q(s,a)ì—ì„œ V(s)ë¥¼ ë¹¼ì¤€ ê°’ì„ ë§ì´ ì‚¬ìš©      
* Q(s,a)ë¥¼ êµ¬í•˜ëŠ” ë¶€ë¶„ì´ ë‚˜ì™€ ìˆì§€ ì•Šìœ¼ë¯€ë¡œ    
![image](https://user-images.githubusercontent.com/76824611/219020047-4b307ce0-fe1c-4e8c-8354-864a0da05c40.png)
     - í˜„ì¬ ê°€ì¹˜: ë³´ìƒ      
     - ë¯¸ë˜ ê°€ì¹˜: ë‹¤ìŒ ìƒíƒœì˜ ê°€ì¹˜ í•¨ìˆ˜ V(sâ€²)     
     - ì‹¤ì œê°€ì¹˜: Q(s,a)     
       ![image](https://user-images.githubusercontent.com/76824611/219020070-e94703dc-0b9b-4511-b05d-28f3c8f9e7c7.png)
* ì¦‰ Advantage ë¥¼ êµ¬í•˜ëŠ” ì‹ì€ ì•„ë˜ì™€ ê°™ì´ ë°”ë€œ
  ![image](https://user-images.githubusercontent.com/76824611/219020087-1922b9a3-a60f-40c5-b1af-c8847afae234.png)

  - Critic Network ì—ì„œ ê³„ì‚°í•œ V(s) ê°’ì´ Actor Network ì˜ ê³„ì‚°ì—ë„ ì˜í–¥ì„ ë¼ì¹˜ê²Œ ë¨

**ë‹¨ì :** íŠ¹íˆ DQN ì²˜ëŸ¼ replay buffer ë¥¼ í™œìš©í•˜ì§€ ì•Šê³  íƒìƒ‰ ë°ì´í„°ë¥¼ ì¦‰ì‹œ í•™ìŠµì— ì´ìš©í•˜ê¸° ë•Œë¬¸ì— ì˜ëª»ëœ ê¸¸ë¡œ í•™ìŠµí–ˆì„ ê²½ìš° ê²°ê³¼ê°’ì´ ì•ˆì¢‹ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ
  
</div>
</details> 


---

## Kakade & Langford    
$$ğœ‹Â Ìƒ
$$ kakade&langford(2002)ì™€ í•´ë‹¹ ë…¼ë¬¸ì˜ appendix Aì— ë”°ë¼,    
ë‹¤ë¥¸ policy Ï€âˆ¼ì˜ expected rewardë¥¼ êµ¬í•˜ê¸° ì‰¬ìš´ policy Ï€ë¥¼ ì´ìš©í•´ êµ¬í•  ìˆ˜ ìˆëŠ” ë‹¤ìŒì˜ ì‹ì´ ìœ ë„ë˜ì—ˆìœ¼ë©°,  


![image](https://user-images.githubusercontent.com/76824611/219031058-b0171e63-870d-4ded-add4-6dcd9c777012.png)


ë‚˜ì¤‘ì— ì¶”ê°€   (ê·¸ë¦¼)          


ìš°ì„  Ï€ì˜ ì„±ëŠ¥ì„ êµ¬í•˜ê³  ê·¸ ê°’ì—ë‹¤ê°€ adventageì˜ ê¸°ëŒ€ê°’ì´ë‹¤.  
$$ğœ‹ Ìƒ $$ë‘ Ï€ëŠ” ë‹¤ë¥¸ policyì´ë‹¤.     
$$ğœ‹ Ìƒ $$ì˜ ì„±ëŠ¥ì„ ì•Œê³  ì‹¶ìœ¼ë©´ ìš°ì„  Ï€ë¥¼ êµ¬í•˜ê³  ê·¸ ê°’ì—ë‹¤ê°€ $$ğœ‹ Ìƒ $$ë¡œ ë¶€í„° ìƒ˜í”Œë§í•œë‹¤.      
$$s_0$$ì™€ $$a_0$$ë¥¼ $$ğœ‹ Ìƒ $$ë¡œ samplingí•˜ê³ ìˆë‹¤.       
ì¦‰ Ï€ë¥¼ ë”°ëì„ ë•Œì˜ adventageë¥¼ ê°€ì¤‘ì¹˜í•´ì„œ ë”í•´ì¤€ í›„ì— $$ğœ‹ Ìƒ $$ë¥¼ ë”°ë¼ë‹¤ë‹ˆë©´ $$ğœ‹ Ìƒ $$ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.    

ì‹œê°„ì— ë”°ë¥¸ í‘œí˜„ë²•ì„ stateì— ë”°ë¥¸ í‘œí˜„ë²•ìœ¼ë¡œ ë³€í™˜í•œ ì‹ì´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.      

q valueëŠ” 2 + 0 ì¸ë° ì „ ë…¸ë“œê°€ 3ì´ë¯€ë¡œ advantageê°€ -1ì´ë‹¤.   

ì–´ë–¤ ë‹¤ë¥¸ policyë¥¼ ì•Œê³  ì‹¶ì„ë• ë‚´ê°€ ì•„ëŠ” policyë¡œ ë¶€í„° ë„ì¶œí•´ë‚¼ ìˆ˜ ìˆë‹¤.



ë‚˜ì¤‘ì— ì¶”ê°€   (ì¦ëª…)          



**[ì‹ì˜ ì˜ë¯¸]**     
![image](https://user-images.githubusercontent.com/76824611/219037332-d971cdef-4bfe-408d-85e5-ef90f8383342.png)
1. ë§Œì¼ ëª¨ë“  state s ì— ëŒ€í•´ ì•„ë˜ì™€ ê°™ë‹¤ë©´, policyì˜ ì„±ëŠ¥ì¸ Î·ì´ ì¦ê°€í•˜ëŠ”ê²Œ ë³´ì¥ëœë‹¤ëŠ” ëœ»ì„     
ì¦‰, Î·(Ï€)ê°€ **policyì˜ ì„±ëŠ¥**ì´ë¯€ë¡œ ì„±ëŠ¥ì´ ì–‘ìˆ˜ì´ë©´ ì¢‹ë‹¤ëŠ” ëœ»ì´ë‹¤.     
![image](https://user-images.githubusercontent.com/76824611/219037771-f6f64cbf-b28f-4445-9f2b-39ae5caee118.png)
2. ê·¸ëŸ¬ë‚˜, ì‹¤ì œì—ì„  $$A_Ï€$$ë¥¼ ì¸¡ì •í•  ë•Œ ë‰´ëŸ´ë„·ì˜ approximation errorë¡œ ì¸í•´ ì•„ë˜ì™€ ê°™ì€ state, action ìŒì´ ì„ì—¬ìˆì„ ê²ƒì´ë‹¤.     
![image](https://user-images.githubusercontent.com/76824611/219039044-3271cb38-052d-4c31-9324-792c2e7a1ac1.png)     
â¡ ì¦‰, ì´ëŠ” policyê°€ ë³´ì¥ë¨ì„ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤.       


íŠ¹ì •í•œ êµ¬ì—­ì¸ trust regionì—ì„œë§Œ ì•„ë˜ì‹ì´ ì¹˜í™˜ì´ ê°€ëŠ¥í•´ì§„ë‹¤.        
![image](https://user-images.githubusercontent.com/76824611/219067718-92e6e9bf-f209-4086-b02b-68cca8630922.png)


ì™œëƒí•˜ë©´ ì•„ë˜ì‹ê³¼ ê°™ê¸° ë•Œë¬¸ì´ë‹¤.   
![image](https://user-images.githubusercontent.com/76824611/219041971-3b814283-b864-42a7-966b-7a31c6db4bc3.png)
* ì´ ì‹ì˜ ì˜ë¯¸ëŠ” LÏ€ì™€ Î·ì´ first - orderë¡œ ê·¼ì‚¬í•˜ë©´ ê°™ë‹¤ëŠ” ëœ»ì´ë‹¤.   
* ì¶©ë¶„íˆ ì‘ì€ stepë§Œí¼ policyë¥¼ updateí•˜ë©´:   
  ![image](https://user-images.githubusercontent.com/76824611/219051105-6bfe2cc6-74b3-4878-bff9-6eefa548f260.png)
  ![image](https://user-images.githubusercontent.com/76824611/219068352-fbe2fdd0-4701-4171-8078-c3632b6e0bc1.png)
  
**[ìœ„ ì‹ì˜ ë‹¨ì ]**    
í•˜ì§€ë§Œ ì´ ì‹ì€ ê·¸ stepì´ ì–¼ë§Œí¼ ì‘ì•„ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•´ì„œëŠ” ì•Œë ¤ì£¼ì§€ ì•ŠìŒ      
ê·¸ë˜ì„œ Kakade & LangfordëŠ” ì–´ë–¤ policy update ë°©ë²•ë¡ ì¸ **<span style="background-color:#DCFFE4"> Conservative policy iteration</span>** ì„ ì œì‹œí•œë‹¤.     



**<span style="background-color:#DCFFE4"> [Conservative policy iteration]</span>**    
old policyì™€ ìƒˆë¡œìš´ policyë¥¼ ì¼ì • ë¹„ìœ¨ë¡œ ì„ì–´ì“°ëŠ” ë°©ë²•ë¡       
aê°€ 1ì— ê°€ê¹Œìš°ë©´ ìƒˆë¡œìš´ê±¸ ë§ì´ì”€      
![image](https://user-images.githubusercontent.com/76824611/219072316-71b4b193-3a30-4b32-bc14-6883545cb092.png)
ì´ë•Œì˜ lower boundë¥¼ Kakade & Langfordê°€ ìœ ë„í–ˆë‹¤.      
![image](https://user-images.githubusercontent.com/76824611/219072594-fedbdbf9-9ba3-4ae7-8f34-ada224f7b876.png)

â¡ í•˜ì§€ë§Œ ì´ ì‹ì€ mixture policyì—ì„œë§Œ ì“¸ ìˆ˜ ìˆë‹¤.   
ì¦‰ ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë˜ì§€ ëª»í•œë‹¤.    


ì¢€ ë” **generalí•˜ê²Œ í†µìš©ë˜ëŠ” ë°©ë²•ë¡ **ì´ í•„ìš”í•˜ë‹¤.    



----
----

# 3 Monotonic Improvement Guarantee for General Stochastic Policies
ìœ„ì˜ ë‹¨ì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ generalí•œ improvement guaranteeë¥¼ ë§Œë“¤ì—ˆë‹¤.   

1) Total variatin divergence ê°œë…ì„ ë„ì…   
2)

