---
title: "TRPO: Trust Region Policy Optimization ì •ë¦¬"
date:   2023-02-5
excerpt: "Effective Approaches to Attention-based Neural Machine Translation paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# ëª©ì°¨



----


# Abstract
ë³¸ ë…¼ë¬¸ì€ ë‹¨ì¡°ë¡œìš´ ê°œì„ ì´ ë³´ì¥ëœ **policies ìµœì í™”**ë¥¼ ìœ„í•œ **ë°˜ë³µì ì¸ ì ˆì°¨ë¥¼ ì„¤ëª…**í•œë‹¤.          
**ì´ë¡ ì **ìœ¼ë¡œ ì •ë‹¹í™”ëœ ì ˆì°¨ì— ëŒ€í•œ ëª‡ ê°€ì§€ ê·¼ì‚¬ì¹˜ë¥¼ ë§Œë“¤ì–´ Trust Region Policy Optimization (TRPO)ë¼ëŠ” ì‹¤ìš©ì ì¸ algorithmì„ ê°œë°œí•œë‹¤.          
ì´ algorithmì€ natural policy gradient ë°©ë²•ê³¼ ìœ ì‚¬í•˜ë©°, ì‹ ê²½ë§ê³¼ ê°™ì€ ëŒ€ê·œëª¨ nonlinear policiesì„ ìµœì í™”í•˜ëŠ” ë° íš¨ê³¼ì ì´ë‹¤.       

**[ì„±ê³¼]**     
* ìš°ë¦¬ì˜ ì‹¤í—˜ì€ ì‹œë®¬ë ˆì´ì…˜ëœ ë¡œë´‡ ìˆ˜ì˜, ê¹¡ì¶©ê¹¡ì¶© ë›°ê¸°, ê±·ê¸° ìš´ë™ í•™ìŠµ, í™”ë©´ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì•„íƒ€ë¦¬ ê²Œì„ì„ í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤.          
* ì´ë¡ ì—ì„œ ë²—ì–´ë‚œ ê·¼ì‚¬ì¹˜ì—ë„ ë¶ˆêµ¬í•˜ê³ , TRPOëŠ” hyperparametersì˜ ì¡°ì •ì´ ê±°ì˜ ì—†ì´ monotonicí•œ ê°œì„ ì„ ì œê³µí•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤.


----

# 1 Introduction
policy optimizationë¥¼ ìœ„í•œ ëŒ€ë¶€ë¶„ì˜ algorithmsì€ í¬ê²Œ ì„¸ ê°€ì§€ ë²”ì£¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤:     
**(1) policy iteration methods:** í˜„ì¬ policyì— ë”°ë¥¸ **ê°€ì¹˜ í•¨ìˆ˜ ì¶”ì •**ê³¼ **policy ê°œì„ **ì´ êµëŒ€ë¡œ ì´ë¤„ì§(Bertsekas, 2005)     
**(2) policy gradient methods:** ê¸°ëŒ€ ìˆ˜ìµë¥  (ì´ ë³´ìƒ) ëª©í‘œì˜ estimator of the gradientë¥¼ ì‚¬ìš© (Peters & Schaal, 2008a) (ê·¸ë¦¬ê³  ì´í›„ ë³¸ ë…¼ë¬¸ì— ë‚˜ì˜¤ëŠ” policy iterationê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŒ),         
**(3) derivative-free optimization methods**: cross-entropy method (CEM) ë° covariance matrix adaptation (CMA)ê³¼ ìµœì í™” ë°©ë²•ì—ì„œ ë„ì¶œë˜ë©°, ì´ëŠ” ë°˜í™˜ì„ ì •ì±… ë§¤ê°œë³€ìˆ˜ ì¸¡ë©´ì—ì„œ ìµœì í™”ë  ë¸”ë™ë°•ìŠ¤ í•¨ìˆ˜ë¡œ ì·¨ê¸‰(Szita & Lorincz, 2006). Â¨


CEM ë° CMAì™€ ê°™ì€ ì¼ë°˜ì ì¸ **ë¯¸ë¶„ ì—†ëŠ” í™•ë¥ ì  ìµœì í™” ë°©ë²•**ì€ ì´í•´í•˜ê³  êµ¬í˜„í•˜ê¸° **ê°„ë‹¨**í•˜ë©´ì„œë„ **ì¢‹ì€ ê²°ê³¼**ë¥¼ ì–»ê¸° ë•Œë¬¸ì— ë§ì€ ë¬¸ì œì—ì„œ ì„ í˜¸ëœë‹¤.    
ex) ì˜ˆë¥¼ ë“¤ì–´, í…ŒíŠ¸ë¦¬ìŠ¤ëŠ” approximate dynamic programming (ADP) ë°©ë²•ì— ëŒ€í•œ ê³ ì „ì ì¸ ë²¤ì¹˜ë§ˆí¬ ë¬¸ì œì´ì§€ë§Œ, í™•ë¥ ì  ìµœì í™” ë°©ë²•ì€ ì´ ì‘ì—…ì—ì„œ ì´ê¸°ê¸° ì–´ë µë‹¤(Gabillon et al., 2013).    

* **ì§€ì†ì ì¸ ì œì–´ ë¬¸ì œ**ì˜ ê²½ìš°, CMAì™€ ê°™ì€ ë°©ë²•ì€ ì €ì°¨ì› ë§¤ê°œ ë³€ìˆ˜í™”ê°€ ìˆëŠ” ìˆ˜ë™ ì—”ì§€ë‹ˆì–´ë§ ì •ì±… í´ë˜ìŠ¤ë¥¼ ì œê³µí•  ë•Œ ì´ë™ê³¼ ê°™ì€ ì–´ë ¤ìš´ ì‘ì—…ì— ëŒ€í•œ **ì œì–´ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë° ì„±ê³µ**í–ˆë‹¤(Wampler & Popovic, 2009).       
* ì§€ì†ì ì¸ **ê·¸ë ˆì´ë””ì–¸íŠ¸ ê¸°ë°˜ ìµœì í™”**ëŠ” ì—„ì²­ë‚œ ìˆ˜ì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê°€ì§„ ì§€ë„ í•™ìŠµ ì‘ì—…ì— ëŒ€í•œ í•™ìŠµ í•¨ìˆ˜ ê·¼ì‚¬ì¹˜ì—ì„œ ë§¤ìš° ì„±ê³µì ì´ì—ˆìœ¼ë©°, ê·¸ ì„±ê³µì„ **ê°•í™” í•™ìŠµìœ¼ë¡œ í™•ì¥í•˜ë©´ ë³µì¡í•˜ê³  ê°•ë ¥í•œ ì •ì±…ì„ íš¨ìœ¨ì ìœ¼ë¡œ í›ˆë ¨í•  ìˆ˜ ìˆë‹¤**   
â¡ ì¦‰, ê°•í™”í•™ìŠµì— **ê·¸ë ˆì´ë””ì–¸íŠ¸ ê¸°ë°˜ ìµœì í™”**ë¥¼ ì ìš©í•˜ê² ë‹¤.     


**[ë…¼ë¬¸ì˜ êµ¬ì„±]**      
* **1)** íŠ¹ì • **surrogate objective functionë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒ**ì´ non-trivial step sizesë¡œ **policy ê°œì„ ì„ ë³´ì¥**í•œë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•œë‹¤.               
* **2)** ì´ë¡ ì ìœ¼ë¡œ ì¦ëª…ëœ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ **ì¼ë ¨ì˜ ê·¼ì‚¬ì¹˜ë¥¼ ë§Œë“¤ì–´ ì‹¤ì œ ì•Œê³ ë¦¬ì¦˜ì„ ì‚°ì¶œ**í•˜ë©°, ì´ë¥¼ trust region policy optimization (TRPO)ë¼ê³  í•œë‹¤.       
* **3)** ìš°ë¦¬ëŠ” ì´ ì•Œê³ ë¦¬ë“¬ì˜ ë‘ ê°€ì§€ ë³€í˜•ì„ ì„¤ëª…í•œë‹¤.       
   * **ì²«ì§¸,** model-freeì—ì„œ ì ìš©í•  ìˆ˜ ìˆëŠ” **single-path method**    
   * **ë‘˜ì§¸,** ì‹œìŠ¤í…œì„ ì¼ë°˜ì ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ì—ì„œë§Œ ê°€ëŠ¥í•œ íŠ¹ì • ìƒíƒœë¡œ ë³µì›í•´ì•¼ í•˜ëŠ” **vine method**.        
 
ìš°ë¦¬ì˜ ì‹¤í—˜ì—ì„œ, ìš°ë¦¬ëŠ” ë™ì¼í•œ TRPO ë°©ë²•ì´ ì›ì‹œ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ ì•„íƒ€ë¦¬ ê²Œì„ì„ í•  ë¿ë§Œ ì•„ë‹ˆë¼ ìˆ˜ì˜, ê¹¡ì¶©ê¹¡ì¶© ë›°ê¸°, ê±·ê¸°ì— ëŒ€í•œ ë³µì¡í•œ ì •ì±…ì„ ë°°ìš¸ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.      



----
----

# 2 Preliminaries


## MDP

**[MDP: Markov Decision Process]**    
$$MDP: (S,A,P,r, ğœŒ_0, ğ›¾)$$   
* **$$S$$**: stateë“¤ì˜ ìœ í•œí•œ set     
* **$$A$$**: actionë“¤ì˜ ìœ í•œí•œ set      
* **$$P$$**: $$S * A * S$$ â¡ $$â„$$ì€ ì „í†µì ì¸ í™•ë¥       
 ì–´ë–¤ stateì—ì„œ ì–´ë–¤ actë¥¼ í•˜ë©´ì€ ë‹¤ìŒ stateë¡œ ê°ˆ í™•ë¥ ì´ ì–¼ë§Œì§€     
* **$$r$$**: $$S$$ â¡ $$â„$$, reward í•¨ìˆ˜    
* **$$ğœŒ_0$$**: $$S$$ â¡ $$â„$$, ì²˜ìŒ state($$s_0$$)ì˜ ë¶„í¬       
ì‹œì‘í•˜ëŠ” stateê°€ ì—¬ëŸ¬ê°œê°€ ìˆì„ ìˆ˜ ìˆëŠ”ë° ì´ ì‹œì‘ ìœ„ì¹˜ì˜ ë¶„í¬
* **$$ğ›¾ âˆˆ (0,1)$$**: discount factor
0~1ì‚¬ì´ì˜ ê°’ì„ ê°–ëŠ” Discount Factor (ì¡°ê¸ˆ ë” íš¨ìœ¨ì ì¸ pathë¥¼ ì°¾ê²Œ í•´ì¤Œ)    
í˜„ì¬ ì–»ëŠ” ë³´ìƒì´ ë¯¸ë˜ì— ì–»ëŠ” ë³´ìƒë³´ë‹¤ ì–¼ë§ˆë‚˜ ë” ì¤‘ìš”í•œì§€ ì˜ë¯¸í•˜ëŠ” ê°’ (ë¯¸ë˜ì™€ ë¹„êµí•œ í˜„ì¬ ë³´ìƒì˜ ê°€ì¹˜)         

**[Policy]**      
$$ğœ‹ : S * A â¡ [0,1]$$   


**[Î·(Ï€) expected discounted reward]**:     
* Î·(Ï€)ëŠ” policyë¥¼ í•˜ë‚˜ ì£¼ë©´ ì´ policyê°€ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ ì•Œë ¤ì¤Œ(**policyì˜ ì„±ëŠ¥**ì„ ì•Œë ¤ì¤Œ)  
* ì¦‰ ìš°ë¦¬ëŠ” ì´ë¥¼ ìµœì í™” í•´ì•¼í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤       
![image](https://user-images.githubusercontent.com/76824611/219015698-515fb90e-7a3a-45ef-937f-7eba0787b406.png)     
   * tëŠ” 0ë¶€í„° ë¬´í•œëŒ€ê¹Œì§€ ê³„ì† reward * discount factorë¥¼ ì‹œê°„ ë‹¨ìœ„ë¡œ ë”í•œë‹¤.    
     ì¦‰, ì´ ê°’ì´ **ë†’ì•„ì§ˆìˆ˜ë¡ ì¢‹ì€ ê²ƒ**ì´ë‹¤.     
   * policyì— ëŒ€í•œê±°ë‹ˆê¹Œ ì‹œì‘($$s_0$$)ë¶€í„° ìˆìŒ    
   * $$s_0$$ëŠ” $$Ï_0(s_0)$$       
   * $$a_t$$ëŠ” $$Ï€(a_t|s_t)$$    
   * $$s_{t+1}$$ëŠ” $$P(s_{t+1}|s_t, a_t)$$   

**action value function $$Q_Ï€$$**)    
stateì—ì„œ ì–´ë–¤ stateì—ì„œ **action**ì— ì¼ì„ í•  ë•Œ, ê·¸ë•Œë¶€í„° ì¶”ê°€ë¡œ ë°›ì„ rewardë“¤ì˜ í•©     
ì–´ëŠ íŠ¹ì • stateë¡œë¶€í„° ì •ì˜ë˜ëŠ” ê²ƒì´ë¼ $$s_{t+1}$$ë¶€í„° ì‹œì‘ë¨    
![image](https://user-images.githubusercontent.com/76824611/219017464-1cb805fa-7093-4e99-9288-02ae9e09b567.png)

**value function $$V_Ï€$$**)    
ê·¸ stateë¡œë¶€í„° ê²Œì„ì´ ëë‚ ë•Œ ê¹Œì§€ ë°›ëŠ” rewardë“¤ì˜ ê¸°ëŒ€ê°’      
ì–´ëŠ íŠ¹ì • stateë¡œë¶€í„° ì •ì˜ë˜ëŠ” ê²ƒì´ë¼ $$s_{t+1}$$ë¶€í„° ì‹œì‘ë¨    
![image](https://user-images.githubusercontent.com/76824611/219017565-85ecfd94-c0a1-49bd-963c-bbdb73afe8c4.png)

**advantage function $$A_Ï€$$**)    
adventageì´ë‹¤.      
ì˜ˆìƒí–ˆë˜ ê²ƒ(V(s))ë³´ë‹¤ ì–¼ë§ˆë‚˜ ë” ì¢‹ì€ ê°’ì¸ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê°’    
![image](https://user-images.githubusercontent.com/76824611/219017616-c3574929-587e-4de1-a0cf-fcd824da0ef1.png)

![image](https://user-images.githubusercontent.com/76824611/219017695-39a6b362-95ca-40cd-a292-4aa12f29bae1.png)

<details>
<summary>ğŸ“œ Advantage </summary>
<div markdown="1">
  
**[Advantage Actor-Critic, A2C]**    
Actor-Critic ì˜ Actor ì˜ ê¸°ëŒ€ì¶œë ¥ìœ¼ë¡œ Advantage ë¥¼ ì‚¬ìš©    

Advantage ëŠ” ì˜ˆìƒí–ˆë˜ ê²ƒ(V(s))ë³´ë‹¤ ì–¼ë§ˆë‚˜ ë” ì¢‹ì€ ê°’ì¸ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê°’   
Q(s,a)ì—ì„œ V(s)ë¥¼ ë¹¼ì¤€ ê°’ì„ ë§ì´ ì‚¬ìš©      
* Q(s,a)ë¥¼ êµ¬í•˜ëŠ” ë¶€ë¶„ì´ ë‚˜ì™€ ìˆì§€ ì•Šìœ¼ë¯€ë¡œ    
![image](https://user-images.githubusercontent.com/76824611/219020047-4b307ce0-fe1c-4e8c-8354-864a0da05c40.png)
     - í˜„ì¬ ê°€ì¹˜: ë³´ìƒ      
     - ë¯¸ë˜ ê°€ì¹˜: ë‹¤ìŒ ìƒíƒœì˜ ê°€ì¹˜ í•¨ìˆ˜ V(sâ€²)     
     - ì‹¤ì œê°€ì¹˜: Q(s,a)     
       ![image](https://user-images.githubusercontent.com/76824611/219020070-e94703dc-0b9b-4511-b05d-28f3c8f9e7c7.png)
* ì¦‰ Advantage ë¥¼ êµ¬í•˜ëŠ” ì‹ì€ ì•„ë˜ì™€ ê°™ì´ ë°”ë€œ
  ![image](https://user-images.githubusercontent.com/76824611/219020087-1922b9a3-a60f-40c5-b1af-c8847afae234.png)

  - Critic Network ì—ì„œ ê³„ì‚°í•œ V(s) ê°’ì´ Actor Network ì˜ ê³„ì‚°ì—ë„ ì˜í–¥ì„ ë¼ì¹˜ê²Œ ë¨

**ë‹¨ì :** íŠ¹íˆ DQN ì²˜ëŸ¼ replay buffer ë¥¼ í™œìš©í•˜ì§€ ì•Šê³  íƒìƒ‰ ë°ì´í„°ë¥¼ ì¦‰ì‹œ í•™ìŠµì— ì´ìš©í•˜ê¸° ë•Œë¬¸ì— ì˜ëª»ëœ ê¸¸ë¡œ í•™ìŠµí–ˆì„ ê²½ìš° ê²°ê³¼ê°’ì´ ì•ˆì¢‹ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ
  
</div>
</details> 


---

## Kakade & Langford    
ë¬´í•œ í™•ì¥ í• ì¸ ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì •(MDP)ì„ ìƒê°í•´ ë³´ì.    
ì—¬ê¸°ì„œ SëŠ” ìƒíƒœì˜ ìœ í•œ ì§‘í•©, AëŠ” ì‘ìš©ì˜ ìœ í•œ ì§‘í•©, P : S Ã— A Ã— S â†’ Rì€ ì „ì´ í™•ë¥  ë¶„í¬, r : S â†’ Rì€ ë³´ìƒ í•¨ìˆ˜, R : 0 : S â†’ Rì€ ì´ˆê¸° ìƒíƒœì˜ ë¶„í¬, ê·¸ë¦¬ê³  (Î¸ 0)ëŠ” 1ì´ë‹¤í• ì¸ìœ¨.   

Ï€ê°€ í™•ë¥ ë¡ ì  ì •ì±… Ï€ë¥¼ ë‚˜íƒ€ë‚´ê³ , S Ã— A â†’ [0, 1], Ï€(Ï€)ê°€ ì˜ˆìƒ í• ì¸ ë³´ìƒì„ ë‚˜íƒ€ë‚´ë„ë¡ í•˜ì:
Consider an infinite-horizon discounted Markov decision process (MDP), defined by the tuple (S, A, P, r, Ï0, Î³), where S is a finite set of states, A is a finite set of actions, P : S Ã— A Ã— S â†’ R is the transition probability distri bution, r : S â†’ R is the reward function, Ï0 : S â†’ R is the distribution of the initial state s0, and Î³ âˆˆ (0, 1) is the discount factor.

Let Ï€ denote a stochastic policy Ï€ : S Ã— A â†’ [0, 1], and let Î·(Ï€) denote its expected discounted reward:

Î·(Ï€) = Es0,a0,... "Xâˆ t=0 Î³ t r(st)

, where s0 âˆ¼ Ï0(s0), at âˆ¼ Ï€(at|st), st+1 âˆ¼ P(st+1|st, at).

We will use the following standard definitions of the stateaction value function QÏ€, the value function VÏ€, and the advantage function AÏ€: QÏ€(st, at) = Est+1,at+1,... "Xâˆ l=0 Î³ l r(st+l)

, VÏ€(st) = Eat,st+1,... "Xâˆ l=0 Î³ l r(st+l)

, AÏ€(s, a) = QÏ€(s, a) âˆ’ VÏ€(s), where at âˆ¼ Ï€(at|st), st+1 âˆ¼ P(st+1|st, at) for t â‰¥ 0.
l=0
Î³
l
r(st+l)
#
,
AÏ€(s, a) = QÏ€(s, a) âˆ’ VÏ€(s), where
at âˆ¼ Ï€(at|st), st+1 âˆ¼ P(st+1|st, at) for t â‰¥ 0.




