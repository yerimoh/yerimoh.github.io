---
title: "FastText: Enriching Word Vectors with Subword Information ì •ë¦¬"
date:   2022-07-04
excerpt: "Enriching Word Vectors with Subword Information, FastText"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## í•µì‹¬  

## ì½ê¸° ìœ„í•´ í•„ìš”í•œ ì§€ì‹
* [word2vec](https://yerimoh.github.io/DL14/): baseline ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ê¼­ ì•Œì•„ì•¼ í•œë‹¤.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec ì†ë„ ê°œì„ ìœ¼ë¡œ ì´ í¬ìŠ¤íŒ…ë„ ê¼­ ì•Œì•„ì•¼ í•œë‹¤.      

## ì› ë…¼ë¬¸
[Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)


---

# ëª©ì°¨  


---


# INTRO
ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë§Œë“œëŠ” ë°©ë²•ì´ë‹¤.     
ê¸°ì¡´ ë°©ë²•ì¸ Word2Vecì˜ **í˜•íƒœí•™ì„ ë¬´ì‹œ(ë‹¨ì–´ì˜ ë‚´ë¶€êµ¬ì¡° ë¬´ì‹œ)** í•œë‹¤ëŠ” ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ê²ƒ ì´ê¸° ë•Œë¬¸ì— ë©”ì»¤ë‹ˆì¦˜ ìì²´ëŠ” Word2Vecì˜ í™•ì¥ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.      

Word2Vecì™€ íŒ¨ìŠ¤íŠ¸í…ìŠ¤íŠ¸ì™€ì˜ ê°€ì¥ í° ì°¨ì´ì ì€ **Word2Vec**ì€ ë‹¨ì–´ë¥¼ **í•œë‹¨ìœ„**ë¡œ ìƒê°í•œë‹¤ë©´,     
**íŒ¨ìŠ¤íŠ¸í…ìŠ¤íŠ¸**ëŠ” **í•˜ë‚˜ì˜ ë‹¨ì–´ ì•ˆì—ë„ ì—¬ëŸ¬ ë‹¨ì–´ë“¤ì´ ì¡´ì¬**í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•œë‹¤. ì¦‰ **ë‚´ë¶€ ë‹¨ì–´(subword)ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµ**í•œë‹¤.

ì´ ë°©ë²•ì€ **ë¹ ë¥´ê³ **, **ëŒ€ê·œëª¨ ë§ë­‰ì¹˜**ì—ì„œ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ í›ˆë ¨í•  ìˆ˜ ìˆìœ¼ë©°, í›ˆë ¨ ë°ì´í„°ì— ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ë‹¨ì–´ì— ëŒ€í•œ ë‹¨ì–´ í‘œí˜„ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤(OOVë¬¸ì œ ê°œì„ ). 


![image](https://user-images.githubusercontent.com/76824611/178596276-54b596b0-5c62-44e6-a0dd-2036e105d928.png)



----
-----


# **1. Introduction** 

### ê¸°ì¡´ ì—°êµ¬: ë¶„í¬ ì˜ë¯¸ë¡ (distribution semantics)    
ê¸°ì¡´ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ëŠ” ë¶„í¬ ì˜ë¯¸ë¡ (distribution semantics)ìœ¼ë¡œ ì•Œë ¤ì§„ ë§ì€ ì—°êµ¬ê°€ ì§„í–‰ë˜ì—ˆìŒ.       
* ë¶„í¬ ì˜ë¯¸ë¡ (distribution semantics): ë‹¨ì–´ì˜ **ì—°ì†ì ì¸ í‘œí˜„ì„ í•™ìŠµ**í•˜ëŠ” ê²ƒ      
  * ì´ëŸ¬í•œ í‘œí˜„ì€ ì¼ë°˜ì ìœ¼ë¡œ **ë™ì‹œ ë°œìƒ í†µê³„**([co-occurrence statistics](https://yerimoh.github.io/DL13/#%EB%8B%A8%EC%96%B4%EC%9D%98-%EB%B6%84%EC%82%B0-%ED%91%9C%ED%98%84))ë¥¼ ì‚¬ìš©í•˜ì—¬ **ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ëŒ€í˜• ë§ë­‰ì¹˜**ì—ì„œ íŒŒìƒëœë‹¤.    
  * [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics)ìœ¼ë¡œ ì•Œë ¤ì§„ ë§ì€ ì—°êµ¬ê°€ ì´ëŸ¬í•œ ë°©ë²•ì˜ íŠ¹ì„±ì„ ì—°êµ¬í–ˆë‹¤.    
  * ì™¼ìª½ì˜ ë‘ ë‹¨ì–´ì™€ ì˜¤ë¥¸ìª½ì˜ ë‘ ë‹¨ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ì—¬ feedforward neural networkì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ì„ë² ë”©ì„ í•™ìŠµí•  ê²ƒì„ ì œì•ˆí–ˆë‹¤.    
  * [Mikolov et al. (2013b)](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)ëŠ” ë§¤ìš° í° ê¸°ì—…ì—ì„œ ë‹¨ì–´ì˜ ì—°ì†ì ì¸ í‘œí˜„ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ ë¡œê·¸ ì´ì§„ ëª¨ë¸([Word2vec](https://yerimoh.github.io/DL15/))ì„ ì œì•ˆí–ˆë‹¤.


---

### ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ 1: OOV ë¬¸ì œ  
ì—°ì†ì  í‘œí˜„ì€ ì¼ë°˜ì ìœ¼ë¡œ **ê³µë™ ë°œìƒ í†µê³„**ë¥¼ ì‚¬ìš©í•œ **ë ˆì´ë¸”ì´ ì—†ëŠ” ëŒ€í˜• ë§ë­‰ì¹˜ì—ì„œ íŒŒìƒ**ëœë‹¤    
â¡ ì´ëŸ¬í•œ ê¸°ë²•ì˜ ëŒ€ë¶€ë¶„ì€ ë§¤ê°œ ë³€ìˆ˜ ê³µìœ  ì—†ì´ **ê°œë³„ ë²¡í„°**ë¡œ ì–´íœ˜ì˜ **ê° ë‹¨ì–´ë¥¼ ë‚˜íƒ€ëƒ„**      
â¡ ì´ë ‡ê²Œ 1:1ë¡œ ê°œë³„ë°±í„°ë¡œ ë‹¨ì–´ë¥¼ ë‚˜íƒ€ë‚´ê²Œ í•˜ë©´ **ì²˜ìŒë³´ëŠ” ìƒˆë¡œìš´ ë‹¨ì–´**(í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ”)ë¥¼ **í‘œí˜„í•˜ì§€ ëª»í•˜ëŠ”**(vector embeddingí•˜ì§€ ëª»í•˜ëŠ”) **OOVë¬¸ì œ**ê°€ ë‚˜íƒ€ë‚œë‹¤.      

 
---   


### ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„: ë‹¨ì–´ ìì²´ì˜ ë‚´ë¶€êµ¬ì¡° ë¬´ì‹œ    
ì´ëŠ” **í˜•íƒœí•™ì **ìœ¼ë¡œ **í’ë¶€**í•œ ì–¸ì–´(ex í•€ë€ë“œ)ë¥¼ ì˜ ë‚˜íƒ€ë‚´ì§€ ëª»í•œë‹¤.       
+ í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€: ì ì€ ë‹¨ì–´ë¥¼ ë§ì´ ì‘ìš©í•˜ì—¬ ì—¬ëŸ¬ í‘œí˜„ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì–¸ì–´         
+ ex) work, working, worked, worker      
í›ˆë ¨ ë§ë­‰ì¹˜ì—ì„œ ê±°ì˜ ë°œìƒí•˜ì§€ ì•ŠëŠ”(ë˜ëŠ” ì „í˜€ ë°œìƒí•˜ì§€ ì•ŠëŠ”) ë§ì€ ë‹¨ì–´ í˜•ì‹ì´ í¬í•¨ë˜ì–´ ìˆì–´ **ì¢‹ì€ ë‹¨ì–´ í‘œí˜„ì„ ë°°ìš°ê¸°ê°€ ì–´ë µë‹¤**.          


---


### Solution
í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€í•œ ì–¸ì–´ì—ëŠ”  ë§ì€ **ë‹¨ì–´ í˜•ì„±ì´ ê·œì¹™**ì„ ë”°ë¦„     
â¡ **ë¬¸ì ìˆ˜ì¤€ ì •ë³´ë¥¼ ì‚¬ìš©**í•˜ì—¬ í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€í•œ ì–¸ì–´ì˜ ë²¡í„° í‘œí˜„ì„ ê°œì„  ê°€ëŠ¥     

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¬¸ì n-gramì— ëŒ€í•œ í‘œí˜„ì„ ë°°ìš°ê³  ë‹¨ì–´ë¥¼ n-gram ë²¡í„°ì˜ í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ê²ƒì„ ì œì•ˆí•œë‹¤.      
â¡ ì£¼ìš” ê¸°ì—¬ëŠ” **subword ì •ë³´ë¥¼ ê³ ë ¤**í•œ ì—°ì† [Skip-gram model](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)**ì˜ í™•ì¥**ì„ ë„ì…í•˜ëŠ” ê²ƒ     
â¡ ë‹¤ì–‘í•œ í˜•íƒœë¥¼ ë³´ì´ëŠ” 9ê°œ ì–¸ì–´ì— ëŒ€í•´ ì´ ëª¨ë¸ì„ í‰ê°€í•˜ì—¬ ì ‘ê·¼ ë°©ì‹ì˜ ì´ì ì„ ë³´ì—¬ì¤Œ        


2 Related workì€ ìƒëµí•˜ê² ë‹¤

-----
-----



# **3. Model**
ì´ ì„¹ì…˜ì—ì„œëŠ” í˜•íƒœí•™ì„ ê³ ë ¤í•˜ë©´ì„œ ë‹¨ì–´ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì„ ì œì•ˆí•œë‹¤.      

**[ëª¨ë¸ ëª©í‘œ]**     
* **subword** ë‹¨ìœ„ ê³ ë ¤     
* ë‹¨ì–´ë¥¼ **ë¬¸ì n-ê·¸ë¨ì˜ í•©**ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ í˜•íƒœí•™ì„ ëª¨ë¸ë§       


**[ì œì•ˆ ìˆœì„œ]**
1. ë‹¨ì–´ ë²¡í„°ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ë° ì‚¬ìš©í•˜ëŠ” **ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œ**        
2. subword ëª¨ë¸ì„ ì œì‹œ    
3. ìµœì¢…ì ìœ¼ë¡œ ë¬¸ì n-ê·¸ë¨ ì‚¬ì „ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…       


---



## 3.1  General model
FastTextì˜ base modelì€ [**skip-gram**](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)ì´ë‹¤.      


ìš”ì•½í•˜ìë©´ í˜„ì¬ ë‹¨ì–´($$W_t$$)(eat)ë¡œ ë§¥ë½ë‹¨ì–´ë“¤($$W_c$$)(I, You, She ...)(snack, rice, cake...)ì„ ìœ ì¶”í•˜ëŠ” ê²ƒì´ë‹¤,       
ì´ë¥¼ í†µí•´ ë‹¨ì–´ì˜ ë¶„ì‚°í‘œí˜„ì„ ì–»ëŠ”ë‹¤.     

![image](https://user-images.githubusercontent.com/76824611/177746087-c7df7756-7fa1-48be-900a-3b4f9e2ad155.png)

----


## 3.2 Subword model
ê¸°ì¡´ Skip-gramëª¨ë¸ì˜ **ë¬¸ì¥ ìì²´ì˜ êµ¬ì¡°ë¥¼ ë¬´ì‹œí•œë‹¤ëŠ” ë¬¸ì œë¥¼ í•´ê²°**í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•œë‹¤.     

ë‹¨ì–´ **ë‚´ë¶€ êµ¬ì¡°**ë¥¼ ë³´ë‹¤ ì˜ ë°˜ì˜í•˜ê¸° ìœ„í•˜ì—¬,      
FastTextëŠ” ë‹¨ì–´ wë¥¼ **n-gram characterì˜ bag**ìœ¼ë¡œ í‘œí˜„í•œë‹¤.        


íŒ¨ìŠ¤íŠ¸í…ìŠ¤íŠ¸ì—ì„œëŠ” ìš°ì„  ê° ë‹¨ì–´ëŠ” ê¸€ìë“¤ì˜ **n-gram**ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ë‹¤.    
nì„ ëª‡ìœ¼ë¡œ ê²°ì •í•˜ëŠ”ì§€ì— ë”°ë¼ì„œ ë‹¨ì–´ë“¤ì´ ì–¼ë§ˆë‚˜ ë¶„ë¦¬ë˜ëŠ”ì§€ ê²°ì •í•œë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´ì„œ nì„ 3ìœ¼ë¡œ ì¡ì€ íŠ¸ë¼ì´ê·¸ë¨(tri-gram)ì˜ ê²½ìš°,      
ë¨¼ì € ê° ë‹¨ì–´ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ë‹¨ì–´ì˜ ì‹œì‘ê³¼ ëì— ```<```,```>```ì„ ë¶™ì¸ë‹¤.      
ê·¸ ë‹¤ìŒ ```where```ì€ ```wh, her, ere, re```ë¡œ ë¶„ë¦¬í•˜ê³  ì´ë“¤ ë˜í•œ ì„ë² ë”©ì„ í•œë‹¤.     
ë§ˆì§€ë§‰ìœ¼ë¡œ ì—¬ê¸°ì— **special sequence**ë¡œ **word ìì²´**ê°€ í¬í•¨ëœë‹¤.       
ë”°ë¼ì„œ bag of n-gramìœ¼ë¡œ í‘œí˜„ëœ ë‹¨ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.    
![image](https://user-images.githubusercontent.com/76824611/177518555-36e5f509-365c-4fea-a33d-c70dd5e7bdb4.png)


**[nì˜ ë²”ìœ„ ì„¤ì •]**       
* ì‹¤ì œ ì‚¬ìš©í•  ë•ŒëŠ” nì˜ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ì„ ì„¤ì •ê°€ëŠ¥       
* nì€ 3ì´ìƒ 6ì´í•˜ì˜ ë²”ìœ„ì˜ ìˆ«ìë¡œ ì„¤ì •í•´ì•¼í•œë‹¤.         
![image](https://user-images.githubusercontent.com/76824611/177522694-59f02607-4d3c-41db-80a9-0f2ff070f1c8.png)

   




**[Scoring Function]**     
* G í¬ê¸°ì˜ nê·¸ë¨ dictionaryê°€ ì£¼ì–´ì¡Œë‹¤ê³  ê°€ì •í•˜ì.      
* w ë‹¨ì–´ê°€ ì£¼ì–´ì§€ë©´, Gw âŠ‚ {1, . . . , G}ë¡œ wì— ë‚˜íƒ€ë‚˜ëŠ” n-gram ì§‘í•©             
* ìš°ë¦¬ëŠ” ê° n-gramì— ë²¡í„° í‘œí˜„ $$z_g$$ë¥¼ ì—°ê´€ì‹œí‚¨ë‹¤.     
* ìš°ë¦¬ëŠ” ë‹¨ì–´ë¥¼ n-gramì˜ ë²¡í„° í‘œí˜„ì˜ í•©ìœ¼ë¡œ í‘œí˜„í•œë‹¤. ë”°ë¼ì„œ ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ë¥¼ ì–»ëŠ”ë‹¤.
![image](https://user-images.githubusercontent.com/76824611/178596197-691c0f25-a1cf-4277-944f-224ff19d9d95.png)


<details>
<summary>ğŸ“œ ê¸°ì¡´ Skip-gram modelì˜ Scoring Function</summary>
<div markdown="1">

* $$w_t$$: target word    
* $$w_c$$: context word   
![image](https://user-images.githubusercontent.com/76824611/202969101-4cfaabd4-5d0a-46e6-aed4-10c78f31bc7d.png)


</div>
</details>  

  


ì´ ê°„ë‹¨í•œ ëª¨ë¸ì„ í†µí•´ ë‹¨ì–´ ê°„ì— **representationì„ ê³µìœ (=ê°€ì¤‘ì¹˜ ê³µìœ )** í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ rareí•œ ë‹¨ì–´(ë‹¨ì–´ ì§‘í•© ë‚´ ë¹ˆë„ ìˆ˜ê°€ ì ì—ˆë˜ ë‹¨ì–´)ì— ëŒ€í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.    
ì˜ˆë¥¼ ë“¤ë©´ work, working, worked, workerì—ì„œ workì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.     


----
----


# **4. Experimental setup**

## 4.1 Baseline
ëŒ€ë¶€ë¶„ì˜ ì‹¤í—˜(5.3í•­ ì œì™¸)ì—ì„œ,    
ë³¸ ë…¼ë¬¸ì€ ë³¸ ë…¼ë¬¸ ëª¨ë¸ì„ word2vec2 package(Cë¡œ êµ¬í˜„ëœ skipgram ë° cbow ëª¨ë¸)ê³¼ ë¹„êµí•œë‹¤.

---

## 4.2 Optimization
* negative log likelihoodì— ëŒ€í•´ [stochastic gradient descent](https://yerimoh.github.io/DL5/#%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95-sgd)ë¥¼ ìˆ˜í–‰í•˜ì—¬ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°        
* [baseline skipgram model](https://yerimoh.github.io/DL15/)ì—ì„œì™€ ê°™ì´, ìš°ë¦¬ëŠ” step sizeì˜ linear decayë¥¼ ì‚¬ìš©          
* T wordsì™€ Pì™€ ë™ì¼í•œ ë°ì´í„°ì— ëŒ€í•œ passes ìˆ˜ê°€ í¬í•¨ëœ trainsetê°€ ì£¼ì–´ì§€ë©´,     
  ì‹œê°„ tì˜ ìŠ¤í… í¬ê¸°ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.    
  $$Î³0(1 âˆ’ \frac{t}{TP})$$    
  â€» ì—¬ê¸°ì„œ $$Î³0$$ì€ ê³ ì • ë§¤ê°œ ë³€ìˆ˜        
* Hogwildì— ì˜ì¡´í•˜ì—¬ ë™ì‹œì— ìµœì í™”ë¥¼ ìˆ˜í–‰      
* ëª¨ë“  threadsëŠ” ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê³µìœ í•˜ê³  ë²¡í„°ë¥¼ ë¹„ë™ê¸°ì‹(threads)ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.




<details>
<summary>ğŸ“œ HogWild ë³‘ë ¬ SGD</summary>
<div markdown="1">
  

HogWildëŠ” ì„œë¡œ ë‹¤ë¥¸ ì“°ë ˆë“œê°€ **ì„œë¡œ ë‹¤ë¥¸ ì›Œë“œ í˜ì–´ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ê¸°ë²•**ìœ¼ë¡œ, ëª¨ë¸ ì—…ë°ì´íŠ¸ ë‹¨ê³„ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì¶©ëŒ ìƒí™©(conflicts)ì€ ë¬´ì‹œí•´ë²„ë¦½ë‹ˆë‹¤.    
 ì´ë¡ ì ìœ¼ë¡œëŠ”, ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ê²½ìš°ì™€ ë¹„êµí–ˆì„ ë•Œ ì´ë ‡ê²Œ í•˜ë©´ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ ë¹„ìœ¨ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ìš¸ëŸ¬, HogWild ê¸°ë²•ì€ ì“°ë ˆë“œë¥¼ í†µí•œ **ì—…ë°ì´íŠ¸ê°€ ë™ì¼í•œ ë‹¨ì–´ê°€ ì•„ë‹ ê²½ìš° ì˜ ë™ì‘**í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ì¦‰, ëŒ€ê·œëª¨ì˜ ì–´íœ˜ì§‘ì— ëŒ€í•´ ì¶©ëŒ ìƒí™©ì´ ìƒëŒ€ì ìœ¼ë¡œ ë§¤ìš° ì ê²Œ ë‚˜íƒ€ë‚˜ë©° ë”°ë¼ì„œ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ì—ë„ ì˜í–¥ì´ ë³„ë¡œ ì—†ìŠµë‹ˆë‹¤. 
 
</div>
</details>  


---


## 4.3 Implementation details
ìš°ë¦¬ì˜ ëª¨ë¸ê³¼ baseline experiments ëª¨ë‘ì—ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.     
* **word vector:** dimension 300       
* **sampling**:ê°ê°ì˜ ê¸ì •ì ì¸ ì˜ˆì— ëŒ€í•´, smapling 5 negativesì„ ë¬´ì‘ìœ„ë¡œ í‘œë³¸ìœ¼ë¡œ ì¶”ì¶œ(uni-gram frequencyì˜ ì œê³±ê·¼ì— ë¹„ë¡€í•˜ëŠ” í™•ë¥ ë¡œ)              
* **context window size**: c (1ê³¼ 5 ì‚¬ì´ì˜ í¬ê¸° cë¥¼ ê· ì¼í•˜ê²Œ ìƒ˜í”Œë§)       
* **rejection threshold**: $$10^{-4}$$ (ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ë¥¼ subsampleí•˜ê¸° ìœ„í•´)    
* **min count**: word dictionaryë¥¼ ë§Œë“¤ ë•Œ, ìš°ë¦¬ëŠ” train ì„¸íŠ¸ì— 5ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë§Œ ìœ ì§€      
* **step size(learninf rate)($$Î³0$$)**: (word2vec íŒ¨í‚¤ì§€ì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •)    
   * **skipgram baseline:** 0.025      
   * **cbow baseline:** 0.05              

ì˜ì–´ ë°ì´í„°ì— ìœ„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµí•˜ë©´,     
ìš°ë¦¬ì˜ character n-grams modelì€ skipgram baselineë³´ë‹¤ í›ˆë ¨ ì†ë„ê°€ ì•½ 1.5ë°° ëŠë¦¬ë‹¤.       


-----


## Datasets
ë°ì´í„°ëŠ” [Wikipedia data](https://dumps.wikimedia.org)ë¥¼ ì‚¬ìš©í•œë‹¤.(5.3 ì œì™¸)       
ìœ„ ë°ì´í„°ì˜ 9ê°œêµ­ ì–¸ì–´ ë²„ì „ìœ¼ë¡œ ì‚¬ìš©:  ì•„ëì–´, ì²´ì½”ì–´, ë…ì¼ì–´, ì˜ì–´, ìŠ¤í˜ì¸ì–´, í”„ë‘ìŠ¤ì–´, ì´íƒˆë¦¬ì•„ì–´, ë£¨ë§ˆë‹ˆì•„ì–´ ë° ëŸ¬ì‹œì•„ì–´.

ìš°ë¦¬ëŠ” [Matt Mahoneyâ€™s pre-processing perl script](http://mattmahoney.net/dc/textdata)ë¥¼ ì‚¬ìš©í•˜ì—¬ raw Wikipedia dataë¥¼ ì •ìƒí™”í•œë‹¤.   

ëª¨ë“  ë°ì´í„° ì„¸íŠ¸ëŠ” shuffledë˜ê³ , ìš°ë¦¬ëŠ” ëª¨ë¸ì— ëŒ€í•´ five passesë¥¼ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¨ë‹¤.



----
-----


# **5 Results**
ìš°ë¦¬ëŠ” ë‹¤ì„¯ ê°€ì§€ ì‹¤í—˜ìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€í•œë‹¤:     
* an evaluation of word similarity and word analogies    
* a comparison to state-of-the-art methods   
* an analysis of the effect of the size of training data and of the **size of character n-grams** that we consider. 


----


## 5.1 Human similarity judgement

ë¨¼ì € ë‹¨ì–´ ìœ ì‚¬ì„±/ê´€ë ¨ì„± ì‘ì—…ì—ì„œ í‘œí˜„ì˜ í’ˆì§ˆì„ í‰ê°€    
â¡ í‰ê°€ ë°©ë²•: ì¸ê°„ì˜ íŒë‹¨ê³¼ ë²¡í„° í‘œí˜„ ì‚¬ì´ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ì„± ì‚¬ì´ì˜ Spearmanâ€™s rank correlation coefficientë¥¼ ê³„ì‚°.          

**[ë°ì´í„°ì…‹]**     
* **ë…ì¼ì–´**: GUR65, GUR350 ë° ZG222ì˜ ì„¸ ê°€ì§€ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ë¹„êµí•œë‹¤.   
* **ì˜ì–´**: [Finkelstein ë“±](https://www.researchgate.net/publication/221023177_Placing_search_in_context_The_concept_revisited)ì´ ì†Œê°œí•œ WS353 ë°ì´í„° ì„¸íŠ¸, (2001) ë° [Luong ë“±(2013)](https://nlp.stanford.edu/~lmthang/data/papers/conll13_morpho.pdf)ì´ ì†Œê°œí•œ the rare word dataset (RW)ë¥¼ ì‚¬ìš©í•œë‹¤.     
* **í”„ë‘ìŠ¤ì–´**: [ë²ˆì—­ëœ ë°ì´í„° ì„¸íŠ¸ RG65](https://link.springer.com/chapter/10.1007/978-3-642-21043-3_26)ì—ì„œ í”„ë‘ìŠ¤ì–´ ë‹¨ì–´ ë²¡í„°ë¥¼ í‰ê°€í•œë‹¤.     
* **ìŠ¤í˜ì¸ì–´, ì•„ëì–´ ë° ë£¨ë§ˆë‹ˆì•„ì–´**: ì´ ì–¸ì–´ë“¤ì˜ ë‹¨ì–´ ë²¡í„°ëŠ” [Hassan and Mihalcea, 2009](https://aclanthology.org/D09-1124.pdf)ì— ì„¤ëª…ëœ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ëœë‹¤.      
* **ëŸ¬ì‹œì•„ì–´:** ì´ ì–¸ì–´ì˜ ë‹¨ì–´ ë²¡í„°ëŠ” [Panchenko et al(2016)](https://arxiv.org/pdf/1708.09702.pdf)ì´ ë„ì…í•œ HJ datasetë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ëœë‹¤.



**[ê²°ê³¼: Table 1]**     
ìœ„ì˜ ëª¨ë“  ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ë°©ë²•ê³¼ ê¸°ì¤€ì„ ì— ëŒ€í•œ ê²°ê³¼ë¥¼ Table 1ì— ë‚˜íƒ€ëƒˆë‹¤.   
<img width="179" alt="image" src="https://user-images.githubusercontent.com/76824611/204183446-bf5f6fea-1ae5-4028-8527-6c8eb9a0d0e5.png">

<details>
<summary>ğŸ“œ [Table 1] </summary>
<div markdown="1">
 
word similarity datasetsì˜ human judgementê³¼ similarity scores ì‚¬ì´ì˜ ìƒê´€ê´€ê³„.     
ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ëª¨ë¸ê³¼ normalized Wikipedia dumpsë¥¼ ì‚¬ìš©í•œ the word2vec baselineë¥¼ ëª¨ë‘ trainí–ˆë‹¤.   
Evaluation datasetsì—ëŠ” training setì˜ ì¼ë¶€ê°€ ì•„ë‹Œ ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ null ë²¡í„°(sisg-)ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œí˜„í•œë‹¤.      
ìš°ë¦¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ n-gram vectors(sisg)ë¥¼ í•©ì‚°í•˜ì—¬ ë³´ì´ì§€ ì•ŠëŠ” ë‹¨ì–´ì— ëŒ€í•œ ë²¡í„°ë„ ê³„ì‚°í•œë‹¤.
 
</div>
</details>  


ì´ëŸ¬í•œ ë°ì´í„° ì„¸íŠ¸ì˜ ì¼ë¶€ ë‹¨ì–´ëŠ” training dataì— ë‚˜íƒ€ë‚˜ì§€ ì•Šìœ¼ë¯€ë¡œ cbow ë° skipgram baselinesì„ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ë‹¨ì–´ì— ëŒ€í•œ ë‹¨ì–´ í‘œí˜„ì„ ì–»ì„ ìˆ˜ ì—†ë‹¤.    
â¡ ê·¸ë˜ì„œ ë¹„êµ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ê¸°ë³¸ì ìœ¼ë¡œ ì´ëŸ¬í•œ ë‹¨ì–´ì— **null ë²¡í„°ë¥¼ ì‚¬ìš©**í•  ê²ƒì„ ì œì•ˆí•œë‹¤.     


ìš°ë¦¬ ëª¨ë¸ì€ subword ì •ë³´ë¥¼ í™œìš©í•˜ê¸° ë•Œë¬¸ì— **ì–´íœ˜ ë¶€ì¡± ë‹¨ì–´**ì— ëŒ€í•œ **ìœ íš¨í•œ í‘œí˜„ë„ ê³„ì‚° ê°€ëŠ¥**
â¡ n-gram vectorsì˜ í•©ì„ ì·¨í•¨ìœ¼ë¡œì¨ ê°€ëŠ¥í•˜ê²Œ í•¨.     

OOV ë‹¨ì–´ê°€ null ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œí˜„ë˜ëŠ” ê²½ìš°, ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ë°©ë²•ì„ sisg- ë° sisgë¼ê³  ë‹¬ë¦¬ ë¶€ë¥¸ë‹¤(Subword Information Skip Gram).      

* **result 1**    
Table 1 ì„ ë³´ë©´ **subword ì •ë³´ë¥¼ ì‚¬ìš©**í•˜ëŠ” ëª¨ë¸(sisg)ì´ ì˜ì–´ **WS353 ë°ì´í„° ì„¸íŠ¸ë¥¼ ì œì™¸í•œ ëª¨ë“  ë°ì´í„° ì„¸íŠ¸ì˜ ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒ**ì„ ì•Œ ìˆ˜ ìˆë‹¤.      
ë˜í•œ, **out-of-vocabulary word(sisg)ì— ëŒ€í•œ ë²¡í„° ê³„ì‚°**ì€ null ë²¡í„°ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸(sisg-)ë³´ë‹¤ ê±°ì˜ **ì¢‹ì€ ì„±ëŠ¥**ì„ ë³´ì¸ë‹¤.       
â¡ ì´ê²ƒì€ ë¬¸ì(character) n-gramsì˜ í˜•íƒœë¡œ **subword ì •ë³´ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì˜ ì´ì **ì„ ì¦ëª…í•œë‹¤.

* **result 2**    
English, French, Spanishë³´ë‹¤ **Arabic, German, Russian**ì—ì„œ **character n-gramsì„ ì‚¬ìš©í•˜ëŠ” íš¨ê³¼ê°€ ë” ì¤‘ìš”**í•˜ë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•œë‹¤.      
ê·¸ë¦¬ê³  ìš°ë¦¬ ëª¨ë¸ì€ ë‘ ì–¸ì–´ì˜ ìœ ì‚¬ì„± ë–„ë¬¸ì— German, Russianì„ ì™„ì „íˆ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ í‘œí˜„í•˜ì§€ ì•ŠëŠ”ë‹¤.      
**German(DE) ë‹¨ì–´ë“¤ì€ ë³µí•©ì–´**ì´ë‹¤: â€œtable tennis"ë¼ëŠ” ëª…ëª©ìƒì˜ ë¬¸êµ¬ëŠ” â€œTischtennisâ€ë¼ëŠ” ë‹¨ì–´ë¡œ ì“°ì—¬ì§„ë‹¤. "Tischtennis"ì™€ "Tennis"ì˜ ë¬¸ì ìˆ˜ì¤€ ìœ ì‚¬ì„±ì„ í™œìš©í•˜ì—¬, ìš°ë¦¬ì˜ ëª¨ë¸ì€ ë‘ ë‹¨ì–´ë¥¼ ì™„ì „íˆ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ í‘œí˜„í•˜ì§€ ì•ŠëŠ”ë‹¤.     
â¡ ì¦‰ Germanê³¼ ê°™ì€ **ë³µí•©ì–´ë¡œ ì´ë£¨ì–´ì§„ ë‹¨ì–´ë“¤ì—ê²Œ ë” ì˜ ì‘ìš©**í•œë‹¤.           

* **result 3**    
ìš°ë¦¬ì˜ ëª¨ë¸ì€ English Rare Words dataset (RW)ì— ëŒ€í•´ì„  baselinesì„ ëŠ¥ê°€í–ˆê°€.    
í•˜ì§€ë§Œ WS353 datasetì—ì„œëŠ” baselinesë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í•˜ë‹¤.   
â¡ ì´ ì´ìœ ëŠ”, **WS353 ë°ì´í„° ì„¸íŠ¸**ì˜ ë‹¨ì–´ê°€ **subword ì •ë³´ë¥¼ í™œìš©í•˜ì§€ ì•Šê³ ë„ ì¢‹ì€ ë²¡í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” í”í•œ ë‹¨ì–´**ì´ê¸° ë•Œë¬¸ì´ë‹¤.      
â¡ í•˜ì§€ë§Œ **ë¹ˆë„ê°€ ë‚®ì€ ë‹¨ì–´**ë¥¼ í‰ê°€í•  ë•Œ, ë‹¨ì–´ ê°„ì˜ ë¬¸ì ìˆ˜ì¤€ì—ì„œ ìœ ì‚¬ì„±ì„ ì‚¬ìš©í•˜ë©´ ì¢‹ì€ ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµí•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.      
â¡ ì¦‰ ë³¸ ëª¨ë¸ì€ í”í•œ ë‹¨ì–´ê°€ ë§ì€ datasetë³´ë‹¨ **ë¹ˆë„ê°€ ë‚®ì€ datasetì—ì„œ ë” íš¨ê³¼ê°€ ì¢‹ë‹¤**.        


---

## 5.2 Word analogy tasks

**[analogy tasks]**     
* ì´ taskëŠ” ì•„ë˜ ë‘ê°œì˜ testë¡œ êµ¬ëœë‹¤.     
* **Semantic**      
son -> daughterì¼ ë•Œ, grandpaë¥¼ ì£¼ì—ˆì„ ë•Œ grandmaë¥¼ ë‹µìœ¼ë¡œ ë‚¼ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ê°–ê³  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ íŒë‹¨í•œë‹¤.    
* **Syntactic**    
amazing -> amazinglyì¼ ë•Œ, most ì£¼ì—ˆì„ ë•Œ mostlyë¥¼ ë‹µìœ¼ë¡œ ë‚¼ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ê°–ê³  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ íŒë‹¨í•œë‹¤.    


**[ì–¸ì–´ë³„ ë°ì´í„° ì„¸íŠ¸]**     
* **ì˜ì–´(EN):** [Mikolov et al.(2013a)](https://arxiv.org/pdf/1301.3781.pdf)     
* **ì²´ì½”(CS):** [Svobodaì™€ Brychin(2016)](https://arxiv.org/pdf/1608.00789.pdf)    
* **ë…ì¼(DE):** [KÃ¶per et al.(2015)](https://arxiv.org/pdf/1508.00106.pdf)   
* **ì´íƒˆë¦¬ì•„(IT):** [Berardi et al.(2015)](http://iir2015.isti.cnr.it/proceedings/IIR_2015_submission_11.pdf)    



**[ê²°ê³¼: Table 2]**     
Table 2ì—ì„œ ë‹¤ì–‘í•œ ëª¨ë¸ì— ëŒ€í•œ ì •í™•ë„ë¥¼ ë³´ê³ í•œë‹¤.  
<img width="146" alt="image" src="https://user-images.githubusercontent.com/76824611/204200278-9f423cd1-e0ab-4e8e-8b85-b7596d5bbc5c.png">

<details>
<summary>ğŸ“œ [Table 2] </summary>
<div markdown="1">
 
ì²´ì½”ì–´, ë…ì¼ì–´, ì˜ì–´ ë° ì´íƒˆë¦¬ì•„ì–´ì— ëŒ€í•œ word analogy tasksì— ëŒ€í•œ ë³¸ ë…¼ë¬¸ì˜ ëª¨ë¸ê³¼ baselines(ê¸°ì¡´ ëª¨ë¸)ì˜ ì •í™•ì„±. 
semantic ê³¼ syntacticì— ëŒ€í•œ ê²°ê³¼ë¥¼ ë³„ë„ë¡œ ë³´ê³ í•œë‹¤.    
 
</div>
</details>  

* **result 1**    
**í˜•íƒœí•™ì  ì •ë³´**ê°€ **syntactic ì‘ì—…ì„ í¬ê²Œ ê°œì„ **í•œë‹¤      
ë³¸ ë…¼ë¬¸ ëª¨ë¸ì€ ì€ ì„±ëŠ¥ì´ ì¢‹ë‹¤.     

* **result 2**   
ë°˜ë©´, ë³¸ ë…¼ë¬¸ì˜ ëª¨ë¸ì€ **semantic ì§ˆë¬¸ì— ë„ì›€ì´ ë˜ì§€ ì•ŠìŒ**    
ì‹¬ì§€ì–´ ë…ì¼ì–´ì™€ ì´íƒˆë¦¬ì•„ì–´ì˜ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¨ë‹¤.     
â¡ ì´ê²ƒì€ ìš°ë¦¬ê°€ ê³ ë ¤í•˜ëŠ”character n-gramsì˜ ê¸¸ì´ ì„ íƒê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆë‹¤.       
â¡ ë³¸ ë…¼ë¬¸ì€ 5.5í•­ì—ì„œ **n-gramsì˜ í¬ê¸°ë¥¼ ìµœì ìœ¼ë¡œ ì„ íƒí•˜ë©´ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±ì´ ëœ ì €í•˜**ëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.    

* **result 3**    
ì²´ì½”ì–´ì™€ ë…ì¼ì–´ì™€ ê°™ì€ **í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€í•œ ì–¸ì–´ì˜ ê²½ìš°** ë³¸ ë…¼ë¬¸ì˜ ëª¨ë¸ì´ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤  **í›¨ì”¬ ë” ì„±ëŠ¥ì´ ì¢‹ì•˜ë‹¤**      



----

## 5.3 Comparison with morphological representations
We also compare our approach to previous work on
word vectors incorporating subword information on
word similarity tasks. The methods used are: the
recursive neural network of Luong et al. (2013),
the morpheme cbow of Qiu et al. (2014) and the
morphological transformations of Soricut and Och
(2015). In order to make the results comparable, we
trained our model on the same datasets as the methods we are comparing to: the English Wikipedia
data released by Shaoul and Westbury (2010), and
the news crawl data from the 2013 WMT shared
task for German, Spanish and French. We also
compare our approach to the log-bilinear language
model introduced by Botha and Blunsom (2014),
which was trained on the Europarl and news commentary corpora. Again, we trained our model on
the same data to make the results comparable. Using our model, we obtain representations of out-ofvocabulary words by summing the representations
of character n-grams. We report results in Table 3.
We observe that our simple approach performs well
relative to techniques based on subword information
obtained from morphological segmentors. We also
observe that our approach outperforms the Soricut
and Och (2015) method, which is based on prefix
and suffix analysis. The large improvement for German is due to the fact that their approach does not
model noun compounding, contrary to ours.


