---
title: "FastText: Enriching Word Vectors with Subword Information ì •ë¦¬"
date:   2022-07-04
excerpt: "Enriching Word Vectors with Subword Information, FastText"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## í•µì‹¬  

## ì½ê¸° ìœ„í•´ í•„ìš”í•œ ì§€ì‹
* [word2vec](https://yerimoh.github.io/DL14/): baseline ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ê¼­ ì•Œì•„ì•¼ í•œë‹¤.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec ì†ë„ ê°œì„ ìœ¼ë¡œ ì´ í¬ìŠ¤íŒ…ë„ ê¼­ ì•Œì•„ì•¼ í•œë‹¤.      

## ì› ë…¼ë¬¸
[Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)


---

# ëª©ì°¨  


---


# INTRO
ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë§Œë“œëŠ” ë°©ë²•ì´ë‹¤.     
ê¸°ì¡´ ë°©ë²•ì¸ Word2Vecì˜ **í˜•íƒœí•™ì„ ë¬´ì‹œ(ë‹¨ì–´ì˜ ë‚´ë¶€êµ¬ì¡° ë¬´ì‹œ)** í•œë‹¤ëŠ” ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ê²ƒ ì´ê¸° ë•Œë¬¸ì— ë©”ì»¤ë‹ˆì¦˜ ìì²´ëŠ” Word2Vecì˜ í™•ì¥ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.      

Word2Vecì™€ íŒ¨ìŠ¤íŠ¸í…ìŠ¤íŠ¸ì™€ì˜ ê°€ì¥ í° ì°¨ì´ì ì€ **Word2Vec**ì€ ë‹¨ì–´ë¥¼ **í•œë‹¨ìœ„**ë¡œ ìƒê°í•œë‹¤ë©´,     
**íŒ¨ìŠ¤íŠ¸í…ìŠ¤íŠ¸**ëŠ” **í•˜ë‚˜ì˜ ë‹¨ì–´ ì•ˆì—ë„ ì—¬ëŸ¬ ë‹¨ì–´ë“¤ì´ ì¡´ì¬**í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•œë‹¤. ì¦‰ **ë‚´ë¶€ ë‹¨ì–´(subword)ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµ**í•œë‹¤.

ì´ ë°©ë²•ì€ **ë¹ ë¥´ê³ **, **ëŒ€ê·œëª¨ ë§ë­‰ì¹˜**ì—ì„œ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ í›ˆë ¨í•  ìˆ˜ ìˆìœ¼ë©°, í›ˆë ¨ ë°ì´í„°ì— ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ë‹¨ì–´ì— ëŒ€í•œ ë‹¨ì–´ í‘œí˜„ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤(OOVë¬¸ì œ ê°œì„ ). 


![image](https://user-images.githubusercontent.com/76824611/178596276-54b596b0-5c62-44e6-a0dd-2036e105d928.png)



----
-----


# **1. Introduction** 

### ê¸°ì¡´ ì—°êµ¬: ë¶„í¬ ì˜ë¯¸ë¡ (distribution semantics)    
ê¸°ì¡´ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ëŠ” ë¶„í¬ ì˜ë¯¸ë¡ (distribution semantics)ìœ¼ë¡œ ì•Œë ¤ì§„ ë§ì€ ì—°êµ¬ê°€ ì§„í–‰ë˜ì—ˆìŒ.       
* ë¶„í¬ ì˜ë¯¸ë¡ (distribution semantics): ë‹¨ì–´ì˜ **ì—°ì†ì ì¸ í‘œí˜„ì„ í•™ìŠµ**í•˜ëŠ” ê²ƒ      
  * ì´ëŸ¬í•œ í‘œí˜„ì€ ì¼ë°˜ì ìœ¼ë¡œ **ë™ì‹œ ë°œìƒ í†µê³„**([co-occurrence statistics](https://yerimoh.github.io/DL13/#%EB%8B%A8%EC%96%B4%EC%9D%98-%EB%B6%84%EC%82%B0-%ED%91%9C%ED%98%84))ë¥¼ ì‚¬ìš©í•˜ì—¬ **ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ëŒ€í˜• ë§ë­‰ì¹˜**ì—ì„œ íŒŒìƒëœë‹¤.    
  * [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics)ìœ¼ë¡œ ì•Œë ¤ì§„ ë§ì€ ì—°êµ¬ê°€ ì´ëŸ¬í•œ ë°©ë²•ì˜ íŠ¹ì„±ì„ ì—°êµ¬í–ˆë‹¤.    
  * ì™¼ìª½ì˜ ë‘ ë‹¨ì–´ì™€ ì˜¤ë¥¸ìª½ì˜ ë‘ ë‹¨ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ì—¬ feedforward neural networkì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ì„ë² ë”©ì„ í•™ìŠµí•  ê²ƒì„ ì œì•ˆí–ˆë‹¤.    
  * [Mikolov et al. (2013b)](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)ëŠ” ë§¤ìš° í° ê¸°ì—…ì—ì„œ ë‹¨ì–´ì˜ ì—°ì†ì ì¸ í‘œí˜„ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ ë¡œê·¸ ì´ì§„ ëª¨ë¸([Word2vec](https://yerimoh.github.io/DL15/))ì„ ì œì•ˆí–ˆë‹¤.


---

### ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ 1: OOV ë¬¸ì œ  
ì—°ì†ì  í‘œí˜„ì€ ì¼ë°˜ì ìœ¼ë¡œ **ê³µë™ ë°œìƒ í†µê³„**ë¥¼ ì‚¬ìš©í•œ **ë ˆì´ë¸”ì´ ì—†ëŠ” ëŒ€í˜• ë§ë­‰ì¹˜ì—ì„œ íŒŒìƒ**ëœë‹¤    
â¡ ì´ëŸ¬í•œ ê¸°ë²•ì˜ ëŒ€ë¶€ë¶„ì€ ë§¤ê°œ ë³€ìˆ˜ ê³µìœ  ì—†ì´ **ê°œë³„ ë²¡í„°**ë¡œ ì–´íœ˜ì˜ **ê° ë‹¨ì–´ë¥¼ ë‚˜íƒ€ëƒ„**      
â¡ ì´ë ‡ê²Œ 1:1ë¡œ ê°œë³„ë°±í„°ë¡œ ë‹¨ì–´ë¥¼ ë‚˜íƒ€ë‚´ê²Œ í•˜ë©´ **ì²˜ìŒë³´ëŠ” ìƒˆë¡œìš´ ë‹¨ì–´**(í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ”)ë¥¼ **í‘œí˜„í•˜ì§€ ëª»í•˜ëŠ”**(vector embeddingí•˜ì§€ ëª»í•˜ëŠ”) **OOVë¬¸ì œ**ê°€ ë‚˜íƒ€ë‚œë‹¤.      

 
---   


### ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„: ë‹¨ì–´ ìì²´ì˜ ë‚´ë¶€êµ¬ì¡° ë¬´ì‹œ    
ì´ëŠ” **í˜•íƒœí•™ì **ìœ¼ë¡œ **í’ë¶€**í•œ ì–¸ì–´(ex í•€ë€ë“œ)ë¥¼ ì˜ ë‚˜íƒ€ë‚´ì§€ ëª»í•œë‹¤.       
+ í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€: ì ì€ ë‹¨ì–´ë¥¼ ë§ì´ ì‘ìš©í•˜ì—¬ ì—¬ëŸ¬ í‘œí˜„ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì–¸ì–´         
+ ex) work, working, worked, worker      
í›ˆë ¨ ë§ë­‰ì¹˜ì—ì„œ ê±°ì˜ ë°œìƒí•˜ì§€ ì•ŠëŠ”(ë˜ëŠ” ì „í˜€ ë°œìƒí•˜ì§€ ì•ŠëŠ”) ë§ì€ ë‹¨ì–´ í˜•ì‹ì´ í¬í•¨ë˜ì–´ ìˆì–´ **ì¢‹ì€ ë‹¨ì–´ í‘œí˜„ì„ ë°°ìš°ê¸°ê°€ ì–´ë µë‹¤**.          


---


### Solution
í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€í•œ ì–¸ì–´ì—ëŠ”  ë§ì€ **ë‹¨ì–´ í˜•ì„±ì´ ê·œì¹™**ì„ ë”°ë¦„     
â¡ **ë¬¸ì ìˆ˜ì¤€ ì •ë³´ë¥¼ ì‚¬ìš©**í•˜ì—¬ í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€í•œ ì–¸ì–´ì˜ ë²¡í„° í‘œí˜„ì„ ê°œì„  ê°€ëŠ¥     

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¬¸ì n-gramì— ëŒ€í•œ í‘œí˜„ì„ ë°°ìš°ê³  ë‹¨ì–´ë¥¼ n-gram ë²¡í„°ì˜ í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ê²ƒì„ ì œì•ˆí•œë‹¤.      
â¡ ì£¼ìš” ê¸°ì—¬ëŠ” **subword ì •ë³´ë¥¼ ê³ ë ¤**í•œ ì—°ì† [Skip-gram model](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)**ì˜ í™•ì¥**ì„ ë„ì…í•˜ëŠ” ê²ƒ     
â¡ ë‹¤ì–‘í•œ í˜•íƒœë¥¼ ë³´ì´ëŠ” 9ê°œ ì–¸ì–´ì— ëŒ€í•´ ì´ ëª¨ë¸ì„ í‰ê°€í•˜ì—¬ ì ‘ê·¼ ë°©ì‹ì˜ ì´ì ì„ ë³´ì—¬ì¤Œ        


2 Related workì€ ìƒëµí•˜ê² ë‹¤

-----
-----



# **3. Model**
ì´ ì„¹ì…˜ì—ì„œëŠ” í˜•íƒœí•™ì„ ê³ ë ¤í•˜ë©´ì„œ ë‹¨ì–´ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì„ ì œì•ˆí•œë‹¤.      

**[ëª¨ë¸ ëª©í‘œ]**     
* **subword** ë‹¨ìœ„ ê³ ë ¤     
* ë‹¨ì–´ë¥¼ **ë¬¸ì n-ê·¸ë¨ì˜ í•©**ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ í˜•íƒœí•™ì„ ëª¨ë¸ë§       


**[ì œì•ˆ ìˆœì„œ]**
1. ë‹¨ì–´ ë²¡í„°ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ë° ì‚¬ìš©í•˜ëŠ” **ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œ**        
2. subword ëª¨ë¸ì„ ì œì‹œ    
3. ìµœì¢…ì ìœ¼ë¡œ ë¬¸ì n-ê·¸ë¨ ì‚¬ì „ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…       


---



## 3.1  General model
FastTextì˜ base modelì€ [**skip-gram**](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)ì´ë‹¤.      


ìš”ì•½í•˜ìë©´ í˜„ì¬ ë‹¨ì–´($$W_t$$)(eat)ë¡œ ë§¥ë½ë‹¨ì–´ë“¤($$W_c$$)(I, You, She ...)(snack, rice, cake...)ì„ ìœ ì¶”í•˜ëŠ” ê²ƒì´ë‹¤,       
ì´ë¥¼ í†µí•´ ë‹¨ì–´ì˜ ë¶„ì‚°í‘œí˜„ì„ ì–»ëŠ”ë‹¤.     

![image](https://user-images.githubusercontent.com/76824611/177746087-c7df7756-7fa1-48be-900a-3b4f9e2ad155.png)

----


## 3.2 Subword model
ê¸°ì¡´ Skip-gramëª¨ë¸ì˜ **ë¬¸ì¥ ìì²´ì˜ êµ¬ì¡°ë¥¼ ë¬´ì‹œí•œë‹¤ëŠ” ë¬¸ì œë¥¼ í•´ê²°**í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•œë‹¤.     

ë‹¨ì–´ **ë‚´ë¶€ êµ¬ì¡°**ë¥¼ ë³´ë‹¤ ì˜ ë°˜ì˜í•˜ê¸° ìœ„í•˜ì—¬,      
FastTextëŠ” ë‹¨ì–´ wë¥¼ **n-gram characterì˜ bag**ìœ¼ë¡œ í‘œí˜„í•œë‹¤.        


íŒ¨ìŠ¤íŠ¸í…ìŠ¤íŠ¸ì—ì„œëŠ” ìš°ì„  ê° ë‹¨ì–´ëŠ” ê¸€ìë“¤ì˜ **n-gram**ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ë‹¤.    
nì„ ëª‡ìœ¼ë¡œ ê²°ì •í•˜ëŠ”ì§€ì— ë”°ë¼ì„œ ë‹¨ì–´ë“¤ì´ ì–¼ë§ˆë‚˜ ë¶„ë¦¬ë˜ëŠ”ì§€ ê²°ì •í•œë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´ì„œ nì„ 3ìœ¼ë¡œ ì¡ì€ íŠ¸ë¼ì´ê·¸ë¨(tri-gram)ì˜ ê²½ìš°,      
ë¨¼ì € ê° ë‹¨ì–´ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ë‹¨ì–´ì˜ ì‹œì‘ê³¼ ëì— ```<```,```>```ì„ ë¶™ì¸ë‹¤.      
ê·¸ ë‹¤ìŒ ```where```ì€ ```wh, her, ere, re```ë¡œ ë¶„ë¦¬í•˜ê³  ì´ë“¤ ë˜í•œ ì„ë² ë”©ì„ í•œë‹¤.     
ë§ˆì§€ë§‰ìœ¼ë¡œ ì—¬ê¸°ì— **special sequence**ë¡œ **word ìì²´**ê°€ í¬í•¨ëœë‹¤.       
ë”°ë¼ì„œ bag of n-gramìœ¼ë¡œ í‘œí˜„ëœ ë‹¨ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.    
![image](https://user-images.githubusercontent.com/76824611/177518555-36e5f509-365c-4fea-a33d-c70dd5e7bdb4.png)


**[nì˜ ë²”ìœ„ ì„¤ì •]**       
* ì‹¤ì œ ì‚¬ìš©í•  ë•ŒëŠ” nì˜ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ì„ ì„¤ì •ê°€ëŠ¥       
* nì€ 3ì´ìƒ 6ì´í•˜ì˜ ë²”ìœ„ì˜ ìˆ«ìë¡œ ì„¤ì •í•´ì•¼í•œë‹¤.         
![image](https://user-images.githubusercontent.com/76824611/177522694-59f02607-4d3c-41db-80a9-0f2ff070f1c8.png)

   




**[Scoring Function]**     
* G í¬ê¸°ì˜ nê·¸ë¨ dictionaryê°€ ì£¼ì–´ì¡Œë‹¤ê³  ê°€ì •í•˜ì.      
* w ë‹¨ì–´ê°€ ì£¼ì–´ì§€ë©´, Gw âŠ‚ {1, . . . , G}ë¡œ wì— ë‚˜íƒ€ë‚˜ëŠ” n-gram ì§‘í•©             
* ìš°ë¦¬ëŠ” ê° n-gramì— ë²¡í„° í‘œí˜„ $$z_g$$ë¥¼ ì—°ê´€ì‹œí‚¨ë‹¤.     
* ìš°ë¦¬ëŠ” ë‹¨ì–´ë¥¼ n-gramì˜ ë²¡í„° í‘œí˜„ì˜ í•©ìœ¼ë¡œ í‘œí˜„í•œë‹¤. ë”°ë¼ì„œ ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ë¥¼ ì–»ëŠ”ë‹¤.
![image](https://user-images.githubusercontent.com/76824611/178596197-691c0f25-a1cf-4277-944f-224ff19d9d95.png)


<details>
<summary>ğŸ“œ ê¸°ì¡´ Skip-gram modelì˜ Scoring Function</summary>
<div markdown="1">

* $$w_t$$: target word    
* $$w_c$$: context word   
![image](https://user-images.githubusercontent.com/76824611/202969101-4cfaabd4-5d0a-46e6-aed4-10c78f31bc7d.png)


</div>
</details>  

  


ì´ ê°„ë‹¨í•œ ëª¨ë¸ì„ í†µí•´ ë‹¨ì–´ ê°„ì— **representationì„ ê³µìœ (=ê°€ì¤‘ì¹˜ ê³µìœ )** í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ rareí•œ ë‹¨ì–´(ë‹¨ì–´ ì§‘í•© ë‚´ ë¹ˆë„ ìˆ˜ê°€ ì ì—ˆë˜ ë‹¨ì–´)ì— ëŒ€í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.    
ì˜ˆë¥¼ ë“¤ë©´ work, working, worked, workerì—ì„œ workì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.     


----
----


# **4. Experimental setup**

## 4.1 Baseline
In most experiments (except in Sec. 5.3), we compare our model to the C implementation of the skipgram and cbow models from the word2vec2 package.
ëŒ€ë¶€ë¶„ì˜ ì‹¤í—˜(5.3í•­ ì œì™¸)ì—ì„œ, ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ëª¨ë¸ì„ ë‹¨ì–´ 2vec2 íŒ¨í‚¤ì§€ì—ì„œ ìŠ¤í‚µê·¸ë¨ ë° cbow ëª¨ë¸ì˜ C êµ¬í˜„ê³¼ ë¹„êµí•œë‹¤.

## 4.2 Optimization
We solve our optimization problem by performing stochastic gradient descent on the negative log
likelihood presented before. As in the baseline
skipgram model, we use a linear decay of the step
size. Given a training set containing T words and
a number of passes over the data equal to P, the
step size at time t is equal to Î³0(1 âˆ’
t
T P ), where
Î³0 is a fixed parameter. We carry out the optimization in parallel, by resorting to Hogwild (Recht et
al., 2011). All threads share parameters and update
vectors in an asynchronous manner.


## 4.3 Implementation details
For both our model and the baseline experiments, we
use the following parameters: the word vectors have
dimension 300. For each positive example, we sample 5 negatives at random, with probability proportional to the square root of the uni-gram frequency.
We use a context window of size c, and uniformly
sample the size c between 1 and 5. In order to subsample the most frequent words, we use a rejection
threshold of 10âˆ’4
(for more details, see (Mikolov et
al., 2013b)). When building the word dictionary, we
keep the words that appear at least 5 times in the
training set. The step size Î³0 is set to 0.025 for the
skipgram baseline and to 0.05 for both our model
and the cbow baseline. These are the default values
in the word2vec package and work well for our
model too.
Using this setting on English data, our model with
character n-grams is approximately 1.5Ã— slower
to train than the skipgram baseline. Indeed,
we process 105k words/second/thread versus 145k
words/second/thread for the baseline. Our model is
implemented in C++, and is publicly available.3










