---
title: "FastText: Enriching Word Vectors with Subword Information ì •ë¦¬"
date:   2022-07-04
excerpt: "Enriching Word Vectors with Subword Information, FastText"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## í•µì‹¬  

## ì½ê¸° ìœ„í•´ í•„ìš”í•œ ì§€ì‹
* [word2vec](https://yerimoh.github.io/DL14/): baseline ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ê¼­ ì•Œì•„ì•¼ í•œë‹¤.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec ì†ë„ ê°œì„ ìœ¼ë¡œ ì´ í¬ìŠ¤íŒ…ë„ ê¼­ ì•Œì•„ì•¼ í•œë‹¤.      

## ì› ë…¼ë¬¸
[Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)


---

# ëª©ì°¨  


---


# INTRO
ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë§Œë“œëŠ” ë°©ë²•ì´ë‹¤.     
ê¸°ì¡´ ë°©ë²•ì¸ Word2Vecì˜ **í˜•íƒœí•™ì„ ë¬´ì‹œ(ë‹¨ì–´ì˜ ë‚´ë¶€êµ¬ì¡° ë¬´ì‹œ)** í•œë‹¤ëŠ” ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ê²ƒ ì´ê¸° ë•Œë¬¸ì— ë©”ì»¤ë‹ˆì¦˜ ìì²´ëŠ” Word2Vecì˜ í™•ì¥ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.      

Word2Vecì™€ íŒ¨ìŠ¤íŠ¸í…ìŠ¤íŠ¸ì™€ì˜ ê°€ì¥ í° ì°¨ì´ì ì€ **Word2Vec**ì€ ë‹¨ì–´ë¥¼ **í•œë‹¨ìœ„**ë¡œ ìƒê°í•œë‹¤ë©´,     
**íŒ¨ìŠ¤íŠ¸í…ìŠ¤íŠ¸**ëŠ” **í•˜ë‚˜ì˜ ë‹¨ì–´ ì•ˆì—ë„ ì—¬ëŸ¬ ë‹¨ì–´ë“¤ì´ ì¡´ì¬**í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•œë‹¤. ì¦‰ **ë‚´ë¶€ ë‹¨ì–´(subword)ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµ**í•œë‹¤.

ì´ ë°©ë²•ì€ **ë¹ ë¥´ê³ **, **ëŒ€ê·œëª¨ ë§ë­‰ì¹˜**ì—ì„œ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ í›ˆë ¨í•  ìˆ˜ ìˆìœ¼ë©°, í›ˆë ¨ ë°ì´í„°ì— ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ë‹¨ì–´ì— ëŒ€í•œ ë‹¨ì–´ í‘œí˜„ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤(OOVë¬¸ì œ ê°œì„ ). 


![image](https://user-images.githubusercontent.com/76824611/178596276-54b596b0-5c62-44e6-a0dd-2036e105d928.png)



----
-----


# **1. Introduction** 

### ê¸°ì¡´ ì—°êµ¬: ë¶„í¬ ì˜ë¯¸ë¡ (distribution semantics)    
ê¸°ì¡´ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ëŠ” ë¶„í¬ ì˜ë¯¸ë¡ (distribution semantics)ìœ¼ë¡œ ì•Œë ¤ì§„ ë§ì€ ì—°êµ¬ê°€ ì§„í–‰ë˜ì—ˆìŒ.       
* ë¶„í¬ ì˜ë¯¸ë¡ (distribution semantics): ë‹¨ì–´ì˜ **ì—°ì†ì ì¸ í‘œí˜„ì„ í•™ìŠµ**í•˜ëŠ” ê²ƒ      
  * ì´ëŸ¬í•œ í‘œí˜„ì€ ì¼ë°˜ì ìœ¼ë¡œ **ë™ì‹œ ë°œìƒ í†µê³„**([co-occurrence statistics](https://yerimoh.github.io/DL13/#%EB%8B%A8%EC%96%B4%EC%9D%98-%EB%B6%84%EC%82%B0-%ED%91%9C%ED%98%84))ë¥¼ ì‚¬ìš©í•˜ì—¬ **ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ëŒ€í˜• ë§ë­‰ì¹˜**ì—ì„œ íŒŒìƒëœë‹¤.    
  * [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics)ìœ¼ë¡œ ì•Œë ¤ì§„ ë§ì€ ì—°êµ¬ê°€ ì´ëŸ¬í•œ ë°©ë²•ì˜ íŠ¹ì„±ì„ ì—°êµ¬í–ˆë‹¤.    
  * ì™¼ìª½ì˜ ë‘ ë‹¨ì–´ì™€ ì˜¤ë¥¸ìª½ì˜ ë‘ ë‹¨ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ì—¬ feedforward neural networkì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ì„ë² ë”©ì„ í•™ìŠµí•  ê²ƒì„ ì œì•ˆí–ˆë‹¤.    
  * [Mikolov et al. (2013b)](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)ëŠ” ë§¤ìš° í° ê¸°ì—…ì—ì„œ ë‹¨ì–´ì˜ ì—°ì†ì ì¸ í‘œí˜„ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ ë¡œê·¸ ì´ì§„ ëª¨ë¸([Word2vec](https://yerimoh.github.io/DL15/))ì„ ì œì•ˆí–ˆë‹¤.


---

### ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ 1: OOV ë¬¸ì œ  
ì—°ì†ì  í‘œí˜„ì€ ì¼ë°˜ì ìœ¼ë¡œ **ê³µë™ ë°œìƒ í†µê³„**ë¥¼ ì‚¬ìš©í•œ **ë ˆì´ë¸”ì´ ì—†ëŠ” ëŒ€í˜• ë§ë­‰ì¹˜ì—ì„œ íŒŒìƒ**ëœë‹¤    
â¡ ì´ëŸ¬í•œ ê¸°ë²•ì˜ ëŒ€ë¶€ë¶„ì€ ë§¤ê°œ ë³€ìˆ˜ ê³µìœ  ì—†ì´ **ê°œë³„ ë²¡í„°**ë¡œ ì–´íœ˜ì˜ **ê° ë‹¨ì–´ë¥¼ ë‚˜íƒ€ëƒ„**      
â¡ ì´ë ‡ê²Œ 1:1ë¡œ ê°œë³„ë°±í„°ë¡œ ë‹¨ì–´ë¥¼ ë‚˜íƒ€ë‚´ê²Œ í•˜ë©´ **ì²˜ìŒë³´ëŠ” ìƒˆë¡œìš´ ë‹¨ì–´**(í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ”)ë¥¼ **í‘œí˜„í•˜ì§€ ëª»í•˜ëŠ”**(vector embeddingí•˜ì§€ ëª»í•˜ëŠ”) **OOVë¬¸ì œ**ê°€ ë‚˜íƒ€ë‚œë‹¤.      

 
---   


### ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„: ë‹¨ì–´ ìì²´ì˜ ë‚´ë¶€êµ¬ì¡° ë¬´ì‹œ    
ì´ëŠ” **í˜•íƒœí•™ì **ìœ¼ë¡œ **í’ë¶€**í•œ ì–¸ì–´(ex í•€ë€ë“œ)ë¥¼ ì˜ ë‚˜íƒ€ë‚´ì§€ ëª»í•œë‹¤.       
+ í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€: ì ì€ ë‹¨ì–´ë¥¼ ë§ì´ ì‘ìš©í•˜ì—¬ ì—¬ëŸ¬ í‘œí˜„ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì–¸ì–´         
+ ex) work, working, worked, worker      
í›ˆë ¨ ë§ë­‰ì¹˜ì—ì„œ ê±°ì˜ ë°œìƒí•˜ì§€ ì•ŠëŠ”(ë˜ëŠ” ì „í˜€ ë°œìƒí•˜ì§€ ì•ŠëŠ”) ë§ì€ ë‹¨ì–´ í˜•ì‹ì´ í¬í•¨ë˜ì–´ ìˆì–´ **ì¢‹ì€ ë‹¨ì–´ í‘œí˜„ì„ ë°°ìš°ê¸°ê°€ ì–´ë µë‹¤**.          


---


### Solution
í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€í•œ ì–¸ì–´ì—ëŠ”  ë§ì€ **ë‹¨ì–´ í˜•ì„±ì´ ê·œì¹™**ì„ ë”°ë¦„     
â¡ **ë¬¸ì ìˆ˜ì¤€ ì •ë³´ë¥¼ ì‚¬ìš©**í•˜ì—¬ í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€í•œ ì–¸ì–´ì˜ ë²¡í„° í‘œí˜„ì„ ê°œì„  ê°€ëŠ¥     

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë¬¸ì n-gramì— ëŒ€í•œ í‘œí˜„ì„ ë°°ìš°ê³  ë‹¨ì–´ë¥¼ n-gram ë²¡í„°ì˜ í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ê²ƒì„ ì œì•ˆí•œë‹¤.      
â¡ ì£¼ìš” ê¸°ì—¬ëŠ” **subword ì •ë³´ë¥¼ ê³ ë ¤**í•œ ì—°ì† [Skip-gram model](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)**ì˜ í™•ì¥**ì„ ë„ì…í•˜ëŠ” ê²ƒ     
â¡ ë‹¤ì–‘í•œ í˜•íƒœë¥¼ ë³´ì´ëŠ” 9ê°œ ì–¸ì–´ì— ëŒ€í•´ ì´ ëª¨ë¸ì„ í‰ê°€í•˜ì—¬ ì ‘ê·¼ ë°©ì‹ì˜ ì´ì ì„ ë³´ì—¬ì¤Œ        


2 Related workì€ ìƒëµí•˜ê² ë‹¤

-----
-----



# **3. Model**
ì´ ì„¹ì…˜ì—ì„œëŠ” í˜•íƒœí•™ì„ ê³ ë ¤í•˜ë©´ì„œ ë‹¨ì–´ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì„ ì œì•ˆí•œë‹¤.      

**[ëª¨ë¸ ëª©í‘œ]**     
* **subword** ë‹¨ìœ„ ê³ ë ¤     
* ë‹¨ì–´ë¥¼ **ë¬¸ì n-ê·¸ë¨ì˜ í•©**ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ í˜•íƒœí•™ì„ ëª¨ë¸ë§       


**[ì œì•ˆ ìˆœì„œ]**
1. ë‹¨ì–´ ë²¡í„°ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ë° ì‚¬ìš©í•˜ëŠ” **ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œ**        
2. subword ëª¨ë¸ì„ ì œì‹œ    
3. ìµœì¢…ì ìœ¼ë¡œ ë¬¸ì n-ê·¸ë¨ ì‚¬ì „ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…       


---



## 3.1  General model
FastTextì˜ base modelì€ [**skip-gram**](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)ì´ë‹¤.      


ìš”ì•½í•˜ìë©´ í˜„ì¬ ë‹¨ì–´($$W_t$$)(eat)ë¡œ ë§¥ë½ë‹¨ì–´ë“¤($$W_c$$)(I, You, She ...)(snack, rice, cake...)ì„ ìœ ì¶”í•˜ëŠ” ê²ƒì´ë‹¤,       
ì´ë¥¼ í†µí•´ ë‹¨ì–´ì˜ ë¶„ì‚°í‘œí˜„ì„ ì–»ëŠ”ë‹¤.     

![image](https://user-images.githubusercontent.com/76824611/177746087-c7df7756-7fa1-48be-900a-3b4f9e2ad155.png)

----


## 3.2 Subword model
ê¸°ì¡´ Skip-gramëª¨ë¸ì˜ **ë¬¸ì¥ ìì²´ì˜ êµ¬ì¡°ë¥¼ ë¬´ì‹œí•œë‹¤ëŠ” ë¬¸ì œë¥¼ í•´ê²°**í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•œë‹¤.     

ë‹¨ì–´ **ë‚´ë¶€ êµ¬ì¡°**ë¥¼ ë³´ë‹¤ ì˜ ë°˜ì˜í•˜ê¸° ìœ„í•˜ì—¬,      
FastTextëŠ” ë‹¨ì–´ wë¥¼ **n-gram characterì˜ bag**ìœ¼ë¡œ í‘œí˜„í•œë‹¤.        


íŒ¨ìŠ¤íŠ¸í…ìŠ¤íŠ¸ì—ì„œëŠ” ìš°ì„  ê° ë‹¨ì–´ëŠ” ê¸€ìë“¤ì˜ **n-gram**ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ë‹¤.    
nì„ ëª‡ìœ¼ë¡œ ê²°ì •í•˜ëŠ”ì§€ì— ë”°ë¼ì„œ ë‹¨ì–´ë“¤ì´ ì–¼ë§ˆë‚˜ ë¶„ë¦¬ë˜ëŠ”ì§€ ê²°ì •í•œë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´ì„œ nì„ 3ìœ¼ë¡œ ì¡ì€ íŠ¸ë¼ì´ê·¸ë¨(tri-gram)ì˜ ê²½ìš°,      
ë¨¼ì € ê° ë‹¨ì–´ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ë‹¨ì–´ì˜ ì‹œì‘ê³¼ ëì— ```<```,```>```ì„ ë¶™ì¸ë‹¤.      
ê·¸ ë‹¤ìŒ ```where```ì€ ```wh, her, ere, re```ë¡œ ë¶„ë¦¬í•˜ê³  ì´ë“¤ ë˜í•œ ì„ë² ë”©ì„ í•œë‹¤.     
ë§ˆì§€ë§‰ìœ¼ë¡œ ì—¬ê¸°ì— **special sequence**ë¡œ **word ìì²´**ê°€ í¬í•¨ëœë‹¤.       
ë”°ë¼ì„œ bag of n-gramìœ¼ë¡œ í‘œí˜„ëœ ë‹¨ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.    
![image](https://user-images.githubusercontent.com/76824611/177518555-36e5f509-365c-4fea-a33d-c70dd5e7bdb4.png)


**[nì˜ ë²”ìœ„ ì„¤ì •]**       
* ì‹¤ì œ ì‚¬ìš©í•  ë•ŒëŠ” nì˜ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ì„ ì„¤ì •ê°€ëŠ¥       
* nì€ 3ì´ìƒ 6ì´í•˜ì˜ ë²”ìœ„ì˜ ìˆ«ìë¡œ ì„¤ì •í•´ì•¼í•œë‹¤.         
![image](https://user-images.githubusercontent.com/76824611/177522694-59f02607-4d3c-41db-80a9-0f2ff070f1c8.png)

   




**[Scoring Function]**     
* G í¬ê¸°ì˜ nê·¸ë¨ dictionaryê°€ ì£¼ì–´ì¡Œë‹¤ê³  ê°€ì •í•˜ì.      
* w ë‹¨ì–´ê°€ ì£¼ì–´ì§€ë©´, Gw âŠ‚ {1, . . . , G}ë¡œ wì— ë‚˜íƒ€ë‚˜ëŠ” n-gram ì§‘í•©             
* ìš°ë¦¬ëŠ” ê° n-gramì— ë²¡í„° í‘œí˜„ $$z_g$$ë¥¼ ì—°ê´€ì‹œí‚¨ë‹¤.     
* ìš°ë¦¬ëŠ” ë‹¨ì–´ë¥¼ n-gramì˜ ë²¡í„° í‘œí˜„ì˜ í•©ìœ¼ë¡œ í‘œí˜„í•œë‹¤. ë”°ë¼ì„œ ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ë¥¼ ì–»ëŠ”ë‹¤.
![image](https://user-images.githubusercontent.com/76824611/178596197-691c0f25-a1cf-4277-944f-224ff19d9d95.png)


<details>
<summary>ğŸ“œ ê¸°ì¡´ Skip-gram modelì˜ Scoring Function</summary>
<div markdown="1">

* $$w_t$$: target word    
* $$w_c$$: context word   
![image](https://user-images.githubusercontent.com/76824611/202969101-4cfaabd4-5d0a-46e6-aed4-10c78f31bc7d.png)


</div>
</details>  

  


ì´ ê°„ë‹¨í•œ ëª¨ë¸ì„ í†µí•´ ë‹¨ì–´ ê°„ì— **representationì„ ê³µìœ (=ê°€ì¤‘ì¹˜ ê³µìœ )** í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ rareí•œ ë‹¨ì–´(ë‹¨ì–´ ì§‘í•© ë‚´ ë¹ˆë„ ìˆ˜ê°€ ì ì—ˆë˜ ë‹¨ì–´)ì— ëŒ€í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.    
ì˜ˆë¥¼ ë“¤ë©´ work, working, worked, workerì—ì„œ workì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.     


----
----


# **4. Experimental setup**

## 4.1 Baseline
ëŒ€ë¶€ë¶„ì˜ ì‹¤í—˜(5.3í•­ ì œì™¸)ì—ì„œ,    
ë³¸ ë…¼ë¬¸ì€ ë³¸ ë…¼ë¬¸ ëª¨ë¸ì„ word2vec2 package(Cë¡œ êµ¬í˜„ëœ skipgram ë° cbow ëª¨ë¸)ê³¼ ë¹„êµí•œë‹¤.

---

## 4.2 Optimization
* negative log likelihoodì— ëŒ€í•´ [stochastic gradient descent](https://yerimoh.github.io/DL5/#%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95-sgd)ë¥¼ ìˆ˜í–‰í•˜ì—¬ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°        
* [baseline skipgram model](https://yerimoh.github.io/DL15/)ì—ì„œì™€ ê°™ì´, ìš°ë¦¬ëŠ” step sizeì˜ linear decayë¥¼ ì‚¬ìš©          
* T wordsì™€ Pì™€ ë™ì¼í•œ ë°ì´í„°ì— ëŒ€í•œ passes ìˆ˜ê°€ í¬í•¨ëœ trainsetê°€ ì£¼ì–´ì§€ë©´,     
  ì‹œê°„ tì˜ ìŠ¤í… í¬ê¸°ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.    
  $$Î³0(1 âˆ’ \frac{t}{TP})$$    
  â€» ì—¬ê¸°ì„œ $$Î³0$$ì€ ê³ ì • ë§¤ê°œ ë³€ìˆ˜        
* Hogwildì— ì˜ì¡´í•˜ì—¬ ë™ì‹œì— ìµœì í™”ë¥¼ ìˆ˜í–‰      
* ëª¨ë“  threadsëŠ” ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê³µìœ í•˜ê³  ë²¡í„°ë¥¼ ë¹„ë™ê¸°ì‹(threads)ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.




<details>
<summary>ğŸ“œ HogWild ë³‘ë ¬ SGD</summary>
<div markdown="1">
  

HogWildëŠ” ì„œë¡œ ë‹¤ë¥¸ ì“°ë ˆë“œê°€ **ì„œë¡œ ë‹¤ë¥¸ ì›Œë“œ í˜ì–´ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ê¸°ë²•**ìœ¼ë¡œ, ëª¨ë¸ ì—…ë°ì´íŠ¸ ë‹¨ê³„ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì¶©ëŒ ìƒí™©(conflicts)ì€ ë¬´ì‹œí•´ë²„ë¦½ë‹ˆë‹¤.    
 ì´ë¡ ì ìœ¼ë¡œëŠ”, ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ê²½ìš°ì™€ ë¹„êµí–ˆì„ ë•Œ ì´ë ‡ê²Œ í•˜ë©´ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ ë¹„ìœ¨ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ìš¸ëŸ¬, HogWild ê¸°ë²•ì€ ì“°ë ˆë“œë¥¼ í†µí•œ **ì—…ë°ì´íŠ¸ê°€ ë™ì¼í•œ ë‹¨ì–´ê°€ ì•„ë‹ ê²½ìš° ì˜ ë™ì‘**í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ì¦‰, ëŒ€ê·œëª¨ì˜ ì–´íœ˜ì§‘ì— ëŒ€í•´ ì¶©ëŒ ìƒí™©ì´ ìƒëŒ€ì ìœ¼ë¡œ ë§¤ìš° ì ê²Œ ë‚˜íƒ€ë‚˜ë©° ë”°ë¼ì„œ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ì—ë„ ì˜í–¥ì´ ë³„ë¡œ ì—†ìŠµë‹ˆë‹¤. 
 
</div>
</details>  


---


## 4.3 Implementation details
ìš°ë¦¬ì˜ ëª¨ë¸ê³¼ baseline experiments ëª¨ë‘ì—ì„œ, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.     
* word vector: dimension 300       
* ê°ê°ì˜ ê¸ì •ì ì¸ ì˜ˆì— ëŒ€í•´, smapling 5 negativesì„ ë¬´ì‘ìœ„ë¡œ í‘œë³¸ìœ¼ë¡œ ì¶”ì¶œ(uni-gram frequencyì˜ ì œê³±ê·¼ì— ë¹„ë¡€í•˜ëŠ” í™•ë¥ ë¡œ)              
* context window size: c (1ê³¼ 5 ì‚¬ì´ì˜ í¬ê¸° cë¥¼ ê· ì¼í•˜ê²Œ ìƒ˜í”Œë§)       
* rejection threshold: $$10^{-4}$$ (ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ë¥¼ subsampleí•˜ê¸° ìœ„í•´)    
* word dictionaryë¥¼ ë§Œë“¤ ë•Œ, ìš°ë¦¬ëŠ” train ì„¸íŠ¸ì— 5ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë§Œ ìœ ì§€      
* step size(learninf rate) $$Î³0$$(word2vec íŒ¨í‚¤ì§€ì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •)    
   * skipgram baseline: 0.025      
   * cbow baseline: 0.05              

ì˜ì–´ ë°ì´í„°ì— ìœ„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµí•˜ë©´,     
ìš°ë¦¬ì˜ character n-grams modelì€ skipgram baselineë³´ë‹¤ í›ˆë ¨ ì†ë„ê°€ ì•½ 1.5ë°° ëŠë¦¬ë‹¤.       


-----


## Datasets
ë°ì´í„°ëŠ” [Wikipedia data](https://dumps.wikimedia.org)ë¥¼ ì‚¬ìš©í•œë‹¤.(5.3 ì œì™¸)       
ìœ„ ë°ì´í„°ì˜ 9ê°œêµ­ ì–¸ì–´ ë²„ì „ìœ¼ë¡œ ì‚¬ìš©:  ì•„ëì–´, ì²´ì½”ì–´, ë…ì¼ì–´, ì˜ì–´, ìŠ¤í˜ì¸ì–´, í”„ë‘ìŠ¤ì–´, ì´íƒˆë¦¬ì•„ì–´, ë£¨ë§ˆë‹ˆì•„ì–´ ë° ëŸ¬ì‹œì•„ì–´.

ìš°ë¦¬ëŠ” [Matt Mahoneyâ€™s pre-processing perl script](http://mattmahoney.net/dc/textdata)ë¥¼ ì‚¬ìš©í•˜ì—¬ raw Wikipedia dataë¥¼ ì •ìƒí™”í•œë‹¤.   

ëª¨ë“  ë°ì´í„° ì„¸íŠ¸ëŠ” shuffledë˜ê³ , ìš°ë¦¬ëŠ” ëª¨ë¸ì— ëŒ€í•´ five passesë¥¼ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¨ë‹¤.



----
-----


# **5 Results**
We evaluate our model in five experiments:  We will describe these experiments in detail in the following sections.

ìš°ë¦¬ëŠ” ë‹¤ì„¯ ê°€ì§€ ì‹¤í—˜ìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€í•œë‹¤:     
* an evaluation of word similarity and word analogies    
* a comparison to state-of-the-art methods   
* an analysis of the effect of the size of training data and of the **size of character n-grams** that we consider. 


----


## 5.1 Human similarity judgement
We first evaluate the quality of our representations
on the task of word similarity / relatedness. We do
so by computing Spearmanâ€™s rank correlation coefficient (Spearman, 1904) between human judgement and the cosine similarity between the vector
representations. For German, we compare the different models on three datasets: GUR65, GUR350
and ZG222 (Gurevych, 2005; Zesch and Gurevych,
2006). For English, we use the WS353 dataset introduced by Finkelstein et al. (2001) and the rare
word dataset (RW), introduced by Luong et al.
(2013). We evaluate the French word vectors on
the translated dataset RG65 (Joubarne and Inkpen,
2011). Spanish, Arabic and Romanian word vectors
are evaluated using the datasets described in (Hassan
and Mihalcea, 2009). Russian word vectors are evaluated using the HJ dataset introduced by Panchenko
et al. (2016).





We report results for our method and baselines
for all datasets in Table 1. Some words from these
datasets do not appear in our training data, and
thus, we cannot obtain word representation for these
words using the cbow and skipgram baselines. In
order to provide comparable results, we propose by
default to use null vectors for these words. Since our
model exploits subword information, we can also
compute valid representations for out-of-vocabulary
words. We do so by taking the sum of its n-gram
vectors. When OOV words are represented using
null vectors we refer to our method as sisg- and
sisg otherwise (Subword Information Skip Gram).
First, by looking at Table 1, we notice that the proposed model (sisg), which uses subword information, outperforms the baselines on all datasets except
the English WS353 dataset. Moreover, computing
vectors for out-of-vocabulary words (sisg) is always at least as good as not doing so (sisg-). This
proves the advantage of using subword information
in the form of character n-grams.


Second, we observe that the effect of using character n-grams is more important for Arabic, German and Russian than for English, French or Spanish. German and Russian exhibit grammatical declensions with four cases for German and six for
Russian. Also, many German words are compound
words; for instance the nominal phrase â€œtable tennisâ€ is written in a single word as â€œTischtennisâ€. By
exploiting the character-level similarities between
â€œTischtennisâ€ and â€œTennisâ€, our model does not represent the two words as completely different words


Finally, we observe that on the English Rare
Words dataset (RW), our approach outperforms the
baselines while it does not on the English WS353
dataset. This is due to the fact that words in the English WS353 dataset are common words for which
good vectors can be obtained without exploiting
subword information. When evaluating on less frequent words, we see that using similarities at the
character level between words can help learning
good word vectors.







