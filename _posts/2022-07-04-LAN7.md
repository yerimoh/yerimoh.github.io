---
title: "FastText: Enriching Word Vectors with Subword Information 정리"
date:   2022-07-04
excerpt: "Enriching Word Vectors with Subword Information, FastText"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)


---

# 목차  


---


# INTRO
단어를 벡터로 만드는 방법이다.     
기존 방법인 Word2Vec의 **형태학을 무시(단어의 내부구조 무시)** 한다는 단점을 보완하기 위해 나온 것 이기 때문에 메커니즘 자체는 Word2Vec의 확장이라고 볼 수 있다.      

Word2Vec와 패스트텍스트와의 가장 큰 차이점은 **Word2Vec**은 단어를 **한단위**로 생각한다면,     
**패스트텍스트**는 **하나의 단어 안에도 여러 단어들이 존재**하는 것으로 간주한다. 즉 **내부 단어(subword)를 고려하여 학습**한다.

이 방법은 **빠르고**, **대규모 말뭉치**에서 모델을 빠르게 훈련할 수 있으며, 훈련 데이터에 나타나지 않은 단어에 대한 단어 표현을 계산할 수 있다(OOV문제 개선). 


![image](https://user-images.githubusercontent.com/76824611/178596276-54b596b0-5c62-44e6-a0dd-2036e105d928.png)



----
-----


# **1. Introduction** 

### 기존 연구: 분포 의미론(distribution semantics)    
기존 자연어 처리 분야는 분포 의미론(distribution semantics)으로 알려진 많은 연구가 진행되었음.       
* 분포 의미론(distribution semantics): 단어의 **연속적인 표현을 학습**하는 것      
  * 이러한 표현은 일반적으로 **동시 발생 통계**([co-occurrence statistics](https://yerimoh.github.io/DL13/#%EB%8B%A8%EC%96%B4%EC%9D%98-%EB%B6%84%EC%82%B0-%ED%91%9C%ED%98%84))를 사용하여 **레이블이 지정되지 않은 대형 말뭉치**에서 파생된다.    
  * [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics)으로 알려진 많은 연구가 이러한 방법의 특성을 연구했다.    
  * 왼쪽의 두 단어와 오른쪽의 두 단어를 기반으로 단어를 예측하여 feedforward neural network을 사용하여 단어 임베딩을 학습할 것을 제안했다.    
  * [Mikolov et al. (2013b)](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)는 매우 큰 기업에서 단어의 연속적인 표현을 효율적으로 학습하기 위한 간단한 로그 이진 모델([Word2vec](https://yerimoh.github.io/DL15/))을 제안했다.


---

### 기존 연구의 한계 1: OOV 문제  
연속적 표현은 일반적으로 **공동 발생 통계**를 사용한 **레이블이 없는 대형 말뭉치에서 파생**된다    
➡ 이러한 기법의 대부분은 매개 변수 공유 없이 **개별 벡터**로 어휘의 **각 단어를 나타냄**      
➡ 이렇게 1:1로 개별백터로 단어를 나타내게 하면 **처음보는 새로운 단어**(학습 데이터에 없는)를 **표현하지 못하는**(vector embedding하지 못하는) **OOV문제**가 나타난다.      

 
---   


### 기존 연구의 한계: 단어 자체의 내부구조 무시    
이는 **형태학적**으로 **풍부**한 언어(ex 핀란드)를 잘 나타내지 못한다.       
+ 형태학적으로 풍부: 적은 단어를 많이 응용하여 여러 표현을 만들 수 있는 언어         
+ ex) work, working, worked, worker      
훈련 말뭉치에서 거의 발생하지 않는(또는 전혀 발생하지 않는) 많은 단어 형식이 포함되어 있어 **좋은 단어 표현을 배우기가 어렵다**.          


---


### Solution
형태학적으로 풍부한 언어에는  많은 **단어 형성이 규칙**을 따름     
➡ **문자 수준 정보를 사용**하여 형태학적으로 풍부한 언어의 벡터 표현을 개선 가능     

본 논문에서는 문자 n-gram에 대한 표현을 배우고 단어를 n-gram 벡터의 합으로 나타낼 것을 제안한다.      
➡ 주요 기여는 **subword 정보를 고려**한 연속 [Skip-gram model](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)**의 확장**을 도입하는 것     
➡ 다양한 형태를 보이는 9개 언어에 대해 이 모델을 평가하여 접근 방식의 이점을 보여줌        


2 Related work은 생략하겠다

-----
-----



# **3. Model**
이 섹션에서는 형태학을 고려하면서 단어 표현을 학습하는 모델을 제안한다.      

**[모델 목표]**     
* **subword** 단위 고려     
* 단어를 **문자 n-그램의 합**으로 표현하여 형태학을 모델링       


**[제안 순서]**
1. 단어 벡터를 훈련시키는 데 사용하는 **일반적인 프레임워크를 제시**        
2. subword 모델을 제시    
3. 최종적으로 문자 n-그램 사전을 처리하는 방법을 설명       


---



## 3.1  General model
FastText의 base model은 [**skip-gram**](https://yerimoh.github.io/DL14/#skip-gram-%EB%AA%A8%EB%8D%B8)이다.      


요약하자면 현재 단어($$W_t$$)(eat)로 맥락단어들($$W_c$$)(I, You, She ...)(snack, rice, cake...)을 유추하는 것이다,       
이를 통해 단어의 분산표현을 얻는다.     

![image](https://user-images.githubusercontent.com/76824611/177746087-c7df7756-7fa1-48be-900a-3b4f9e2ad155.png)

----


## 3.2 Subword model
기존 Skip-gram모델의 **문장 자체의 구조를 무시한다는 문제를 해결**하기 위해 본 논문에서는 새로운 방법을 제안한다.     

단어 **내부 구조**를 보다 잘 반영하기 위하여,      
FastText는 단어 w를 **n-gram character의 bag**으로 표현한다.        


패스트텍스트에서는 우선 각 단어는 글자들의 **n-gram**으로 나타낸다.    
n을 몇으로 결정하는지에 따라서 단어들이 얼마나 분리되는지 결정한다. 

예를 들어서 n을 3으로 잡은 트라이그램(tri-gram)의 경우,      
먼저 각 단어를 구분하기 위해 단어의 시작과 끝에 ```<```,```>```을 붙인다.      
그 다음 ```where```은 ```wh, her, ere, re```로 분리하고 이들 또한 임베딩을 한다.     
마지막으로 여기에 **special sequence**로 **word 자체**가 포함된다.       
따라서 bag of n-gram으로 표현된 단어는 다음과 같다.    
![image](https://user-images.githubusercontent.com/76824611/177518555-36e5f509-365c-4fea-a33d-c70dd5e7bdb4.png)


**[n의 범위 설정]**       
* 실제 사용할 때는 n의 최소값과 최대값을 설정가능       
* n은 3이상 6이하의 범위의 숫자로 설정해야한다.         
![image](https://user-images.githubusercontent.com/76824611/177522694-59f02607-4d3c-41db-80a9-0f2ff070f1c8.png)

   




**[Scoring Function]**     
* G 크기의 n그램 dictionary가 주어졌다고 가정하자.      
* w 단어가 주어지면, Gw ⊂ {1, . . . , G}로 w에 나타나는 n-gram 집합             
* 우리는 각 n-gram에 벡터 표현 $$z_g$$를 연관시킨다.     
* 우리는 단어를 n-gram의 벡터 표현의 합으로 표현한다. 따라서 스코어링 함수를 얻는다.
![image](https://user-images.githubusercontent.com/76824611/178596197-691c0f25-a1cf-4277-944f-224ff19d9d95.png)


<details>
<summary>📜 기존 Skip-gram model의 Scoring Function</summary>
<div markdown="1">

* $$w_t$$: target word    
* $$w_c$$: context word   
![image](https://user-images.githubusercontent.com/76824611/202969101-4cfaabd4-5d0a-46e6-aed4-10c78f31bc7d.png)


</div>
</details>  

  


이 간단한 모델을 통해 단어 간에 **representation을 공유(=가중치 공유)** 할 수 있으므로 rare한 단어(단어 집합 내 빈도 수가 적었던 단어)에 대한 신뢰할 수 있는 표현을 학습할 수 있다.    
예를 들면 work, working, worked, worker에서 work의 가중치를 공유시키는 것이다.     


----
----


# **4. Experimental setup**

## 4.1 Baseline
대부분의 실험(5.3항 제외)에서,    
본 논문은 본 논문 모델을 word2vec2 package(C로 구현된 skipgram 및 cbow 모델)과 비교한다.

---

## 4.2 Optimization
* negative log likelihood에 대해 [stochastic gradient descent](https://yerimoh.github.io/DL5/#%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95-sgd)를 수행하여 최적화 문제를 해결        
* [baseline skipgram model](https://yerimoh.github.io/DL15/)에서와 같이, 우리는 step size의 linear decay를 사용          
* T words와 P와 동일한 데이터에 대한 passes 수가 포함된 trainset가 주어지면,     
  시간 t의 스텝 크기는 아래와 같다.    
  $$γ0(1 − \frac{t}{TP})$$    
  ※ 여기서 $$γ0$$은 고정 매개 변수        
* Hogwild에 의존하여 동시에 최적화를 수행      
* 모든 threads는 매개 변수를 공유하고 벡터를 비동기식(threads)으로 업데이트합니다.




<details>
<summary>📜 HogWild 병렬 SGD</summary>
<div markdown="1">
  

HogWild는 서로 다른 쓰레드가 **서로 다른 워드 페어를 병렬로 처리하는 기법**으로, 모델 업데이트 단계에서 발생할 수 있는 충돌 상황(conflicts)은 무시해버립니다.    
 이론적으로는, 순차적으로 실행되는 경우와 비교했을 때 이렇게 하면 알고리즘의 수렴 비율을 줄일 수 있습니다. 아울러, HogWild 기법은 쓰레드를 통한 **업데이트가 동일한 단어가 아닐 경우 잘 동작**하는 것으로 알려져 있습니다. 즉, 대규모의 어휘집에 대해 충돌 상황이 상대적으로 매우 적게 나타나며 따라서 알고리즘의 수렴에도 영향이 별로 없습니다. 
 
</div>
</details>  


---


## 4.3 Implementation details
우리의 모델과 기준 실험 모두에서, 우리는 다음과 같은 매개 변수를 사용한다. 단어 벡터는 차원 300을 갖는다. 각각의 긍정적인 예에 대해, 우리는 유니그램 주파수의 제곱근에 비례하는 확률로 5개의 부정을 무작위로 표본으로 추출한다. 우리는 c 크기의 컨텍스트 창을 사용하고, 1과 5 사이의 크기 c를 균일하게 샘플링한다. 가장 빈번한 단어를 하위 샘플링하기 위해 거부 임계값을 10-4로 사용한다(자세한 내용은 (Mikolov et al., 2013b) 참조). 단어 사전을 만들 때, 우리는 훈련 세트에 5번 이상 나오는 단어를 유지한다. 스텝 크기 θ0은 스킵그램 기준선의 경우 0.025로 설정되고 모델과 C보우 기준선의 경우 0.05로 설정됩니다. 이것들은 word2vec 패키지의 기본값이며 우리 모델에서도 잘 작동한다. 영어 데이터에 이 설정을 사용하여 문자 n-그램이 있는 모델은 스킵그램 기준선보다 훈련 속도가 약 1.5배 느리다. 실제로, 우리는 기준에 대해 105k 워드/초/스레드 대 145k 워드/초/스레드를 처리한다. 우리 모델은 C++로 구현되어 있으며 공개적으로 사용할 수 있다.3

For both our model and the baseline experiments, we use the following parameters: the word vectors have dimension 300. For each positive example, we sample 5 negatives at random, with probability proportional to the square root of the uni-gram frequency. We use a context window of size c, and uniformly sample the size c between 1 and 5. In order to subsample the most frequent words, we use a rejection threshold of 10−4 (for more details, see (Mikolov et al., 2013b)). When building the word dictionary, we keep the words that appear at least 5 times in the training set. The step size γ0 is set to 0.025 for the skipgram baseline and to 0.05 for both our model and the cbow baseline. These are the default values in the word2vec package and work well for our model too. Using this setting on English data, our model with character n-grams is approximately 1.5× slower to train than the skipgram baseline. Indeed, we process 105k words/second/thread versus 145k words/second/thread for the baseline. Our model is implemented in C++, and is publicly available.3










