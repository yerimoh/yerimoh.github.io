---
title: "[021] Deep learning 1:Transfer Learning"
date:   2020-02-18
excerpt: "ì „ì´ í•™ìŠµ(Transfer Learning), íŒŒì¸íŠœë‹(fine tuning),ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ,ê¸°ë³¸ ëª¨ë¸ ë™ê²°,ìƒˆ ë ˆì´ì–´ ì¶”ê°€,ëª¨ë¸ ì»´íŒŒì¼,ë°ì´í„° ì¦ê°•,ëª¨ë¸ íŠ¸ë ˆì´ë‹"
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---


# ëª©ì°¨
  * [comments: true](#comments--true)
- [ëª©ì°¨](#--)
- [ì‹œí€€ìŠ¤ ë°ì´í„°](#-------)
  * [êµ¬í˜„í•˜ê¸° ì „ ì½ì–´ ë³¼ ì§€ì‹](#--------------)
  * [ëª©í‘œ](#--)
- [ë°ì´í„° ì½ê¸°](#------)
  * [ë°ì´í„° ì •ë¦¬](#------)
    + [1ï¸âƒ£ NULLì œê±°](#1---null--)
    + [3ï¸âƒ£ í† í°í™”](#3------)
- [ì‹œí€€ìŠ¤ ìƒì„±](#------)
  * [ì‹œí€€ìŠ¤ íŒ¨ë”©](#------)
  * [ì˜ˆì¸¡ë³€ìˆ˜ ë° íƒ€ê¹ƒ ìƒì„±](#------------)
- [ëª¨ë¸ ìƒì„±](#-----)
  * [ì„ë² ë”© ë ˆì´ì–´](#-------)
  * [LSTM(Long Short Term Memory Layer)](#lstm-long-short-term-memory-layer-)
- [ëª¨ë¸ ì»´íŒŒì¼](#------)
- [ëª¨ë¸ íŠ¸ë ˆì´ë‹](#-------)
  * [ê²°ê³¼ ë¶„ì„](#-----)
- [ì˜ˆì¸¡ ìˆ˜í–‰](#-----)
      - [ë­”ì§€ ì¶”ê°€ í•„ë£Œ](#--------)
  * [ìƒˆ í—¤ë“œë¼ì¸ ìƒì„±](#---------)
- [ë©”ëª¨ë¦¬](#ë©”ëª¨ë¦¬)




----

# ì‹œí€€ìŠ¤ ë°ì´í„°
* **ë…ë¦½í˜• ë°ì´í„°**: ì‹œí€€ìŠ¤ê°€ ì•„ë‹˜, ex) ì´ë¯¸ì§€ ë°ì´í„°      
* **ì˜ì¡´í˜• ë°ì´í„°**: ì‹œí€€ìŠ¤, ë°ì´í„° í•­ëª©ì— ì˜ì¡´í•˜ëŠ” ë°ì´í„° ex) ê¸€ ë°ì´í„°, ì£¼ê°€ë°ì´í„°, ë‚ ì”¨ ë°ì´í„°, ì˜ìƒë°ì´í„°         
  * ë°ì´í„°ì˜ ì—˜ë¦¬ë¨¼íŠ¸ê°€ ì „í›„ì— ì˜¤ëŠ” ë‚´ìš©ê³¼ ê´€ê³„ë¥¼ ì§€ë‹˜     
  * ë…ë¦½í˜• ë°ì´í„°ì™€ ë‹¤ë¥¸ ì ‘ê¸‰ë°©ì‹ í•„ìš”     


## êµ¬í˜„í•˜ê¸° ì „ ì½ì–´ ë³¼ ì§€ì‹
* ğŸ“Œ**í•„ìˆ˜**ğŸ“Œ[ ]


## ëª©í‘œ
* [RNN(Recurrent Neural Network)]()ì— ì‚¬ìš©í•  ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„     
* ë‹¨ì–´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ëª¨ë¸ êµ¬ì¶• ë° íŠ¸ë ˆì´ë‹    
* í—¤ë“œë¼ì¸ ìƒì„±ê¸° ì œì‘
  * ê²€ìƒ‰ ì°½, íœ´ëŒ€í° ë˜ëŠ” í…ìŠ¤íŠ¸ í¸ì§‘ê¸° ë“±ì—ì„œ ë¬¸ì¥ ìë™ì™„ì„±ì„ ì œê³µí•˜ëŠ” í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ë³€ìˆ˜   


-----

# ë°ì´í„° ì½ê¸°
ëª‡ ê°œì›”ì— ê±¸ì³ ë‰´ìš• íƒ€ì„ì¦ˆ ì‹ ë¬¸ì—ì„œ ë°œì·Œëœ í—¤ë“œë¼ì¸ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ     
ë¨¼ì € ê¸°ì‚¬ì˜ ëª¨ë“  í—¤ë“œë¼ì¸ì„ ì½ì–´ë³´ê² ë‹¤   
[**CSV** í˜•ì‹](https://yerimoh.github.io/DL6.5/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9D%BD%EA%B8%B0)ì˜ ë°ì´í„°ë‹¤!    

```python
import os 
import pandas as pd

nyt_dir = 'data/nyt_dataset/articles/' ## ì“°ê³  ì‹¶ì€ ë°ì´í„°ì˜ ê²½ë¡œë¥¼ ì„¤ì •í•˜ë©´ ëœë‹¤!

all_headlines = []
for filename in os.listdir(nyt_dir):
    if 'Articles' in filename:
        # Read in all all the data from the CSV file
        headlines_df = pd.read_csv(nyt_dir + filename)
        # Add all of the headlines to our list
        all_headlines.extend(list(headlines_df.headline.values))
len(all_headlines)
```
```python
# 9335
```
ì²« í—¤ë“œë¼ì¸ì„ í™•ì¸í•´ë³´ì!!
<details>
<summary>ğŸ‘€ ê²°ê³¼ ë³´ê¸°</summary>
<div markdown="1">

```python
['My Beijing: The Sacred City',
 '6 Million Riders a Day, 1930s Technology',
 'Seeking a Cross-Border Conference',
 'Questions for: â€˜Despite the â€œYuck Factor,â€ Leeches Are Big in Russian Medicineâ€™',
 'Who Is a â€˜Criminalâ€™?',
 'An Antidote to Europeâ€™s Populism',
 'The Cost of a Speech',
 'Degradation of the Language',
 'On the Power of Being Awful',
 'Trump Garbles Pitch on a Revised Health Bill',
 'Whatâ€™s Going On in This Picture? | May 1, 2017',
 'Unknown',
 'When Patients Hit a Medical Wall',
 'Unknown',
 'For Pregnant Women, Getting Serious About Whooping Cough',
 'Unknown',
 'New York City Transit Reporter in Wonderland: Riding the London Tube',
 'How to Cut an Avocado Without Cutting Yourself',
 'In Fictional Suicide, Health Experts Say They See a Real Cause for Alarm',
 'Claims of Liberal Media Bias Hit ESPN, Too']
```

</div>
</details>


## ë°ì´í„° ì •ë¦¬
NLP ì‘ì—…ì˜ ì¤‘ìš”í•œ ë¶€ë¶„ì€ **ì»´í“¨í„°ê°€ ì´í•´**í•  ìˆ˜ ìˆëŠ” ë°©ì‹ìœ¼ë¡œ **í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬**í•˜ëŠ” ê²ƒì´ë‹¤      
1ï¸âƒ£ **NULLì œê±°**: ì´ëŸ¬í•œ í•­ëª©ì€ íŠ¸ë ˆì´ë‹ ì„¸íŠ¸ì— í¬í•¨í•˜ì§€ ì•Šì•„ì•¼ í•˜ë¯€ë¡œ í•„í„°ë§ì„ í†µí•´ ì œê±°          
2ï¸âƒ£ **íŠ¹ìˆ˜ë¬¸ì ì œê±°**: ì´ë¥¼ ë‹¨ì–´ë¡œ ì¸ì‹í•˜ì§€ ì•Šê²Œ ì œê±°í•´ì•¼ í•œë‹¤ -> í•˜ì§€ë§Œ Keras ```Tokenizer```ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ì‘ì—…ì„ ì „ë¶€ ìˆ˜í–‰ ê°€ëŠ¥í•˜ë¯€ë¡œ í•„ìš” X        
3ï¸âƒ£ **í† í°í™”**: ë°ì´í„°ì…‹ì— ë‚˜íƒ€ë‚˜ëŠ” ê° ë‹¨ì–´ë¥¼ ìˆ«ìë¡œ í‘œí˜„í•˜ëŠ” ê²ƒ        


### 1ï¸âƒ£ NULLì œê±°
```python
# Remove all headlines with the value of "Unknown"
all_headlines = [h for h in all_headlines if h != "Unknown"]
len(all_headlines)

## ê²°ê³¼: 8603
```
```python
all_headlines[:20]
```

<details>
<summary>ğŸ‘€ ì œê±° ê²°ê³¼ ë³´ê¸°</summary>
<div markdown="1">

```python
['My Beijing: The Sacred City',
 '6 Million Riders a Day, 1930s Technology',
 'Seeking a Cross-Border Conference',
 'Questions for: â€˜Despite the â€œYuck Factor,â€ Leeches Are Big in Russian Medicineâ€™',
 'Who Is a â€˜Criminalâ€™?',
 'An Antidote to Europeâ€™s Populism',
 'The Cost of a Speech',
 'Degradation of the Language',
 'On the Power of Being Awful',
 'Trump Garbles Pitch on a Revised Health Bill',
 'Whatâ€™s Going On in This Picture? | May 1, 2017',
 'When Patients Hit a Medical Wall',
 'For Pregnant Women, Getting Serious About Whooping Cough',
 'New York City Transit Reporter in Wonderland: Riding the London Tube',
 'How to Cut an Avocado Without Cutting Yourself',
 'In Fictional Suicide, Health Experts Say They See a Real Cause for Alarm',
 'Claims of Liberal Media Bias Hit ESPN, Too',
 'Is the dream in Australia crumbling?',
 'Police in Texas Change Account in Officerâ€™s Fatal Shooting of 15-Year-Old',
 'Most Adults Favor Sex Ed. Most Students Donâ€™t Get It.']
```
ìœ„ì˜ ê²°ê³¼ì™€ ë¹„êµí•´ë³´ë©´ 'Unknown' ê°’ì´ ì œê±°ë˜ì—ˆë‹¤!!
  
</div>
</details>

### 3ï¸âƒ£ í† í°í™”
í˜„ì¬ ë°ì´í„°ì„¸íŠ¸ê°€ ê°ê° **ì¼ë ¨ì˜ ë‹¨ì–´**ë¡œ ì´ë£¨ì–´ì§„ **í—¤ë“œë¼ì¸ ì„¸íŠ¸**ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ      
* ìš°ë¦¬ëŠ” ëª¨ë¸ì´ ì´í•´ ê°€ëŠ¥í•œ ë°©ì‹ìœ¼ë¡œ ì´ëŸ¬í•œ ë‹¨ì–´ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ëª¨ë¸ì— ì œê³µí•´ì•¼ í•¨    

**ì´ë¥¼ ìœ„í•´ í† í°í™” ì‚¬ìš©**    
* **1)** í…ìŠ¤íŠ¸ì˜ ì¡°ê°ì„ ë” ì‘ì€ ì²­í¬(í† í°)ìœ¼ë¡œ ë¶„ë¦¬(ì—¬ê¸°ì„  ë‹¨ì–´ ë‹¨ìœ„)      
* **2)** ê° ê³ ìœ  ë‹¨ì–´ì— ìˆ«ìê°€ í• ë‹¹ ë¨     
ì´ëŸ° ë°©ë²•ì„ í†µí•´ ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì´í•´ê°€ëŠ¥      

**[Kerasì˜ ë°ì´í„°ì˜ í† í°í™”ë¥¼ ë„ì™€ì£¼ëŠ” í´ë˜ìŠ¤]**     
* ```filters```: ë¬¸ìì—´ì€ ë¯¸ë¦¬ êµ¬ë‘ì ì„ ì œê±°    
* ```lower```: ë‹¨ì–´ë¥¼ ì†Œë¬¸ìë¡œ ì„¤ì •    
* ```num_words```: ë‹¨ì–´ ë¹ˆë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìœ ì§€í•  ìµœëŒ€ ë‹¨ì–´ ìˆ˜    
* ```split```: í† í°í™” í•˜ëŠ” ê¸°ì¤€(ì—¬ê¸°ì„œëŠ” ê³µë°±ìœ¼ë¡œí•˜ì—¬ ë„ì–´ì“°ê¸°(ë‹¨ì–´)ë¡œ ë‚˜ëˆ”)      
* ```char_level```: Trueì´ë©´ ëª¨ë“  ë¬¸ìê°€ í† í°ìœ¼ë¡œ ì²˜ë¦¬ë¨     
* ```oov_token```: ì§€ì •ëœ ê²½ìš° text_to_sequence í˜¸ì¶œ ì¤‘ì— ë‹¨ì–´ ì™¸ ë‹¨ì–´ë¥¼ ëŒ€ì²´í•˜ëŠ” ë° ì‚¬ìš©ë¨    
      
```python
tf.keras.preprocessing.text.Tokenizer(
    num_words=None, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower=True,
    split=' ', char_level=False, oov_token=None, document_count=0, **kwargs
)
```

```python
from tensorflow.keras.preprocessing.text import Tokenizer

# Tokenize the words in our headlines
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_headlines)
total_words = len(tokenizer.word_index) + 1
print('Total words: ', total_words)

## ê²°ê³¼
## Total words:  11753
```

**[ë‹¨ì–´ ì €ì¥ ë°©ì‹ í™•ì¸]**     
```word_index```: í† í¬ë‚˜ì´ì €ê°€ ë‹¨ì–´ë¥¼ **ì–´ë–»ê²Œ ì €ì¥**í•˜ëŠ”ì§€ í™•ì¸ê°€ëŠ¥(ì‚¬ì „ ì—­í• )         
```texts_to_sequences```: í† í¬ë‚˜ì´ì €ê°€ ë‹¨ì–´ë¥¼ ì €ì¥í•˜ëŠ” **ë°©ì‹**ì„ í™•ì¸ ê°€ëŠ¥     
```python
from tensorflow.keras.preprocessing.text import Tokenizer

# Print a subset of the word_index dictionary created by Tokenizer
subset_dict = {key: value for key, value in tokenizer.word_index.items() \
               if key in ['a','man','a','plan','a','canal','panama']}
print(subset_dict))

## ê²°ê³¼ 
## {'a': 2, 'plan': 82, 'man': 139, 'panama': 2732, 'canal': 7047}
```

```python
tokenizer.texts_to_sequences(['a','man','a','plan','a','canal','panama'])
## ê²°ê³¼ 
## [[2], [139], [2], [82], [2], [7047], [2732]]
```


-----

# ì‹œí€€ìŠ¤ ìƒì„±
ì´ì œ ê° ë‹¨ì–´ë¥¼ í‘œí˜„ ìˆ«ìë¡œ ë³€í™˜í•˜ë©´ **í—¤ë“œë¼ì¸ì—ì„œ í† í°ì˜ ì‹œí€€ìŠ¤ê°€ ìƒì„±**ë¨    
ì´ëŸ¬í•œ ì‹œí€€ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹     

ì´ì œ í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•´ í—¤ë“œë¼ì¸ì„ ì‹œí€€ìŠ¤ë¡œ ë°”ê¾¸ê² ë‹¤  

```python
# Convert data to sequence of tokens 
input_sequences = []
for line in all_headlines:
    # Convert our headline into a sequence of tokens
    token_list = tokenizer.texts_to_sequences([line])[0]
    
    # Create a series of sequences for each headline
    for i in range(1, len(token_list)):
        partial_sequence = token_list[:i+1]
        input_sequences.append(partial_sequence)

print(tokenizer.sequences_to_texts(input_sequences[:5]))
input_sequences[:5]
```

<details>
<summary>ğŸ‘€ ê²°ê³¼ ë³´ê¸°</summary>
<div markdown="1">

```python
['my beijing', 'my beijing the', 'my beijing the sacred', 'my beijing the sacred city', '6 million']
[[52, 1616],
 [52, 1616, 1],
 [52, 1616, 1, 1992],
 [52, 1616, 1, 1992, 125],
 [126, 346]]
```

  
</div>
</details>


## ì‹œí€€ìŠ¤ íŒ¨ë”©
[íŒ¨ë”©ì´ë€ ë­”ê°€ìš”?](######)
ì§€ê¸ˆì€ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ê°€ ë‹¤ì–‘í•¨   
í•˜ì§€ë§Œ ëª¨ë¸ì„ ë°ì´í„°ì— ëŒ€í•´ íŠ¸ë ˆì´ë‹í•  ìˆ˜ ìˆìœ¼ë ¤ë©´ ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ ê°™ì€ ê¸¸ì´ë¡œ ë§Œë“¤ì–´ì•¼ í•¨    

```pad_sequences```:  Kerasì— ë‚´ì¥ë˜ì–´ ìˆëŠ” íŒ¨ë”©ì„ ë§Œë“¤ì–´ ì£¼ëŠ” ë§¤ì„œë“œ      
* ```Args```: List of sequences(ê° ì‹œí€€ìŠ¤ëŠ” ì •ìˆ˜ ëª©ë¡ì„)    
* ```maxlen```: ì„ íƒ ì‚¬í•­ Int,ëª¨ë“  ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´. ì œê³µë˜ì§€ ì•Šì„ ê²½ìš° ì‹œí€€ìŠ¤ëŠ” ê°€ì¥ ê¸´ ê°œë³„ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¡œ íŒ¨ë”©ë¨     
* ```dtype```: (ì„ íƒì‚¬í•­, ê¸°ë³¸ê°’ì€ int32) ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ìœ í˜•     
* ```padding```    
   *  ```pre ```: defaultsê°’, ì‹œí€€ìŠ¤ ì•ì— íŒ¨ë”© ì¶”ê°€    
   *  ```post```:  ì‹œí€€ìŠ¤ ë’¤ì— íŒ¨ë”© ì¶”ê°€    
* ```truncating```    
   *  ```pre ```: defaultsê°’, maxlenë³´ë‹¤ í° ì‹œí€€ìŠ¤ì˜ ì‹œì‘ì—ì„œ ê°’ì„ ì œê±°      
   *  ```post```: maxlenë³´ë‹¤ í° ì‹œí€€ìŠ¤ì˜ ëì—ì„œ ê°’ì„ ì œê±°        
* ```value```:  Float or String, íŒ¨ë”© ê°’(ì„ íƒ ì‚¬í•­, ê¸°ë³¸ê°’ì€ 0).       

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Determine max sequence length
max_sequence_len = max([len(x) for x in input_sequences])

# Pad all sequences with zeros at the beginning to make them all max length
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))
input_sequences[0]
```

<details>
<summary>ğŸ‘€ ê²°ê³¼ ë³´ê¸°</summary>
<div markdown="1">

```python
array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
          0,    0,    0,    0,   52, 1616], dtype=int32)
```
  
</div>
</details>




## ì˜ˆì¸¡ë³€ìˆ˜ ë° íƒ€ê¹ƒ ìƒì„±
[ì¶”ê°€ì„¤ëª…](###########)
ì‹œí€€ìŠ¤ë¥¼ **ì˜ˆì¸¡ ë³€ìˆ˜ ë° íƒ€ê¹ƒìœ¼ë¡œ ë¶„í• **í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ      
* **íƒ€ê¹ƒ**: ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ë‹¨ì–´    
* **ì˜ˆì¸¡ ë³€ìˆ˜**: ì‹œí€€ìŠ¤ì˜ ì²« ë²ˆì§¸ ë‹¨ì–´    

```python
# Predictors are every word except the last
predictors = input_sequences[:,:-1]
# Labels are the last word
labels = input_sequences[:,-1]
labels[:5]

## ê²°ê³¼
## array([1616,    1, 1992,  125,  346], dtype=int32)
```

ì´ë¯¸ì§€ ë¶„ë¥˜ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì´ëŸ¬í•œ **íƒ€ê¹ƒì€ ë²”ì£¼í˜•**ì„    
ê°€ëŠ¥í•œ ì „ì²´ ì–´íœ˜ ì¤‘ì—ì„œ í•œ ê°œì˜ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•¨.     

ë„¤íŠ¸ì›Œí¬ê°€ ìŠ¤ì¹¼ë¼ ìˆ«ìë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëŒ€ì‹  **ë°”ì´ë„ˆë¦¬ ë²”ì£¼**ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ    
```python
from tensorflow.keras import utils

labels = utils.to_categorical(labels, num_classes=total_words)
```


----

# ëª¨ë¸ ìƒì„±
ìˆœì°¨ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ëª‡ ê°œì˜ ìƒˆ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ê² ë‹¤ 

## ì„ë² ë”© ë ˆì´ì–´
í† í°í™”ëœ ì‹œí€€ìŠ¤ë¥¼ ì·¨í•˜ì—¬ íŠ¸ë ˆì´ë‹ ë°ì´í„°ì„¸íŠ¸ì˜ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•œ ì„ë² ë”©ì„ í•™ìŠµí•¨    
* **ìˆ˜í•™ì  ì„ë² ë”©**: ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì˜ ë‰´ëŸ´ê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‘ë™      
* **ê°œë…ìƒ ì„ë² ë”©***: ì´ ëª©í‘œëŠ” í”¼ì²˜ì˜ ì¼ë¶€ë‚˜ ì „ë¶€ì˜ ì°¨ì› ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒ      
   * ì´ ê²½ìš°, ê° ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„     
   * ë²¡í„° ë‚´ì˜ ì •ë³´ì— ê° ë‹¨ì–´ ê°„ì˜ ê´€ê³„ê°€ í¬í•¨ë¨
   
```python
model.add(Embedding(input_dimension, output_dimension, input_length=input_len))
```

![image](https://user-images.githubusercontent.com/76824611/129435829-bc860d34-13f4-4cb3-b4c4-4339b2ddef7e.png)


## LSTM(Long Short Term Memory Layer)
LSTMì€ RNN(Recurrent Neural Network)ì˜ í•œ ìœ í˜•     
ì§€ê¸ˆê¹Œì§€ ë³¸ ê¸°ì¡´ì˜ í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ì™€ ë‹¬ë¦¬, ìˆœí™˜ ë„¤íŠ¸ì›Œí¬ëŠ” ë£¨í”„ê°€ í¬í•¨ë˜ì–´ ìˆì–´ ì •ë³´ ì§€ì†ì´ ê°€ëŠ¥í•˜ê²Œ í•¨    

```python
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.models import Sequential

# Input is max sequence length - 1, as we've removed the last word for the label
input_len = max_sequence_len - 1 

model = Sequential()

# Add input embedding layer
model.add(Embedding(total_words, 10, input_length=input_len))

# Add LSTM layer with 100 units
model.add(LSTM(100))
model.add(Dropout(0.1))

# Add output layer
model.add(Dense(total_words, activation='softmax'))
```

```python
model.summary()
```


<details>
<summary>ğŸ‘€ ê²°ê³¼ ë³´ê¸°</summary>
<div markdown="1">

```python
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 27, 10)            117530    
_________________________________________________________________
lstm (LSTM)                  (None, 100)               44400     
_________________________________________________________________
dropout (Dropout)            (None, 100)               0         
_________________________________________________________________
dense (Dense)                (None, 11753)             1187053   
=================================================================
Total params: 1,348,983
Trainable params: 1,348,983
Non-trainable params: 0
_________________________________________________________________
```

  
</div>
</details>


----


# ëª¨ë¸ ì»´íŒŒì¼
**ë²”ì£¼í˜• í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ë¡œ ëª¨ë¸ì„ ì»´íŒŒì¼**: ì „ì²´ ì–´íœ˜ì—ì„œ í•œ ë‹¨ì–´ë¥¼ ë²”ì£¼ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸      
* ì´ ê²½ìš°ì—ëŠ” í…ìŠ¤íŠ¸ ì˜ˆì¸¡ì´ ì´ë¯¸ì§€ ë¶„ë¥˜ì™€ ì–´ëŠ ì •ë„ ë™ì¼í•œ ì •í™•ë„ë¡œ ì¸¡ì •ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì •í™•ë„ë¥¼ ì§€í‘œë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ    

[**Adam**](https://yerimoh.github.io/DL5/#adam): LSTM ì‘ì—…ì— ì í•©   

```python
model.compile(loss='categorical_crossentropy', optimizer='adam')
```

----

# ëª¨ë¸ íŠ¸ë ˆì´ë‹
ëª¨ë¸ì„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë§ì¶°ì•¼ í•¨    
* 30ì—í¬í¬ì— ëŒ€í•´ íŠ¸ë ˆì´ë‹í•  ê²ƒì„      


```python
model.fit(predictors, labels, epochs=30, verbose=1)
```




## ê²°ê³¼ ë¶„ì„

<details>
<summary>ğŸ‘€ ê²°ê³¼ ë³´ê¸°</summary>
<div markdown="1">

```python
Epoch 1/30
1666/1666 [==============================] - 7s 4ms/step - loss: 7.8878
Epoch 2/30
1666/1666 [==============================] - 7s 4ms/step - loss: 7.4762
Epoch 3/30
1666/1666 [==============================] - 7s 4ms/step - loss: 7.2884
Epoch 4/30
1666/1666 [==============================] - 8s 5ms/step - loss: 7.0803
Epoch 5/30
1666/1666 [==============================] - 7s 4ms/step - loss: 6.8526
Epoch 6/30
1666/1666 [==============================] - 7s 4ms/step - loss: 6.6031
Epoch 7/30
1666/1666 [==============================] - 7s 4ms/step - loss: 6.3513
Epoch 8/30
1666/1666 [==============================] - 7s 4ms/step - loss: 6.0987
Epoch 9/30
1666/1666 [==============================] - 7s 4ms/step - loss: 5.8597
Epoch 10/30
1666/1666 [==============================] - 7s 4ms/step - loss: 5.6192
Epoch 11/30
1666/1666 [==============================] - 7s 4ms/step - loss: 5.4023
Epoch 12/30
1666/1666 [==============================] - 7s 4ms/step - loss: 5.1901
Epoch 13/30
1666/1666 [==============================] - 7s 4ms/step - loss: 4.9913
Epoch 14/30
1666/1666 [==============================] - 7s 4ms/step - loss: 4.8028
Epoch 15/30
1666/1666 [==============================] - 7s 4ms/step - loss: 4.6247
Epoch 16/30
1666/1666 [==============================] - 7s 4ms/step - loss: 4.4579
Epoch 17/30
1666/1666 [==============================] - 7s 4ms/step - loss: 4.3036
Epoch 18/30
1666/1666 [==============================] - 7s 4ms/step - loss: 4.1574
Epoch 19/30
1666/1666 [==============================] - 7s 4ms/step - loss: 4.0225
Epoch 20/30
1666/1666 [==============================] - 8s 5ms/step - loss: 3.8956
Epoch 21/30
1666/1666 [==============================] - 7s 4ms/step - loss: 3.7719
Epoch 22/30
1666/1666 [==============================] - 7s 4ms/step - loss: 3.6607
Epoch 23/30
1666/1666 [==============================] - 7s 4ms/step - loss: 3.5537
Epoch 24/30
1666/1666 [==============================] - 7s 4ms/step - loss: 3.4617
Epoch 25/30
1666/1666 [==============================] - 7s 4ms/step - loss: 3.3671
Epoch 26/30
1666/1666 [==============================] - 7s 4ms/step - loss: 3.2778
Epoch 27/30
1666/1666 [==============================] - 7s 4ms/step - loss: 3.1999
Epoch 28/30
1666/1666 [==============================] - 8s 5ms/step - loss: 3.1162
Epoch 29/30
1666/1666 [==============================] - 7s 4ms/step - loss: 3.0433
Epoch 30/30
1666/1666 [==============================] - 7s 4ms/step - loss: 2.9722
<tensorflow.python.keras.callbacks.History at 0x7f03e89a7048>
```

</div>
</details>

íŠ¸ë ˆì´ë‹ì´ ì§„í–‰ë˜ëŠ” ë™ì•ˆ ì†ì‹¤ì´ ê°ì†Œ    
â¡ ëª¨ë¸ì„ ì¶”ê°€ì ìœ¼ë¡œ íŠ¸ë ˆì´ë‹í•˜ì—¬ ì†ì‹¤ ê°ì†Œ ê°€ëŠ¥(í•˜ì§€ë§Œ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼)     


----

# ì˜ˆì¸¡ ìˆ˜í–‰
ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ë ¤ë©´ **í† í°í™”ì™€ íŒ¨ë”©** ìˆ˜í–‰ í•„ìš”    
í† í°í™”ì™€ íŒ¨ë”©ì´ ì™„ë£Œë˜ë©´ **ëª¨ë¸ì— ì „ë‹¬**í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰ ê°€ëŠ¥     

```predict_next_token```: ì´ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
#### ë­”ì§€ ì¶”ê°€ í•„ë£Œ

```python
def predict_next_token(seed_text):
    token_list = tokenizer.texts_to_sequences([seed_text])[0]
    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
    prediction = model.predict_classes(token_list, verbose=0)
    return prediction
```
  
```python
prediction = predict_next_token("today in new york")
prediction
```
  
<details>
<summary>ğŸ‘€ ê²°ê³¼ ë³´ê¸°</summary>
<div markdown="1">

```python
WARNING:tensorflow:From <ipython-input-31-bd91571ab9e2>:4: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.
Instructions for updating:
Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype("int32")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).
array([145])
```

  
</div>
</details>
  
ì´ë ‡ê²Œ ìˆ«ìë¡œ ë‚˜ì˜¤ë©´ ë¬´ìŠ¨ ë‹¨ì–´ì¸ì§€ ëª¨ë¥¸ë‹¤ã… ã… ã…     
ê·¸ëŸ¬ë¯€ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ **ë””ì½”ë”©**ì´ í•„ìš”í•˜ë‹¤    
```python
tokenizer.sequences_to_texts([prediction])
  
## ['rules']
```  
  
-----
  
## ìƒˆ í—¤ë“œë¼ì¸ ìƒì„±
ìƒˆ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìœ¼ë‹ˆ ì´ì œ ë‹¨ì–´ 2ê°œ ì´ìƒìœ¼ë¡œ ëœ í—¤ë“œë¼ì¸ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ê² ë‹¤!!   
```python
def generate_headline(seed_text, next_words=1):
    for _ in range(next_words):
        # Predict next token
        prediction = predict_next_token(seed_text)
        # Convert token to word
        next_word = tokenizer.sequences_to_texts([prediction])[0]
        # Add next word to the headline. This headline will be used in the next pass of the loop.
        seed_text += " " + next_word
    # Return headline as title-case
    return seed_text.title()
```   
  
ê·¸ëŸ¼ ì´ì œ ì—´ì‹¬íˆ ë§Œë“  ëª¨ë¸ì„ ì¨ë³´ì!!  
```python
seed_texts = [
    'washington dc is',
    'today in new york',
    'the school district has',
    'crime has become']
for seed in seed_texts:
    print(generate_headline(seed, next_words=5))
```    
```
Washington Dc Is Wiser Really It Donâ€™T Just
Today In New York Rules With The Global Year
The School District Has The Broken Lifts East History
Crime Has Become The Fictional Vibe Sandwiches The
```    
ë§‰ ì˜ ì˜ˆì¸¡í•˜ëŠ”ê±´ ì•„ë‹Œ ê²ƒ ê°™ë‹¤ ì™œëƒí•˜ë©´ ë¬¸ë²•ë„ ê·¸ë ‡ê³  ì–´ìƒ‰í•œ ë¶€ë¶„ì´ ë³´ì´ê¸° ë•Œë¬¸ì´ë‹¤.   
â¡ 30 ì—í¬í¬ë¡œ íŠ¸ë ˆì´ë‹ í–ˆê¸° ë•Œë¬¸ì´ë‹¤    
â¡ ë” ì¢‹ì€ ê²°ê³¼ê°’ì„ ì›í•˜ë©´ ì—í­ì„ ëŠ˜ë ¤ë³´ì!!
  
----

# ë©”ëª¨ë¦¬
ë„˜ì–´ê°€ê¸° ì „ì— ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ GPU ë©”ëª¨ë¦¬ë¥¼ ì§€ìš°ê¸°     
ì´ëŠ” ë‹¤ìŒ ë…¸íŠ¸ë¶ìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•œ í•„ìˆ˜ ì‘ì—…     
```python
import IPython
app = IPython.Application.instance()
app.kernel.do_shutdown(True)
```

  
  
  
  
