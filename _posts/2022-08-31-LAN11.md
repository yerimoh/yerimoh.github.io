---
title: "New Intent Discovery with Pre-training and Contrastive Learning ì •ë¦¬"
date:   2022-08-31
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


   

# ì› ë…¼ë¬¸
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)

**[ì†ŒìŠ¤ì½”ë“œ]**     
* [git adress](https://github.com/zhang-yu-wei/MTP-CLNN)


**[ì‚¬ì „ í•™ìŠµ]**
* ì½ê¸° ì „ ì•„ë˜ì˜ í¬ìŠ¤íŠ¸ë“¤ì„ ì½ì–´ì•¼ ë¬´ìŠ¨ì†Œë¦°ì§€ ì•Œì•„ë“£ê¸° í¸í•˜ë‹¤..!   
* [contrastive learning](https://daebaq27.tistory.com/97)    

---

# Abstract
ìƒˆë¡œìš´ ì˜ë„(intent) ë°œê²¬ì€ ì§€ì›ë˜ëŠ” **ì˜ë„(intent) í´ë˜ìŠ¤ ì„¸íŠ¸**ë¥¼ í™•ì¥í•˜ê¸° ìœ„í•´,       
**ì‚¬ìš©ì ë°œí™”ì—ì„œ ìƒˆë¡œìš´ ì˜ë„ ë²”ì£¼ë¥¼ ë°œê²¬**í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.       

ì´ ë°œê²¬ì€ ì‹¤ì§ˆì ì¸ ëŒ€í™” ì‹œìŠ¤í…œì˜ ê°œë°œê³¼ ì„œë¹„ìŠ¤ í™•ëŒ€ë¥¼ ìœ„í•œ ì¤‘ìš”í•œ ê³¼ì œì´ë‹¤.   

ì´ëŸ¬í•œ ì¤‘ìš”ì„±ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ ë¬¸ì œëŠ” ì—¬ì „íˆ ë¬¸í—Œì—ì„œ ì¶©ë¶„íˆ ì—°êµ¬ë˜ì§€ ì•Šê³  ìˆë‹¤.  

**[ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ì˜ ë¬¸ì œ]**        
* ì¼ë°˜ì ìœ¼ë¡œ ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°œí™”(labeled utterances)ì— ì˜ì¡´     
* í‘œí˜„ í•™ìŠµ ë° í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•´ pseudo-labeling ë°©ë²•ì„ ì‚¬ìš© â¡ ì´ëŠ” ë ˆì´ë¸” ì§‘ì•½ì ì´ê³  ë¹„íš¨ìœ¨ì ì´ë©° ë¶€ì •í™•í•˜ë‹¤.     



**[ë³¸ ë…¼ë¬¸ì˜ í•´ê²°ì±…]**    
* ìš°ë¦¬ëŠ” ìƒˆë¡œìš´ ì˜ë„ ë°œê²¬ì„ ìœ„í•œ ë‘ ê°€ì§€ ì¤‘ìš”í•œ ì—°êµ¬ ì§ˆë¬¸ì— ëŒ€í•œ ìƒˆë¡œìš´ í•´ê²°ì±…ì„ ì œê³µí•œë‹¤:    
* **(1)** ì˜ë¯¸ë¡ ì  ë°œí™” í‘œí˜„ì„ ë°°ìš°ëŠ” ë°©ë²•     
* **(2)** ë°œí™”ë¥¼ ë” ì˜ í´ëŸ¬ìŠ¤í„°ë§í•˜ëŠ” ë°©ë²•      
* íŠ¹íˆ, ìš°ë¦¬ëŠ” ë¨¼ì € í‘œí˜„ í•™ìŠµì„ ìœ„í•´ ì™¸ë¶€ ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°ì´í„°ì™€ í•¨ê»˜ ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ í’ë¶€í•œ ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” multi-task pre-training ì „ëµì„ ì œì•ˆí•œë‹¤.     
* ê·¸ëŸ° ë‹¤ìŒ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•´ ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°ì´í„°ì—ì„œ self-supervisory ì‹ í˜¸ë¥¼ ì´ìš©í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ëŒ€ì¡° ì†ì‹¤(contrastive loss)ì„ ì„¤ê³„í•œë‹¤.    
* ì„¸ ê°€ì§€ ì˜ë„ ì¸ì‹ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ì€ ì œì•ˆëœ ë°©ë²•ì˜ ë†’ì€ íš¨ê³¼ë¥¼ ì…ì¦í•˜ëŠ”ë°, ì´ëŠ” ë¹„ì§€ë„ ë° ì¤€ì§€ë„ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë‘ì—ì„œ ìµœì²¨ë‹¨ ë°©ë²•ì„ í° í­ìœ¼ë¡œ ëŠ¥ê°€í•œë‹¤. 


---
---


# **1 Introduction**

## Why Study New Intent Discovery (NID)      
ìµœê·¼ ëª‡ ë…„ ë™ì•ˆ ëŒ€í™”í˜• AI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê¸‰ì†í•œ ì„±ì¥ì„ ëª©ê²©í–ˆë‹¤.   

**[ë¬¸ì œ]**      
ìì—°ì–´ ì´í•´ ì‹œìŠ¤í…œ(NLU)ì„ ì„¤ê³„í•˜ê¸° ìœ„í•´, ì˜ë„ ì¸ì‹ ëª¨ë¸(intent recognition model)ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ **ë¯¸ë¦¬** **ì˜ˆìƒ ê³ ê°ì˜ ì˜ë„ ì„¸íŠ¸(predefined intents)ë¥¼ ìˆ˜ì§‘**í•œë‹¤.    
ê·¸ëŸ¬ë‚˜ ë¯¸ë¦¬ ì •ì˜ëœ ì˜ë„(predefined intents)ëŠ” ê³ ê°ì˜ ìš”êµ¬ë¥¼ ì™„ì „íˆ ì¶©ì¡±ì‹œí‚¬ ìˆ˜ ì—†ë‹¤.      
â¡ ì´ëŠ” ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ì‚¬ìš©ì ë°œí™”ì—ì„œ ë°œê²¬ëœ ìƒˆë¡œìš´ ì˜ë„ë¥¼ ë°˜ë³µì ìœ¼ë¡œ í†µí•©í•˜ì—¬ **ì˜ë„ ì¸ì‹ ëª¨ë¸ì„ í™•ì¥í•  í•„ìš”ì„±**ì„ ì‹œì‚¬í•œë‹¤
![image](https://user-images.githubusercontent.com/76824611/187633563-3b928c35-4da8-42d9-b82b-b4f7be06c397.png)


**[í•´ê²°ì±…]**      
ë‹¤ìˆ˜ì˜ ë°œí™”ì—ì„œ ì•Œë ¤ì§€ì§€ ì•Šì€ ì˜ë„ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‹ë³„í•˜ëŠ” ë…¸ë ¥ì„ ì¤„ì´ê¸° ìœ„í•´, ì´ì „ ì—°êµ¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì˜ë„ë¥¼ ê°€ì§„ ë°œí™”ë¥¼ ê·¸ë£¹í™”í•˜ê¸° ìœ„í•´ clustering algorithmsì„ ì‚¬ìš©í•œë‹¤(Cheung and Li, 2012; Hakkani-TÃ¼r et al., 2015; Padmasundari, 2018).     
â¡ ì´í›„ í´ëŸ¬ìŠ¤í„° í• ë‹¹ì€ **ìƒˆë¡œìš´ ì˜ë„ ë ˆì´ë¸”ë¡œ ì§ì ‘ ì‚¬ìš©**í•˜ê±°ë‚˜ **ë” ë¹ ë¥¸ ì£¼ì„ì„ ìœ„í•œ íœ´ë¦¬ìŠ¤í‹±**ìœ¼ë¡œ ì‚¬ìš©ê°€ëŠ¥í•¨


**[ê°œìš”]**    
ì•„ë˜ì˜ 2ê°€ì§€ ë°©ë²•ìœ¼ë¡œ NIDë¥¼ í•´ê²°
1) multi-task pre-training method     
2) í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ contrastive learning      


----


## Research Questions (RQ) and Challenges

NID centersì— ëŒ€í•œ í˜„ì¬ ì—°êµ¬ëŠ” ë‘ ê°€ì§€ ê¸°ë³¸ ì—°êµ¬ ì§ˆë¬¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤.     

**1)** í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ì ì ˆí•œ ë‹¨ì„œë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ **ì˜ë¯¸ë¡ ì  ë°œí™” í‘œí˜„ì„ ë°°ìš°ëŠ”** ë°©ë²•?    
**2)** ë°œí™”ë¥¼ **ë” ì˜ í´ëŸ¬ìŠ¤í„°ë§**í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€?   

ê·¸ ë‘ ì§ˆë¬¸ì— ëŒ€í•œ ì—°êµ¬ëŠ” ì¢…ì¢… ê¸°ì¡´ ì—°êµ¬ì—ì„œ ì„œë¡œ ì–½í˜€ ìˆë‹¤.    
ë°œí™”ëŠ” ë‹¤ì–‘í•œ ìš”ì†Œì— ë”°ë¼ í‘œí˜„ëœë‹¤ (ì–¸ì–´ì˜ ìŠ¤íƒ€ì¼, ê´€ë ¨ëœ ì£¼ì œ ë˜ëŠ” ì‹¬ì§€ì–´ ë¬¸ì¥ì˜ ê¸¸ì´ì™€ ê°™ì€ ë‹¤ì–‘í•œ ì¸¡ë©´)     

**[ì‚¬ì „ ì—°êµ¬ì˜ í•œê³„]**    
í´ëŸ¬ìŠ¤í„°ë§ì— ëŒ€í•œ ì ì ˆí•œ ë‹¨ì„œë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ **ì˜ë¯¸ë¡ ì  ë°œì„±(semantic utterance) í‘œí˜„ì„ ë°°ìš°ëŠ” ê²ƒ**ì´ ì¤‘ìš”í•˜ë‹¤.     
* **vanilla pre-trained language model:**     
    * ë°œì„± í‘œí˜„ì„ ìƒì„±í•˜ê¸° ìœ„í•´ vanilla pre-trained language model (PLM)ì„ ë‹¨ìˆœíˆ ì ìš©í•˜ëŠ” ê²ƒì€ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±…ì´ ì•„ë‹˜      
    * ì´ëŠ” 4.2ì ˆì˜ ì‹¤í—˜ ê²°ê³¼ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ NIDì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ëœë‹¤.      
* **known intentsì˜ labeled utterances ì‚¬ìš©:**     
    * ì¼ë¶€ ìµœê·¼ ì—°êµ¬ëŠ” í‘œí˜„ í•™ìŠµ(representation learning)ì„ ìœ„í•´ known intentsì˜ labeled utterances ì‚¬ìš©í•  ê²ƒì„ ì œì•ˆí–ˆì§€ë§Œ,     
      íŠ¹íˆ ê°œë°œì˜ ì´ˆê¸°ë‹¨ê³„ì—ì„œ ê·¸ë“¤ì€ ìƒë‹¹í•œ ì–‘ì˜ ì´ë¯¸ ì•Œë ¤ì§„ ì˜ë„ì™€ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥í•œ ê° ì˜ë„ì˜ **ì´ë¯¸ ì¶©ë¶„íˆ ë ˆì´ë¸”ë§ëœ ë°œí™”**ë¥¼ í•„ìš”ë¡œ í•œë‹¤.          
      (Forman ë“±, 2015; Haponchyk ë“±, 2018; Lin ë“±, 2020; Zhang ë“±, 2021c; Haponchyk ë° Moschitti, 2021)
* **pseudo-labeling ì ‘ê·¼ ë°©ì‹:**        
    * ë˜í•œ pseudo-labeling ì ‘ê·¼ ë°©ì‹ì€ í‘œí˜„ í•™ìŠµ ë° í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ê°ë… ì‹ í˜¸(supervision signals)ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì¢…ì¢… ì‚¬ìš©ëœë‹¤.      
    * **Lin et al. (2020):** ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°œí™”ì— ëŒ€í•œ **ë°œí™” ìœ ì‚¬ì„± ì˜ˆì¸¡ ì‘ì—…**(utterance similarity prediction task)ìœ¼ë¡œ,      
    vanilla pre-trained language model(PLM)ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ **ìœ ì‚¬ ë ˆì´ë¸”ì´ ìˆëŠ” ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ë¥¼ train**í•œë‹¤.      
    * **Zhang et al.(2021c):**ì€ k-means í´ëŸ¬ìŠ¤í„°ë§ì„ ì‚¬ìš©í•˜ì—¬ **pseudo-labelsì„ ìƒì„±í•˜ëŠ” ì‹¬ì¸µ í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•**(Caron et al., 2018)ì„ ì±„íƒí•œë‹¤.          
    â¡ ê·¸ëŸ¬ë‚˜ pseudo-labelsì€ ì¢…ì¢… ë…¸ì´ì¦ˆê°€ ë§ê³  ì˜¤ë¥˜ ì „íŒŒë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤.      









---



## Our Solutions    

ë³¸ ì—°êµ¬ì—ì„œëŠ” ê° ì—°êµ¬ ì§ˆë¬¸ì— ëŒ€í•œ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ í•´ê²°ì±…ì„ ì œì•ˆí•œë‹¤.      

**[Solution to RQ 1: multi-task pre-training]**          
* representation learningì„ ìœ„í•´ ì™¸ë¶€ ë°ì´í„°ì™€ ë‚´ë¶€ ë°ì´í„°ë¥¼ ëª¨ë‘ í™œìš©í•˜ëŠ” a multi-task pre-training ì „ëµì„ ì œì•ˆí•œë‹¤.      
* **ë°©ë²•**: NIDì— ëŒ€í•œ task-specific utterance representationsì„ í•™ìŠµí•˜ê¸° ìœ„í•´ pre-trained PLMì„ fine-tuneí•œë‹¤.      
   * **fine-tune ë°©ë²• 1:** ë³¸ ë…¼ë¬¸ì—ì„  Zhang et al. (2021d)ì— ì´ì–´ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ê³ í’ˆì§ˆ ì˜ë„ íƒì§€ ë°ì´í„° ì„¸íŠ¸( high-quality intent detection datasets) ì´ìš©          
   * **fine-tune ë°©ë²• 2:** í˜„ì¬ ë„ë©”ì¸ì—ì„œ ì œê³µë˜ëŠ” ë ˆì´ë¸”ì´ ì§€ì •ëœ ë° ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°œí™”ë¥¼ í™œìš© 
* **íš¨ê³¼:** multi-task í•™ìŠµ ì „ëµì€ ì¼ë°˜ì ì¸ ì˜ë„ íƒì§€ ì‘ì—…ì—ì„œ **íŠ¹ì • ì‘ìš© í”„ë¡œê·¸ë¨ ì˜ì—­ìœ¼ë¡œ ì§€ì‹ì„ ì „ë‹¬**í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.     

**[Solution to RQ 2: contrastive learning with nearest neighbors]**         
* ë³¸ ë…¼ë¬¸ì—ì„  **ëŒ€ì¡°ì  ì†ì‹¤ì„ ì‚¬ìš©í•˜ì—¬ ì†Œí˜• í´ëŸ¬ìŠ¤í„°(contrastive loss to produce compact clusters)ë¥¼ ìƒì„±**í•  ê²ƒì„ ì œì•ˆí•œë‹¤.           
   * ì´ í•™ìŠµì€ ì»´í“¨í„° ë¹„ì „(Bachman et al., 2019; He et al., 2019; Chen et al., 2020; Khosla et al., 2020)ê³¼ ìì—°ì–´ ì²˜ë¦¬(Gunel et al., 2021; Gao et al., 2021; Yan et al. 2021) ëª¨ë‘ì—ì„œ ìµœê·¼ ëŒ€ì¡° í•™ìŠµì˜ ì„±ê³µì— ë™ê¸°ë¥¼ ë¶€ì—¬í•  ì •ë„ë¡œ ìœ ì˜ë¯¸í•œ ë°©ë²•ì„.       
* **contrastive learning(ëŒ€ì¡° í•™ìŠµ):**   
   * ì‚¬ì „ì— ì •ë‹µ ë°ì´í„°ë¥¼ êµ¬ì¶•í•˜ì§€ ì•ŠëŠ” íŒë³„ ëª¨ë¸     
   * í•™ìŠµëœ í‘œí˜„ ê³µê°„ ìƒì—ì„œ **"ë¹„ìŠ·í•œ" ë°ì´í„°ëŠ” ê°€ê¹ê²Œ**, "ë‹¤ë¥¸" ë°ì´í„°ëŠ” ë©€ê²Œ ì¡´ì¬í•˜ë„ë¡ **í‘œí˜„ ê³µê°„ì„ í•™ìŠµ**         
   * ì´í›„, classification ë“± ë‹¤ì–‘í•œ downstream taskì— ëŒ€í•´ì„œ ë„¤íŠ¸ì›Œí¬ë¥¼ fine-tuningì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ í™œìš©í•¨.    
   * **ì¥ì :**     
      * ë°ì´í„° êµ¬ì¶• ë¹„ìš©ì´ ë“¤ì§€ ì•ŠìŒ     
      * í•™ìŠµ ê³¼ì •ì— ìˆì–´ì„œ ë³´ë‹¤ ìš©ì´      
      * labelì´ ì—†ê¸° ë•Œë¬¸ì— ë³´ë‹¤ ì¼ë°˜ì ì¸ feature representation ê°€ëŠ¥      
      * ìƒˆë¡œìš´ classê°€ ë“¤ì–´ì™€ë„ ëŒ€ì‘ì´ ê°€ëŠ¥    
   * **ë‹¨ì :** ê·¸ëŸ¬ë‚˜ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì‹ë³„ ì‘ì—…(instance discrimination task)ì€ false negativesì„ ë°€ì–´ë‚´ê³  í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ì„ í•´ì¹  ìˆ˜ ìˆë‹¤.           
   * ì•„ë˜ ë‘ ê°œì˜ ë°©ë²• ëª¨ë‘ì— ë”°ë¼ **í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ contrastive lossì„ ë§ì¶¤í™”**í•˜ê¸° ìœ„í•œ **neighborhood relationshipë¥¼ ì†Œê°œ**í•œë‹¤.
     * **ë°©ë²• 1:** unsupervised(ì¦‰, ì•Œë ¤ì§„ intentsì˜ labeled utterancesì—†ì´)     
     * **ë°©ë²• 2:** semi-supervised scenarios 
   * ì§ê´€ì ìœ¼ë¡œ, ì˜ë¯¸ë¡ ì  íŠ¹ì§• ê³µê°„ì—ì„œ, ì´ì›ƒí•œ ë°œí™”ëŠ” ìœ ì‚¬í•œ ì˜ë„ë¥¼ ê°€ì ¸ì•¼ í•˜ë©°, ì´ì›ƒí•œ ìƒ˜í”Œì„ í•¨ê»˜ ë‹¹ê¸°ë©´ í´ëŸ¬ìŠ¤í„°ê°€ ë” ì½¤íŒ©íŠ¸í•´ì§„ë‹¤.      
* **ë³¸ ë…¼ë¬¸ì˜ 3ê°€ì§€  main contributions**    
   * ìš°ë¦¬ëŠ” ì œì•ˆëœ multi-task pretraining ë°©ë²•ì´ ì´ë¯¸ unsupervised NIDì™€ semi-supervised NID ëª¨ë‘ì— ëŒ€í•œ ìµœì²¨ë‹¨ ëª¨ë¸ì— ë¹„í•´ í° ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ì´ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.     
   * ìš°ë¦¬ëŠ” ì´ì›ƒ ê´€ê³„ë¥¼ contrastive í•™ìŠµ ëª©í‘œì— í†µí•©í•˜ì—¬ NIDì— ëŒ€í•œ self-supervised clustering ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¨ë‹¤.       
   * ìš°ë¦¬ëŠ” ë°©ë²•ì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ ì„¸ ê°€ì§€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì‹¤í—˜ê³¼ ì ˆì œ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•œë‹¤.     




----
----

# **2. Related Works**

## New Intent Discovery.    

NIDì— ëŒ€í•œ ì—°êµ¬ëŠ” ì•„ì§ ì´ˆê¸° ë‹¨ê³„ì— ìˆë‹¤. ì„ êµ¬ì ì¸ ì‘ì—…ì€ unsupervised clustering ë°©ë²•ì— ì¤‘ì ì„ ë‘”ë‹¤.     

**[ì„ í–‰ì—°êµ¬]**    
* **Shi et al. (2018):** ìë™ ì¸ì½”ë”ë¥¼ í™œìš©í•˜ì—¬ ê¸°ëŠ¥ì„ ì¶”ì¶œ.   
* **Perkins and Yang (2019):** ëŒ€í™”ì—ì„œ ë°œì–¸ì˜ ë§¥ë½ì„ ê³ ë ¤            
* **Chatterjee and Sengupta (2020):** ë°€ë„ ê¸°ë°˜ ëª¨ë¸(density-based models)ì„ ê°œì„ í•  ê²ƒì„ ì œì•ˆ             
* **ì¼ë¶€ ìµœê·¼ ì—°êµ¬(Haponchyk et al., 2018; Haponchyk and Moschitti, 2021):** intent labelingì„ ìœ„í•œ  supervised clustering ì•Œê³ ë¦¬ì¦˜ì„ ì—°êµ¬í–ˆì§€ë§Œ **ìƒˆë¡œìš´ ì˜ë„ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ë‹¤**.         
* **ë‹¤ë¥¸ ì‘ì—… ë¼ì¸(Forman et al., 2015; Lin et al., 2020; Zhang et al., 2021c):** ì¢…ì¢… semisupervised NIDë¡œ ë¶ˆë¦¬ëŠ” **ì•Œë ¤ì§€ì§€ ì•Šì€ intentsì˜ ë°œê²¬ì„ ì§€ì›**í•˜ê¸° ìœ„í•´ ì•Œë ¤ì§„ ì¼ë¶€ intentsê°€ ì œê³µë˜ëŠ” ë³´ë‹¤ ì‹¤ìš©ì ì¸ ì‚¬ë¡€ë¥¼ ì¡°ì‚¬í–ˆë‹¤.   
* **Lin et al.(2020)**:  semi-supervised NIDë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ ë¨¼ì € ë¬¸ì¥ **ìœ ì‚¬ì„± ì‘ì—…**ìœ¼ë¡œ **ì•Œë ¤ì§„ intents**ì— ëŒ€í•œ ì§€ë„ êµìœ¡ì„ ìˆ˜í–‰í•œ ë‹¤ìŒ **ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°œí™”**ì— ëŒ€í•œ pseudo labelingì„ ì‚¬ìš©í•˜ì—¬ ë” ë‚˜ì€ ì„ë² ë”© ê³µê°„ì„ í•™ìŠµí•  ê²ƒì„ ì œì•ˆ      
* **Zhang et al.(2021c)**: ë¨¼ì € ì•Œë ¤ì§„ intentsë¥¼ pre-trainí•œ ë‹¤ìŒ k-means clusteringì„ ìˆ˜í–‰í•˜ì—¬ Deep Clusteringì— ì´ì–´ representation learningì„ ìœ„í•´ ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°ì´í„°ì— pseudo labelingì„ í• ë‹¹í•  ê²ƒì„ ì œì•ˆí–ˆë‹¤      
* **Caron et al., (2018):** ìµœìƒìœ„ ê³„ì¸µì˜ í•™ìŠµì„ ê°€ì†í™”í•˜ê¸° ìœ„í•´ **í´ëŸ¬ìŠ¤í„°ë¥¼ ì •ë ¬**í•  ê²ƒì„ ì œì•ˆí–ˆë‹¤.     
* **Vedula et al., 2020; Zhang et al., 2021b:** ë¨¼ì € ë°œí™”ë¥¼ ì•Œë ¤ì§„ ê²ƒê³¼ ì•Œë ¤ì§€ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë¶„ë¥˜í•œ ë‹¤ìŒ ì•Œë ¤ì§€ì§€ ì•Šì€ ë°œí™”ë¡œ ìƒˆë¡œìš´ ì˜ë„ë¥¼ ë°íˆëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ê·¸ê²ƒì€ **ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œ ì •í™•í•œ ë¶„ë¥˜ì— ì˜ì¡´**í•œë‹¤.      


**[ë³¸ ì—°êµ¬]**      
ë³¸ ì—°êµ¬ì—ì„œ, ìš°ë¦¬ëŠ” í‘œí˜„ í•™ìŠµì„ ìœ„í•œ multi-task pre-training methodì™€ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ contrastive learning ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ NIDë¥¼ í•´ê²°í•œë‹¤.    
pre-trainì„ ìœ„í•´ í˜„ì¬ ë„ë©”ì¸ì˜ ì£¼ì„ì´ ë‹¬ë¦° ì¶©ë¶„í•œ ë°ì´í„°ì— ì˜ì¡´í•˜ëŠ” ì´ì „ ë°©ë²•ê³¼ ë‹¬ë¦¬, ìš°ë¦¬ì˜ ë°©ë²•ì€ unsupervisedí•œ í™˜ê²½ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆê³  data-scarce scenariosì—ì„œ ì˜ ì‘ë™í•  ìˆ˜ ìˆë‹¤(ì„¹ì…˜ 4.3).     



------



## Pre-training for Intent Recognition.    
**[ë¬¸ì œ]**    
large-scale pre-trained ì–¸ì–´ ëª¨ë¸ì˜ íš¨ê³¼ì—ë„ ë¶ˆêµ¬í•˜ê³ ,     
pre-trained ë°ì´í„° ì„¸íŠ¸ì™€ ëŒ€í™” ì‚¬ì´ì˜ ì–¸ì–´ í–‰ë™ì˜ ê³ ìœ í•œ ë¶ˆì¼ì¹˜ì˜ ë¬¸ì œê°€ ì§€ì†ë˜ê³ ìˆë‹¤.        


**[ì‚¬ì „ ì—°êµ¬]**    
ëŒ€ë¶€ë¶„ì˜ ì´ì „ ì—°êµ¬ëŠ” selfsupervised ë°©ì‹ìœ¼ë¡œ ê°œë°©í˜• ë„ë©”ì¸ ëŒ€í™”ì— ëŒ€í•œ pre-trainì„ ì œì•ˆí–ˆë‹¤      
ìµœê·¼ ì—¬ëŸ¬ ì‘í’ˆì—ì„œ **ê´€ë ¨ ê³¼ì œë¥¼ ê°€ì§„ pre-train**ì´ **intent ì¸ì‹ì— íš¨ê³¼ì **ì¼ ìˆ˜ ìˆë‹¤ëŠ” ì§€ì ì´ ë‚˜ì™”ë‹¤.     
* Zhang et al. (2020): intent ì¸ì‹ì„ ë¬¸ì¥ ìœ ì‚¬ì„± ê³¼ì œë¡œ ê³µì‹í™”í•˜ê³  ìì—°ì–´ ì¶”ë¡ (NLI) ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ pre-trainì„ ë°›ì•˜ë‹¤.       
* Vulic et al.(2021); Zhang et al.(2021e): intent íƒì§€ ì‘ì—…ì—ì„œ contrastive lossë¡œ ì‚¬ì „ í›ˆë ¨ì„ ë°›ì•˜ë‹¤.      


**[ë³¸ ë…¼ë¬¸]**     
ë³¸ ë…¼ë¬¸ì˜ multi-task pre-training methodëŠ” í˜„ì¬ **ë„ë©”ì¸ì˜ ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°**ì™€ **ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ intent ë°ì´í„° ì„¸íŠ¸**ë¥¼ í™œìš©í•˜ì—¬ few-shot intent íƒì§€ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” Zhang ì™¸(2021d)ì—ì„œ ì˜ê°ì„ ë°›ì•˜ë‹¤.      
âœ¨ ê·¸ëŸ¬ë‚˜ **ë ˆì´ë¸”ì´ ì—†ëŠ” ë°œí™”ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì¡´ì¬**ë¡œ ì¸í•´ ë³¸ ë…¼ë¬¸ì´ ì œì•ˆí•œ ë°©ë²•ì´ NIDì— ì ìš©í•˜ê¸°ì— ë” ì í•©í•˜ë‹¤ê³  ì£¼ì¥í•œë‹¤.






-----


## Contrastive Representation Learning.   
Contrastive learningì€ ì»´í“¨í„° ë¹„ì „ì—ì„œ ìœ ë§í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆê³  ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì¸ê¸°ë¥¼ ì–»ì—ˆë‹¤.      


**[ì‚¬ì „ ì—°êµ¬]**    
* **ì¼ë¶€ ìµœê·¼ ì—°êµ¬:** ë¬¸ì¥ ì„ë² ë”©ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ unsupervised Contrastive learningì„ ì‚¬ìš©í–ˆë‹¤    
êµ¬ì²´ì ìœ¼ë¡œ, contrastive lossì´ ë¹„ë“±ë°©ì„±(anisotropic) ì„ë² ë”© ê³µê°„ì„ í”¼í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.    
* **Kim et al.(2021)**: BERT í‘œí˜„ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ self-guided contrastive trainingì„ ì œì•ˆí–ˆë‹¤.    
* **Giorgi et al.(2021):** ì¸ê·¼ ë¬¸ì¥ì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§ëœ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ëŒ€ì¡°í•˜ì—¬ universal sentence encoderë¥¼ ì‚¬ì „ êµìœ¡í•  ê²ƒì„ ì œì•ˆí–ˆë‹¤.     
* **Zhang ì™¸(2021e):** self-supervised contrastive pre-trainingê³¼ supervised contrastive fine-tuningì´ few-shot intent ì¸ì‹ì— ë„ì›€ì´ ë  ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆë‹¤.    
* **Zhang et al. (2021a):** contrastive lossì„ í´ëŸ¬ìŠ¤í„°ë§ ëª©í‘œì™€ ê²°í•©í•˜ë©´ ì§§ì€ í…ìŠ¤íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ì„ ê°œì„ í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.    


<details>
<summary>ğŸ“œ anisotropic ì„¤ëª… ë³´ê¸°</summary>
<div markdown="1">
   
**Isotropic:** ë²¡í„°ê°€ ì£¼ì–´ì§„ ê³µê°„ì—ì„œ ëª¨ë“  ë°©í–¥ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ìƒí™©ì„ ì˜ë¯¸í•œë‹¤. ì„ë² ë”©ì— ëŒ€ì…í•´ë³´ë©´ ì£¼ì–´ì§„ ë³´ìº¡ì˜ ì„ë² ë”© ë²¡í„°ë“¤ì´ ì‚¬ë°©íŒ”ë°©ìœ¼ë¡œ í–¥í•´ ìˆëŠ” ìƒí™©ì´ë‹¤.


**anisotropic:** ë²¡í„°ê°€ ì£¼ì–´ì§„ ê³µê°„ì—ì„œ íŠ¹ì • ë°©í–¥ìœ¼ë¡œë§Œ í–¥í•´ ìˆì–´ì„œ ì¼ì¢…ì˜ cone í˜•íƒœë¥¼ ì´ë£¨ê³  ìˆëŠ” ê²ƒì„ ë§í•œë‹¤  


**ì´ìƒì ì¸** ìƒí™©ì´ë¼ë©´ ì„ë² ë”© ë²¡í„°ëŠ” **Isotropic**í•´ì•¼ í•œë‹¤.    
ê·¸ë˜ì•¼ ê° ë‹¨ì–´ì˜ ì˜ë¯¸ê°€ ëª…í™•íˆ êµ¬ë¶„ë˜ê³ , ìœ ì‚¬í•œ ë‹¨ì–´ëŠ” ì„œë¡œ ê°€ê¹ê²Œ, ì˜ ëš±ë”´ì§€ì¸ ë‘ ë‹¨ì–´ëŠ” ì„œë¡œ ë©€ë¦¬ ë¶„í¬í•˜ê²Œ ë  ê²ƒì´ë‹¤. ë§Œì•½ Anisotropicí•˜ê²Œ ì„ë² ë”© ë²¡í„°ê°€ ë¶„í¬í•´ ìˆë‹¤ë©´, ë‹¨ì–´ì˜ ì˜ë¯¸/ë¬¸ë²•ì  ìœ ì‚¬ë„ì™€ ê´€ê³„ì—†ì´ ëª¨ë“  ë‹¨ì–´ë“¤ì´ ê°€ê¹ê²Œ ë¶„í¬í•˜ê¸° ë•Œë¬¸ì— ê° ë‹¨ì–´ì˜ ì˜ë¯¸ê°€ ëª…í™•íˆ êµ¬ë¶„ë˜ì§€ ì•Šì„ ìš°ë ¤ê°€ ìˆë‹¤.   
![image](https://user-images.githubusercontent.com/76824611/187747971-fbdb3733-a539-4858-a2ef-6c6dcc4a1488.png)

[ì¶œì²˜](https://velog.io/@stapers/Contextual-Embedding-How-Contextual-are-Contextualized-Word-Representations)

  
</div>
</details>  

**[ë³¸ ë…¼ë¬¸]**     
ë³¸ ë…¼ë¬¸ì´ ì œì•ˆí•œ contrastive lossëŠ” í´ëŸ¬ìŠ¤í„°ë§ì— ë§ê²Œ ì¡°ì •ë˜ì–´,     
**ìœ ì‚¬í•œ ì˜ë¯¸ë¡ **ì„ ê°€ì§„ **ë°œí™”**ë¥¼ í•¨ê»˜ **ê·¸ë£¹í™”**í•˜ê³      
**ê¸°ì¡´ì˜ ëŒ€ì¡° ì†ì‹¤**ì—ì„œì²˜ëŸ¼ **false negativesì„ ë°€ì–´ë‚´ëŠ” ê²ƒì„ í”¼í•œë‹¤**.    



-----
-----

# **3 Method**    



## Problem Statement

**[ì´ë¯¸ ë¼ë²¨ë§ ëœ ë°ì´í„°ë¥¼ ì¤€ë¹„]**     
* intent recognition modelì„ ê°œë°œí•˜ê¸° ìœ„í•´,      
* ì£¼ì„ì´ ë‹¬ë¦° ëª‡ ê°€ì§€ ë°œì–¸(annotated utterances)ê³¼ í•¨ê»˜       
* ê° intentì— ëŒ€í•œ ì•„ë˜ ì‹ ì¤€ë¹„.     
* $$D_{known}^{labeled}= {(x_i, y_i)|y_i âˆˆ C_k}$$    
   * $$C_{k}$$: a set of expected intents, predefined (known) intents    

**[ë¼ë²¨ë§ ëœ ë°ì´í„°ì™€ í•¨ê¼ ë¼ë²¨ë§ ë˜ì§€ ì•Šì€ ë°ì´í„° ì¤€ë¹„]**     
* ë°°í¬(deployed)ëœ í›„,     
* ì‹œìŠ¤í…œì€ predefined (known) intents $$C_k$$ì™€ unknown intents $$C_{u}$$ ëª¨ë‘ì˜ ê²½ìš°ê°€ ìˆëŠ” ë°œí™”ë¥¼ ì¤€ë¹„í•œë‹¤      
* $$D^{unlabeled}= {x_i | y_iâˆˆ {C_k, C_u}}$$     
   * $$C_{u}$$: unknown intents       
   * $$C_{k}$$: a set of expected intents, predefined (known) intents   


**[ìƒˆë¡œìš´ intent ë°œê²¬(NID; new intent discovery)ì˜ ëª©ì ]**          
* $$D^{unlabeled}$$ì—ì„œ ìƒˆë¡œìš´ intent $$C_{u}$$ë¥¼ ì‹ë³„í•˜ëŠ” ê²ƒ      


**[NID]**    
* OOD(Out-of-Distribution) íƒì§€ì˜ ì§ì ‘ì ì¸ í™•ì¥ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ.      
* ì—¬ê¸°ì„œ OOD ì˜ˆë¥¼ ì‹ë³„í•  ë¿ë§Œ ì•„ë‹ˆë¼ ê¸°ë³¸ í´ëŸ¬ìŠ¤í„°ë¥¼ ë°œê²¬í•´ì•¼ í•œë‹¤.     
* NIDëŠ” ë˜í•œ í›ˆë ¨ ì¤‘ì— **ëª¨ë“  class informationì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ë¥¼ ê°€ì •í•˜ì§€ ì•ŠëŠ”ë‹¤**ëŠ” ì ì—ì„œ **ì œë¡œìƒ· í•™ìŠµê³¼ ë‹¤ë¥´ë‹¤**.     


ì´ ì—°êµ¬ì—ì„œ ìš°ë¦¬ëŠ”  Zhang et al(2021c)ì— ì´ì–´ $$D_{known}^{labeled}$$ì˜ ì¡´ì¬ë¡œ êµ¬ë³„ë˜ëŠ” unsupervised NIDì™€  semi-supervised NIDë¥¼ ëª¨ë‘ ê³ ë ¤í•œë‹¤.      

----

## Overview of Our Approach  
ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë³¸ ë…¼ë¬¸ì€ Sec. 1ì— ì–¸ê¸‰ëœ ì—°êµ¬ ì§ˆë¬¸ì„ ë‹¤ë£¨ëŠ” **2ë‹¨ê³„ í”„ë ˆì„ì›Œí¬**ë¥¼ ì œì•ˆí•œë‹¤.     

**[STAGE 1: MTP]**     
* **multi-task pre-training (MTP) ìˆ˜í–‰:** ì™¸ë¶€ labeled dataì— ëŒ€í•œ crossentropy lossê³¼ ë ˆì´ë¸”ë§ë˜ì§€ ì•Šì€ target ë°ì´í„°ì— ëŒ€í•œ self supervised lossì„ ê³µë™ìœ¼ë¡œ ìµœì í™”(3.1ì ˆ).      

**[STAGE 2: MTP]**   
* **contrastive learning with nearest neighbors (CLNN) ìˆ˜í–‰:** ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œëŠ” ì„ë² ë”© ê³µê°„ì—ì„œ ê° í›ˆë ¨ ì¸ìŠ¤í„´ìŠ¤ì˜ ìƒìœ„ Kê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒì„ ë¨¼ì € ë°œêµ´í•œ ë‹¤ìŒ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒê³¼ì˜ ëŒ€ì¡° í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤(3.2ì¥).    
* training í›„, ìš°ë¦¬ëŠ” clusteringì˜ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ê°„ë‹¨í•œ **parametric clustering algorithm**ì„ ì‚¬ìš©í•œë‹¤.      

 ![image](https://user-images.githubusercontent.com/76824611/187759519-4f532a4b-8bed-43cc-996a-7eadfb0b6e85.png)


-----

## 3.1 Stage 1: Multi-task Pre-training (MTP)
**[ì•„ë˜ì˜ 1)ê³¼ 2)ë¥¼ ê²°í•©í•œ multi-task pre-training ëª©í‘œë¥¼ ì œì•ˆ]**     
* 1) ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ intent detection ë°ì´í„° ì„¸íŠ¸ì˜ ì™¸ë¶€ ë°ì´í„°ì— ëŒ€í•œ ë¶„ë¥˜ ì‘ì—…     
* 2) í˜„ì¬ ë„ë©”ì¸ì˜ ë‚´ë¶€ ë°ì´í„°ì— ëŒ€í•œ self-supervised learning ì‘ì—…     


**[ë³¸ ë…¼ë¬¸ì˜ ì°¨ë³„ì ]**     
ì„ í–‰ ì—°êµ¬(Lin et al., 2020; Zhang et al., 2021c)ì™€ ë‹¬ë¦¬, ë³¸ ë…¼ë¬¸ì˜ pre-training ë°©ë²•ì€ í˜„ì¬ ë„ë©”ì¸ì˜ **ì£¼ì„ì´ ë‹¬ë¦° ë°ì´í„°($$D_{known}^{labeled}$$)ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ”ë‹¤**.   
â¡ unsupervised ì•Šì€ í™˜ê²½ì—ì„œ ì ìš©ë  ìˆ˜ ìˆë‹¤.     


**[êµ¬ì œì ì¸ ê³¼ì •]**     
* **1)** ë¨¼ì € [pre-trained BERT](https://yerimoh.github.io/Lan2/#1-pre-training-bert) ì¸ì½”ë”ë¡œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•œë‹¤ (Devlin et al., 2019).     
* **2)** ê·¸ëŸ° ë‹¤ìŒ joint pre-training lossì„ ì‚¬ìš©í•œë‹¤ (Zhang et al. (2021d)).   



<details>
<summary>ğŸ“œ joint pre-training loss ë³´ê¸°</summary>
<div markdown="1">

**[joint pre-training loss ê°’ì˜ êµ¬ì„±]**    
![image](https://user-images.githubusercontent.com/76824611/188257427-282551fa-58f4-48c0-926f-1e35a2604052.png)
   * Î¸: ëª¨ë¸ parameters    
   
**$$[L_{ce}$$]**   
* ì™¸ë¶€ external labeled dataì˜ [crossentropy loss](https://yerimoh.github.io/DL3/#%EA%B5%90%EC%B0%A8-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC-%EC%98%A4%EC%B0%A8cross-entropy-error-cee)    
* **$$D_{known}^{labeled}$$**: supervised classificationì„ ìœ„í•´ ì™¸ë¶€ ë ˆì´ë¸”ë¡œ í‘œì‹œëœ ë‹¤ì–‘í•œ ë„ë©”ì¸(ì˜ˆ: CLINC150(Larson et al., 2019), ì¦‰ $$D_{known}^{labeled}$$)ì„ ê°€ì§„ ì™¸ë¶€ ê³µê³µ intent datasetë¥¼ í™œìš©í•œë‹¤.    
* **ëª©í‘œ:** ì§ê´€ì ìœ¼ë¡œ ì™¸ë¶€ intent datasetsì—ì„œ ì£¼ì„ì´ ë‹¬ë¦° ë°œí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ intent ì¸ì‹ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì§€ì‹(general knowledge)ì„ í•™ìŠµí•˜ëŠ” ê²ƒ    

**$$[L_{mlm}]$$**     
* í˜„ì¬ ë„ë©”ì¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë°ì´í„°ì˜ masked language modelling(MLM) ì†ì‹¤ë¡œ êµ¬ì„±ëœë‹¤.    
* **$$D_{internal}^{all}$$**: $$D_{internal}^{all}$$ë¡œ í‘œì‹œëœ í˜„ì¬ ë„ë©”ì¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë°ì´í„°(labeled or unlabeled)ë¥¼ ì‚¬ìš©í•œë‹¤.     
* **ëª©í‘œ:** í˜„ì¬ ë„ë©”ì¸ì—ì„œ ìˆ˜ì§‘ëœ ë°œí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„ë©”ì¸ë³„ ì˜ë¯¸ë¡ ì„ í•™ìŠµ   

**[ê³µí†µ]**        
* ë‘ ì‘ì—…ì€ ëª¨ë‘ **ì˜ë¯¸ë¡ ì  ë°œí™” í‘œí˜„**ì„ í•™ìŠµí•˜ì—¬ í›„ì† í´ëŸ¬ìŠ¤í„°ë§ ì‘ì—…ì— ëŒ€í•œ ì ì ˆí•œ ë‹¨ì„œë¥¼ ì œê³µ      
* ë‘ ì‘ì—… ëª¨ë‘ NIDì— í•„ìˆ˜ì ì´ë‹¤.      


**[semi-supervised NID]**    
* semi-supervised NIDì˜ ê²½ìš°, ìœ„ì˜ ì‹ì—ì„œ $$D_{labeled}^{external}$$ë¥¼ $$D_{known}^{labeled}$$ë¡œ ëŒ€ì²´í•˜ì—¬ í˜„ì¬ ë„ë©”ì¸ì˜ ì£¼ì„ì´ ë‹¬ë¦° ë°ì´í„°ë¥¼ ì¶”ê°€ë¡œ í™œìš©í•˜ì—¬ ì§€ì†ì ì¸ ì‚¬ì „ í›ˆë ¨ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.    
* ì´ ë‹¨ê³„ëŠ” unsupervised NIDì— í¬í•¨ë˜ì§€ ì•ŠëŠ”ë‹¤

   
 ![image](https://user-images.githubusercontent.com/76824611/187759519-4f532a4b-8bed-43cc-996a-7eadfb0b6e85.png)

</div>
</details>  



-----

##  Stage 2: Contrastive Learning with Nearest Neighbors (CLNN)

 
**[contrastive learning ì œì•ˆ]**    
clusteringì— ëŒ€í•œ **ê°„ê²°í•œ í‘œí˜„**ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ **ì¸ì ‘ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ëª¨ìœ¼ê³ ** ì„ë² ë”© ê³µê°„ì—ì„œ **ë¨¼ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°€ì–´ëƒ„**      

**[a simple example of CLNN]**
![image](https://user-images.githubusercontent.com/76824611/188260026-8c943b50-82c6-4e4e-9c50-e82e6a95f4b9.png)


 . A batch of four training instances {xi}
4
i=1 (solid
markers) and their respective neighborhoods {Ni}
4
i=1 are plotted (hollow markers within large circles). Since x2
falls within N1, x2 along with its neighbors are taken as positive instance for x1 (but not vice versa since x1 is not
in N2). We also show an example of adjacency matrix A0
and augmented batch B
0
. The pairwise relationships
with the first instance in the batch are plotted with solid lines indicating positive pairs and dashed lines indicating



**[êµ¬ì œì ì¸ ê³¼ì •]**     
* **1)** ë³¸ ë…¼ë¬¸ì—ì„  ë¨¼ì € 1ë‹¨ê³„ë¶€í„° pre-trained ëª¨ë¸ë¡œ utteranceë¥¼ ì¸ì½”ë”©í•œë‹¤.      
* **2)** ê·¸ëŸ° ë‹¤ìŒ  ì„ë² ë”© ê³µê°„ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ìƒìœ„ Kê°œì˜ ì´ì›ƒì„ ê²€ìƒ‰í•˜ì—¬ ì´ì›ƒ $$N_i$$ì„ í˜•ì„±í•œë‹¤.     
    * ê° utterance $$x_i$$ì— ëŒ€í•´ inner productë¥¼  distance ë©”íŠ¸ë¦­ìœ¼ë¡œ ì‚¬ìš©í•¨   
    * $$N_i$$ì—ì„œì˜ ë°œí™”ëŠ” xiì™€ ìœ ì‚¬í•œ ì˜ë„ë¥¼ ê³µìœ í•˜ë„ë¡ ë˜ì–´ ìˆë‹¤.      
    * í›ˆë ¨ ì¤‘ì—, ìš°ë¦¬ëŠ” ë°œí™”ì˜ ë¯¸ë‹ˆ ë°°ì¹˜ B = {xi}Mi=1ì„ ìƒ˜í”Œë§í•œë‹¤. ê° ë°œì„± xi b Bì— ëŒ€í•´ ì´ì›ƒ Niì—ì„œ í•˜ë‚˜ì˜ ì´ì›ƒ x 0 ië¥¼ ê· ì¼í•˜ê²Œ ìƒ˜í”Œë§í•œ ë‹¤ìŒ ë°ì´í„° í™•ëŒ€ë¥¼ ì‚¬ìš©í•˜ì—¬ xiì™€ x 0 iì— ëŒ€í•´ ê°ê° x i iì™€ x 0 0 ië¥¼ ìƒì„±í•œë‹¤. ì—¬ê¸°ì„œ, ìš°ë¦¬ëŠ” xiiì™€ x 00ië¥¼ ì–‘ì˜ ìŒì„ í˜•ì„±í•˜ëŠ” xiì˜ ë‘ ê°œì˜ ë·°ë¡œ ì·¨ê¸‰í•œë‹¤. ê·¸ëŸ° ë‹¤ìŒ ìƒì„±ëœ ëª¨ë“  ìƒ˜í”Œê³¼ í•¨ê»˜ ì¦ê°• ë°°ì¹˜ B 0 = {xpoti, xpot 0 i }M i = 1ì„ ì–»ëŠ”ë‹¤. ëŒ€ì¡° ì†ì‹¤ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” B0ì— ëŒ€í•œ ì¸ì ‘ í–‰ë ¬ A0ì„ êµ¬ì„±í•˜ëŠ”ë°, B0ì€ 2M Ã— 2M ì´ì§„ í–‰ë ¬ì´ë‹¤. 1ì€ ì–‘ì˜ ê´€ê³„(ì´ì›ƒì´ê±°ë‚˜ ì¤€ê°ë… NIDì—ì„œ ë™ì¼í•œ ì˜ë„ ë ˆì´ë¸”ì„ ê°€ì§€ê³  ìˆìŒ)ë¥¼ ë‚˜íƒ€ë‚´ê³  0ì€ ìŒì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ëŒ€ì¡°ì ì¸ ì†ì‹¤ì„ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤.


In the second stage, we propose a contrastive learning objective that pulls together neighboring instances and pushes away distant ones in the embedding space to learn compact representations
for clustering. Concretely, we first encode the utterances with the pre-trained model from stage 1.
Then, for each utterance xi
, we search for its topK nearest neighbors in the embedding space using
inner product as distance metric to form a neighborhood Ni
. The utterances in Ni are supposed to
share a similar intent as xi
. During training, we
sample a minibatch of utterances B = {xi}M
i=1. For
each utterance xi âˆˆ B, we uniformly sample one
neighbor x
0
i
from its neighborhood Ni
. We then
use data augmentation to generate xËœi and xËœ
0
i
for xi
and x
0
i
respectively. Here, we treat xËœi and xËœ
0
i
as
two views of xi
, which form a positive pair. We
then obtain an augmented batch B
0 = {xËœi
, xËœ
0
i
}M
i=1
with all the generated samples. To compute contrastive loss, we construct an adjacency matrix A0
for B
0
, which is a 2M Ã— 2M binary matrix where
1 indicates positive relation (either being neighbors
or having the same intent label in semi-supervised
NID) and 0 indicates negative relation. Hence, we
can write the contrastive loss as:







