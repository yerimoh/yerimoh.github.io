---
title: "Rethinking Style Transformer with Energy-based Interpretation: Adversarial Unsupervised Style Transfer using a Pretrained Modelì •ë¦¬"
date:   2023-02-17
excerpt: "Rethinking Style Transformer with Energy-based Interpretation: Adversarial Unsupervised Style Transfer using a Pretrained Model paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# Abstract
<span style="background-color:#F5F5F5">**[í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œ]**</span>        
Style control, content preservation ë° fluencyëŠ” text style transfer modelsì˜ í’ˆì§ˆì„ ê²°ì •í•œë‹¤.      
nonparallel ë§ë­‰ì¹˜ì— ëŒ€í•´ trainí•˜ê¸° ìœ„í•´, ëª‡ ê°€ì§€ ê¸°ì¡´ ì ‘ê·¼ë²•ì€ **adversarial lossë¡œ  style discriminatorë¥¼ ì†ì´ëŠ” ê²ƒì„ ëª©í‘œ**ë¡œ í•œë‹¤.     
ê·¸ëŸ¬ë‚˜ <span style="background-color:#FFE6E6">adversarial trainingì€ ë‹¤ë¥¸ ë‘  metricsì— ë¹„í•´ **fluencyë¥¼ í¬ê²Œ ì €í•˜**</span>ì‹œí‚¨ë‹¤.     


<details>
<summary>ğŸ“œ Adversarial lossë€? </summary>
<div markdown="1">
  
Adversarial lossëŠ” Generatorë¡œ í•˜ì—¬ê¸ˆ ì§„ì§œì²˜ëŸ¼ ë³´ì¼ ì •ë„ë¡œ ì‚¬ì‹¤ì ì¸ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë„ë¡ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜

</div>
</details>


<details>
<summary>ğŸ“œ adversarial trainingë€? </summary>
<div markdown="1">
  
ì ëŒ€ì  í›ˆë ¨ Adversarial trainingì€ regularizationì˜ í•œ ë°©ë²•ì´ë‹¤.

ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ ë¶„ì•¼ì—ì„œ regularizationì€ overfittingì„ ë§‰ê³  ëª¨ë¸ì„ robustí•˜ê²Œ ë§Œë“¤ì–´ì£¼ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤.

ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ Input dataë¥¼ í•™ìŠµí•˜ì—¬ ì ì ˆí•œ labelì„ ë°˜í™˜í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í›ˆë ¨ì„ ì§„í–‰í•œë‹¤.

Adversarial attackì€ ë°˜ëŒ€ë¡œ ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ì— ëŒ€í•˜ì—¬ ì…ë ¥ ë°ì´í„°ë¥¼ ì˜ëª» ì˜ˆì¸¡í•˜ë„ë¡ Inputì„ ì¡°ì‘í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤ .

ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ inputìœ¼ë¡œ ë°›ëŠ” CNNì˜ ê²½ìš° ì •ë‹µ Yì—ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì˜ˆì¸¡ê°’Y'ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì¡°ê¸ˆì”© ì´ë¯¸ì§€ì˜ í”½ì…€ì„ ìˆ˜ì •í•œë‹¤.
  
</div>
</details>


<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ ëª©ì ]**</span>        
* ë³¸ ì—°êµ¬ì—ì„œëŠ” <span style="background-color:#fff5b1">energy-based interpretationì„ ì‚¬ìš©í•˜ì—¬ **ì´ í˜„ìƒì„ ì„¤ëª…**</span>í•˜ê³ ,        
<span style="background-color:#fff5b1">pretrained language modelì„ í™œìš©í•˜ì—¬ **fluencyì„ í–¥ìƒ**</span>ì‹œí‚¨ë‹¤.      
* êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” <span style="background-color:#fff5b1">**discriminatorì™€ ëª¨ë¸ ìì²´ë¥¼ ì¬êµ¬ì„±**</span>í•˜ê³ ,    
<span style="background-color:#fff5b1">**pretrained language modelì„ text style transfer frameworkì— ì ìš©**</span>í•˜ì—¬ **generatorì™€  discriminatorê°€ pretrained modelì˜ í˜ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆ**í•œë‹¤.         


ë³¸ ë…¼ë¬¸ì€ GYAFC, Amazon ë° Yelpì˜ ì„¸ ê°€ì§€ ê³µê°œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•˜ê³  **ì „ì²´ ë©”íŠ¸ë¦­ì—ì„œ SOTA ë‹¬ì„±**í–ˆë‹¤.


---
---

# 1 Introduction

**Text style transfer**ì€ **ìŠ¤íƒ€ì¼ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ì˜ë¯¸ë¡ ì„ ìœ ì§€**í•˜ë©´ì„œ, **í•œ ìŠ¤íƒ€ì¼ì—ì„œ ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ë¡œ ë¬¸ì¥ì„ ë³€í™˜í•˜ëŠ” ì‘ì—…**ì´ë‹¤.          

Text style transfer taskì„ í•´ê²°í•  ë•ŒëŠ” **ì„¸ ê°€ì§€ ê¸°ì¤€**ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.            
1) ***style control***, styleì´ ì›ë˜ ë¬¸ì¥ì—ì„œ ìƒì„±ëœ ë¬¸ì¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì˜ transferë˜ëŠ”ì§€,        
2) ***content preservation***,  ìƒì„±ëœ ë¬¸ì¥ì´ ì›ë³¸ì˜ ì˜ë¯¸ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ìœ ì§€í–ˆëŠ”ì§€,       
3) ***fluency***,ìƒì„±ëœ ë¬¸ì¥ì´ ì–¼ë§ˆë‚˜ ìì—°ìŠ¤ëŸ¬ìš´ì§€.      


**ë¬¸ì¥ì˜ styleì„ fluentlyí•˜ê²Œ ë³€í™˜**í•˜ëŠ” ê²ƒì€ ì¢…ì¢… **content preservationê³¼ ì¶©ëŒ**í•˜ê¸° ë•Œë¬¸ì— Text style transferì€ ì–´ë µë‹¤.          

ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´,   
ì—¬ëŸ¬ supervised text style transfer methodì´ ì‹œë„ë˜ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ style-labeledì´ ìˆëŠ” ë¬¸ì¥ ìŒì„ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì— ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì´ ì‹¤ìš©ì ì´ì§€ ì•Šë‹¤.   

ë‹¤ì–‘í•œ ë¹„ì§€ë„ í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼

Various unsupervised text style
transfer approaches have become popular, including those using an autoencoder (Hu et al., 2017;
Huang et al., 2020), back-translation (Prabhumoye
et al., 2018; Lample et al., 2019), and reinforcement learning (Xu et al., 2018; Luo et al., 2019).
Among previous studies, Style Transformer (Dai
et al., 2019) achieved fine-grained style control by
deceiving the style discriminator through adversarial training. Aside from their strengths, however,
adversarial models including Style Transformer degrades the fluency of generated sentences.


In this paper, we review Style Transformer to
investigate the reason behind the fluency degradation in adversarial models. To more precisely
interpret what fluency is, we introduce the notion of energy (Hinton, 2002; Lecun et al., 2006),
which is the entropy of variables. The energy function, which measures the energy of input variables
with respect to a particular style, outputs low energy if the inputs are common in that style and
outputs high energy otherwise. For example, formal/informal sentences would likely have low energy in the formality corpus, while sentences that
have nothing to do with formality (e.g., political
expressions) would have high energy in the corpus. Accordingly, we define fluency as having low
energy in a particular corpus, in which the fluent
sentences express one of the styles in the corpus.
As illustrated in Figure 1, fluency degrades while
deceiving the discriminator, since adversarial learning maximizes the energy to the source style and
drives the generated sentence far away from the distribution of the corpus. To counter fluency degradation, we introduce a regularizer using a language
model (LM) to keep the generated sentences in the
distribution of the corpus. This LM-based regularizer keeps the generated sentences in the corpus by
pulling the sentence to the target corpus.


To apply the LM-based regularizers, we can
leverage pretrained models such as GPT-2 to generate fluent sentences. Moreover, fluency is expected to further improve when the generator and
the discriminator are also replaced with a pretrained
model. However, as shown in Figure 2, the generator, discriminator, and LM must share the same
vocabulary and tokenizer in order to propagate gradients successfully. Thus, inefficiency can arise,
in that two of the three modules may need to be
re-trained from scratch because the existing pretrained models are based on different tokenizers.
To address this issue, we restructured the discriminator and LM such that a single pretrained model is
applied to all three modules: the generator, discriminator, and LM. By using a single pretrained model,
our method not only solves the inconsistent vocabulary problem but also has advantages when applying further pretraining for domain adaptation (Gururangan et al., 2020) or additional dataset (Lai
et al., 2021a). This is because additional pretraining is necessary when only a single model is used
to improve style transfer performance.
Our contributions can be summarized as follows:

â€¢ We analyze the fluency degradation in adversarial training with an energy-based interpretation, and propose a regularizer leveraging a
language model to prevent fluency degradation.


â€¢ We reconstruct the discriminator and language
model such that the single pretrained language
model can be employed in the text style transfer framework.


â€¢ We achieve new state-of-the-art results on
GYAFC, Amazon, and Yelp datasets and carefully analyze the contribution of each component of our model.


---
---

# 2 Related Work

## 2.1 Unsupervised style transfer
Many of the previous studies have attempted to
learn disentangled representations of text by separating representations of content and style in a latent space. For instance, Shen et al. (2017) trained
a cross-aligned autoencoder to learn a shared latent space for contents, while learning a separate
representation for styles using adversarial learning. Yang et al. (2018) further extended this crossaligned approach by leveraging LM as a discriminator to enhance the informativeness and stability
of adversarial training. Yi et al. (2020) leveraged
multiple instances of the same style to model the
latent space of underlying stylistic characteristics,
and samples extracted from this space were fed into
the decoder to balance with contents. These works
using disentangled representations exhibited reasonable performance with high interpretability, but
disentangled content representations can still contain style-relevant information, as pointed out by
Lample et al. (2019). In addition, there is a limitation in that the meaning of the input sentence must
be expressed in a fixed-size vector with a limited
capacity (Dai et al., 2019).


In contrast, there are methods without disentangled representations that do not explicitly disentan
gle the content and style of text using reinforcement learning (Xu et al., 2018; Luo et al., 2019)
and back-translation (Lample et al., 2019; Prabhumoye et al., 2018). Dai et al. (2019) proposed a
novel style transfer model based on the transformer
architecture without disentangled representations,
and Wang et al. (2019) also utilized a transformer
for an unsupervised framework by editing entangled latent representations. These models are novel
in their model architecture or training strategy, but
they do not utilize pretraining models, which results in lower performance than the state-of-the-art
methods. On the other hand, our work proposes a
novel approach to effectively leverage a pretrained
LM in an unsupervised text style transfer task.



## 2.2 Style transfer with pretrained models
Recently, pretrained models have achieved great
success on various NLP tasks such as machine
translation (Chronopoulou et al., 2020; CONNEAU
and Lample, 2019) and text summarization (Liu
and Lapata, 2019). The pretrained models are also
being used for text style transfer tasks. Sudhakar
et al. (2019) used two variants of â€˜decoder-onlyâ€™
transformer to generate sentences in a target style
and leveraged the power of GPT (Radford et al.,
2018). Malmi et al. (2020) used a padded masked
language model (Mallinson et al., 2020) variant,
whose architecture and the trained corpus were
identical to those of BERT (Devlin et al., 2019).
Although these studies exploited the power of pretrained models, our approach differs in that we train
our model adversarially in an end-to-end manner.


In addition, several works have focused on style
transfer in a specific domain, or for leveraging an
additional corpus. To transfer writing styles between authors, Syed et al. (2020) pretrained LM on
the author corpus from scratch using masked language modeling. Laugier et al. (2021) detoxified
toxic texts by fine tuning a pretrained T5 (Raffel et al., 2020) using additional denoising and
cycle-consistency objectives. However, these studies only focused on a specific domain. Lai et al.
(2021a) built a pseudo-parallel dataset by leveraging generic resources including WordNet (Baccianella et al., 2010) and Parabank (Hu et al., 2019)
to fine tune BART on style transfer tasks. Lai et al.
(2021b) fine tuned BART (Lewis et al., 2020) using
parallel data with a policy gradient (Sutton et al.,
1999) which maximized the style classifier reward
and the BLEU score reward. Our work incorporates
pretrained models with adversarial training on various domains, while trained only on a non-parallel
corpus.


## Energy-based model
The conventional probabilistic model outputs the
normalized probability p(x) for input variable x.
In contrast, the energy-based model outputs the
non-normalized scalar value E(x) denoted as energy (Hinton, 2002; Lecun et al., 2006) Using the
energy-based model, we can classify x by comparing the energy of each label, or generate x by
optimizing arg minx E(x).


Several works have leveraged the energy-based
model for image generation (Ngiam et al., 2011;
Zhao et al., 2017), text generation (Deng et al.,
2020; Bakhtin et al., 2021), and reinforcement
learning (Haarnoja et al., 2017). We borrow the
main idea of the energy-based model, which expresses the classifier in the form of an energy function. In the work of Che et al. (2020), the authors
interpreted the GAN discriminator (Goodfellow
et al., 2014) using the energy-based model, but
we apply this interpretation to the style transfer
task. We show that Style Transformer can be interpreted as an energy-based model by decomposing
the discriminator, and provide the reason why fluency degradation occurs when we try to deceive
the style discriminator.






