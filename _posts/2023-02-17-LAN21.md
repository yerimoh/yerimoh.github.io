---
title: "MatSciBERT: A materials domain language model for text mining and information extraction 정리"
date:   2023-02-15
excerpt: "Specializing Multi-domain NMT via Penalizing Low Mutual Information paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



materials science에 대한 knowledge가 많이 생성되며, 텍스트로 저장되어왔다.    

<span style="background-color:#F5F5F5">**[문제]**</span>         
Bidirectional Encoder Representations from Transformers (BERT)과 같은 NLP 모델은 유용한 정보 추출 도구이지만, 이러한 모델은 <span style="background-color:#fff5b1">**materials science 특정 표기법(materials science specific notations)**과 **전문 용어(jargons)**에 대해 train되지 않았기 때문에</span> materials science에 적용할 때 좋은 결과를 얻지 못한다.         

<span style="background-color:#F5F5F5">**[해결: MatSciBERT]**</span>     
* <span style="background-color:#FFE6E6">**peer-reviewed materials science publications의 large corpus**에 대해 train</span>된 materials-aware language model이다.        
* 이 모델은 MatSciBERT가  science corpus에 대해 훈련된 언어 모델인 **[SciBERT](https://yerimoh.github.io/LAN22/)보다 성능이 좋다.**    
* downstream tasks, named entity recognition, relation classification, and abstract classification에서 SOTA성능이 난다.   


본 논문은 MatSciBERT의 pre-trained weights를 공개적으로 액세스할 수 있도록 한다.      

----


# INTRODUCTION
<span style="background-color:#fff5b1">**materials를 발견하고 실용적인 응용에 활용**하는 것은 수십 년 단위로 걸려 **시간이 매우 많이든다**.</span>       
**이 과정을 가속화**하기 위해서는 엄격한 과학적 절차를 통해 
<span style="background-color:#fff5b1"**수세기에 걸쳐 개발된 재료에 대한 지식을 응집력 있는 방식으로 활용하고 활용**</span>해야 한다.       

Textbooks, scientific publications, reports, handbooks, websites 등은 이미 존재하는 정보를 얻기 위해 채굴할 수 있는 큰 데이터 저장소의 역할을 한다.     
그러나 대부분의 과학 데이터는 the form of text, paragraphs with cross reference, image captions, and tables의 형태로 **반구조화되거나 구조화되지 않았기 때문**에 이러한 텍스트에서 **유용한 정보를 추출하는 것은 어려운 작업**이다.       

⚠ 이러한 정보를 수동으로 추출하는 것은, **시간과 자원이 매우 많이 소요**되며 **domain 전문가의 해석에 의존**한다.   





<span style="background-color:#F5F5F5">**[NLP 모델 동향]**</span>    
인공지능의 하위 도메인인 자연어 처리(NLP)는 텍스트에서 **정보 추출을 자동화**할 수 있는 대체 접근법을 제시한다. * **word2vec 및 GloVe:** Neural pretrained embeddings으로, 꽤 인기가 있지만 **domain-specific knowledge이 부족**하고 **상황별 임베딩을 생성하지 않는다**.        
* 최근 NLP의 발전은 **domain-specific tasks에 대해 pre-trained language model (LM)이 finetuned되는 계산 패러다임의 개발**로 이어졌다.        
* 연구에 따르면 이러한 pretrain-finetune paradigm은 19-23년 사이에 SOTA였다.    
* 최근에, 많은 양의 텍스트와 높은 컴퓨팅 능력의 가용성 때문에, 연구자들은 이러한 **큰 신경 언어 모델을 pretrain시킬 수 있었다**.          
예를 들어, Bidirectional Encoder Representations from Transformers (BERT)은 BookCorpus 및 English Wikipedia에서 훈련되어  question answering과 entity recognition과 같은 여러 NLP 작업에서 SOTA를 발휘한다.         



<span style="background-color:#F5F5F5">**[materials science domain에서의 NLP applications]**</span>       
연구원들은 **materials science domain**에서 ML 응용 프로그램을 위한 **데이터베이스 생성을 자동화**하기 위해 **NLP tools를 사용**했다. 아래는 활용의 예시이다.          
아래 예시들은 모두 NLP 파이프라인인 [ChemDataExtractor](https://pubs.acs.org/doi/10.1021/acs.jcim.6b00207)를 사용했다.    
* [create databases of battery materials](https://www.nature.com/articles/s41597-020-00602-2)       
* [Curie and Néel temperatures of magnetic materials](https://www.nature.com/articles/sdata2018111)              
* [inorganic material synthesis routes](https://www.nature.com/articles/s41597-019-0224-1)       
* [composition and dissolution rate of calcium aluminosilicate glassy materials](https://ceramics.onlinelibrary.wiley.com/doi/10.1111/jace.17631)의 조성     
* [용해율과 zeolite 합성경로를 수집하여 zeolite](https://pubs.acs.org/doi/10.1021/acscentsci.9b00193)를 함유하는 게르마늄을 합성     
* [process and testing parameters of oxide glasses 및 testing parameters를 추출하여 Vickers
hardness의 향상된 예측을 가능하게 함](https://web.iitd.ac.in/~krishnan/publications/Krishnan/102_Extracting%20processing%20and%20test.html)              
* [computational materials science research papers에서 추출한 정보를 사용하여 데이터베이스를 만드는 자동화된 NLP](https://www.researchgate.net/publication/349533689_MatScIE_An_automated_tool_for_the_generation_of_databases_of_methods_and_parameters_used_in_the_computational_materials_science_literature) 도구를 만듦              
* [topic modeling in glasses](http://www.gstatic.com/generate_204): unsupervised fashion으로 문학을 다른 주제로 그룹화함, specific queries(elements present, synthesis, or characterization techniques, and applications)를 기반으로 이미지를 찾는 데 사용되었다.           




[Olivetti et al.(2019)](https://aip.scitation.org/doi/abs/10.1063/5.0021106)의 포괄적 검토는 NLP가 materials science에 이익을 줄 수 있는 몇 가지 방법을 설명한다.    
* [OSCAR4](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-3-41): insights into chemical parsing tools제공, 화학적 구문 분석 도구에 대한 통찰력을 제공        
* [Artificial Chemis](https://onlinelibrary.wiley.com/doi/10.1002/adma.202001626): 전구체 정보의 입력을 받아 목표 대역 간격을 가진 광전자 반도체를 제조하기 위한 합성 경로를 생성          
* [robotic system](https://www.science.org/doi/10.1126/sciadv.aaz8867): 보다 깨끗하고 지속 가능한 에너지 솔루션을 생산하기 위해 thin films을 만듦     
➡ 37과 8천만 개 이상의 materials science domain-specific named entities를 식별하는 연구는 ML과 NLP 기술의 결합을 통해 **다양한 응용을 위한 재료의 가속화를 촉진**했다.      


<span style="background-color:#F5F5F5">**[Word2vec와 BERT의 확장]**</span>       
연구자들은 Word2vec와 BERT의 도메인 적응 능력을 아래와 같이 확장하였다.    
* [BioWordVec](https://www.nature.com/articles/s41597-019-0055-0), [BioBERT](https://arxiv.org/abs/1901.08746): 생물과학 분야에서 확장            
* [SciBERT](https://yerimoh.github.io/LAN22/): scientific and biomedical corpus으로 train        
* [clinicalBERT](https://arxiv.org/abs/1904.05342): 2 million clinical notes in [MIMIC-III v1.4 database](https://www.nature.com/articles/sdata201635)에서 훈련됨    
* [mBERT](https://aclanthology.org/2020.findings-emnlp.150/): multilingual machine translations tasks    
* [PatentBERT](http://www.digital.ntu.edu.tw/hsiang/pdf/WPI%20Patent%20classification%20by%20fine-tuning%20BERT.PDF): patent classification    
* [FinBERT](https://arxiv.org/abs/1908.10063): financial tasks        



✨ 이는 **materials-aware LM**이 downstream tasks에 추가로 적응함으로써 **해당 분야의 연구를 크게 가속화할 수 있음을 시사**한다.       


이 연구 이전에는 재료 인식 언어 모델 개발에 대한 논문이 없었지만, 최근 [a recent preprint](https://www.researchgate.net/publication/355846376_The_Impact_of_Domain-Specific_Pre-Training_on_Named_Entity_Recognition_Tasks_in_Materials_Science)은  materials science에서 domain-specific language models이 named entity recognition (NER) 작업에 미치는 영향을 강조한다.       


<span style="background-color:#F5F5F5">**[본 논문]**</span>       
이 연구에서, 우리는 materials science domain-specific BERT, 즉 **MatSciBERT**를 훈련시킨다.      
Fig. 1은 materials science 말뭉치 생성, MatSciBERT 훈련 및 다양한 downstream tasks 평가를 포함하는 본 연구에 채택된 방법론의 그래픽 요약을 보여준다.              
우리는 아래 나열된 것처럼 **domain-specific tasks에 대한 SoTA**를 달성한다.      
* **a.**  [NER on SOFC, SOFC Slot dataset by Friedrich et al. (2020)45     
 Matscholar dataset by Weston et al](https://pubmed.ncbi.nlm.nih.gov/31361962/)                    
* **b.** [Glass vs. Non-Glass classification of paper abstracts](https://www.sciencedirect.com/science/article/pii/S2666389921001239)             
* **c.**  [Relation Classification on MSPT corpus](https://aclanthology.org/W19-4007/)            


현재 연구는 materials domain language model의 가용성 격차를 해소하여 연구자가 정보 추출, knowledge graph completion 및 기타  downstream tasks을 자동화하여 **materials 발견을 가속화**할 수 있다.        


<span style="background-color:#F5F5F5">**[open source]**</span>       
* [huggingface/matscibert](https://huggingface.co/m3rg-iitd/matscibert): pre-trained weights을 제공    
[github/MatSciBERT](https://github.com/M3RG-IITD/MatSciBERT): MatSciBERT의 pretraining과 finetuning on downstream tasks 코드 공개            
* [zenodo](https://doi.org/ 10.5281/zenodo.6413296): downstream tasks을 위한finetuned
models이 있는 코드      


**[Fig. 1 Methodology for training MatSciBERT]**
![image](https://user-images.githubusercontent.com/76824611/225840065-ddbf1a52-fd41-4e8f-9f53-55d4f966fb06.png)   
* 우리는 관련 연구 논문을 선택한 후 query search을 통해 Materials Science Corpus (MSC)를 만든다.     
* MSC에서 pre-trained된 MatSciBERT는 다양한 downstream tasks에서 평가된다.       


---
---

# RESULTS AND DISCUSSION
## Dataset
Textual datasets는 LM 훈련의 필수적인 부분이다.     

<span style="background-color:#F5F5F5">**[materials domain에 적합하지 않은 corpora]**</span>       
* BookCorpus, 위키피디아과 영어와 같은 많은 범용 말뭉치     
* biomedical corpus 39, clinical database 41: 같은 domain-specific corpora   


<span style="background-color:#F5F5F5">**[materials domain에 적합한 corpora로 data 만들기]**</span>       
* materials specific LM을 제공하기 위해 **4개의 important materials 제품군에 걸친 말뭉치**를 생성    
   * inorganic glasses    
   * metallic glasses     
   * alloys    
   * cement & concrete     
* 위와 같은 광범위한 범주가 언급되었지만, 2차원 물질을 포함한 **몇 가지 다른 범주의 물질도 말뭉치에 존재했다는 점에 유의**해야 한다.     
* 구체적으로, 우리는 Elsevier Science Direct Database에서 다운로드한 약 1백만 개의 논문 중에서 약 150만 개의 논문을 선택했다.     
* 말뭉치를 만드는 단계는 방법은  Methods section에 나와 있다.     
   * 각 가정의 논문 수와 단어 수에 대한 자세한 내용은 Supplementary Table 1에 나와 있다.     
   * 우리는 또한 이 작업을 위해 GitHub 저장소에서 MatSciBERT를 사전 훈련하는 데 사용되는 논문의 DOI와 PII 목록을 제공했다   
* 이 연구를 위해 개발된 materials science corpus는 ~285M 단어를 가지고 있으며,
 이는 SciBERT(3.17B 단어)와 BERT(3.3B 단어)를 사전 훈련하는 데 사용되는 단어의 거의 9%이다.    
 ➡ **SciBERT pre-train을 계속**하기 때문에 MatSciBERT는 3.17 + 0.28 = 3.45B 단어로 구성된 말뭉치에 대해 **효과적으로 훈련**된다.      


**[Supplementary Table 1]**      
![image](https://user-images.githubusercontent.com/76824611/225872626-6ca1a9b1-fd9a-49fb-b015-8e0ca1d22b83.png)
* 단어의 40%는 norganic glasses 및 ceramics과 관련된 연구 논문에서 나왔다.        
* 단어의 각각 20%는 bulk metallic glasses(BMG), alloys에서 나온 것을 알 수 있다.      
* 'cement and concrete'의 연구 논문 수는 'inorganic glasses and ceramics'보다 많지만,   
 'inorganic glasses and ceramics'가 단어 수가 더 많다.     
➡ 이는 'inorganic glasses and ceramics'의 범주와 관련하여 검색된 많은 수의 full-text documents가 존재하기 때문이다.    


**[Supplementary Table 2]**     
![image](https://user-images.githubusercontent.com/76824611/225872675-f316ca7f-b015-4ec4-95f1-20f6b527545a.png)
* materials science 분야와 관련된 중요한 문자열의 단어 수를 나타낸다.    
* 말뭉치는 thermoelectric, nanomaterials, polymers, and biomaterials의 중요한 분야를 포괄한다는 점에 유의해야 한다.     
* 또한 언어 모델을 훈련하는 데 사용되는 말뭉치는 실험(experimental) 및 계산(computational) task로 구성되어 있으며, 이 두 가지 접근 방식 **모두 물질 반응(material response)을 이해**하는 데 중요한 역할을 한다.    
* 이 말뭉치의 평균 논문 길이는 ~1848 단어이며,    
이는 **SciBERT** 말뭉치의 평균 논문 길이 2769 단어의 **3분의 2이**다.     
* 평균 논문 길이가 적은 2가지 이유   
    * **(a)** 일반적으로 **materials science 논문**은 biomedical 논문보다 **짧다**.         
    * **(b)** 우리 말뭉치에도 full text가 없는 논문이 있다. 그러한 경우, 우리는 최종 말뭉치에 도달하기 위해 그러한 **논문의 요약을 사용**했다.            





---



## Pre-training of MatSciBERT
MatSciBERT pre-training의 경우,      
우리는  Gururangan et al. (2020) (20)이 제안한 e domain adaptive pretraining을 따른다.          
이 연구에서 저자들은 domain-specific text (20)의 말뭉치에 대한 initial LM의 pre-training을 계속했다.     

그들은 initial LM 어휘와 domain별 어휘 사이의 중복이 54.1% 미만임에도 불구하고,        
4개 domain 모두에 대한 **domain별 downstream tasks의 성능이 크게 향상**되는 것을 관찰했다.           

BioBERT19와 FinBERT22: vanilla BERT 모델에 domain-specific text를 추가로 pretrained한 비슷한 접근 방식을 사용하여 개발됨(토큰화는 원래 BERT 어휘를 사용)      


MatSciBERT weights: 일부 적합한 LM의 가중치로 초기화한 다음, MSC에서 pre-train한다.      
MatSciBERT에 대한 적절한 초기 가중치를 결정하기 위해, 우리는  tokenizers library(48)을 사용하여   
MSC를 기반으로 uncased wordpiece(47)를 훈련시켰다.     

MSC 어휘의 중복: 
uncased SciBERT21 어휘의 경우 53.64%, uncased는 BERT 어휘의 경우 38.90%이다.    
SciBERT의 어휘와 중복이커서, 우리는 **SciBERT 어휘를 사용하여 말뭉치를 토큰화**하고,   
 Beltagy et al. (2019)(21)에 의해 공개된 **SciBERT의 가중치로 MatSciBERT 가중치를 초기화**한다.     
 
materials science별 어휘는 말뭉치를 **더 적은 수의 단어로 표현**하고 **잠재적으로 더 나은 언어 모델로 이어질 수 있다는 것**을 언급할 가치가 있다.      
예를 들어,    
“yttria-stabilized zirconia”    
SciBERT vocabulary로 토큰화: [ “yt”, “##tri”, “##a”, “-”, “stabilized”, “zircon”, “##ia”]     
domain-specific 토큰화: ["yttria", "-", "stabilized", "zircon"]       
⚠️ 그러나, domain-specific 토큰화를 사용하면 SciBERT 가중치를 사용할 수 없으며,    
SciBERT가 이미 학습한 과학 지식을 활용한다.      
⚠️ 또한  deep neural language models 은 기존 토큰화를 사용하여 **새로운 단어를 나타내는 반복 패턴을 학습할 수 있기 때문에**, **재료 영역에 SciBERT 어휘를 사용하는 것이 반드시 해로운 것은 아니다**.     

예를 들어, 단어 조각 "yt", "##tri", 그리고 "##a"가 연속적으로 발생할 때,   
SciBERT는 downstream tasks에서 입증된 것처럼 어떤 물질이 논의되고 있음을 인식한다.  
이러면 domain-specific이 "yttria"라고 토큰화 한것과 비슷한 결과를 얻을 수 있다는 것이다.     
➡ 이게 왜 BERT-based LMs(FinBERT22, BioBERT19, and ClinicalBERT40)이 **domain-specific tokenizers를 안쓰고 기존 pre-training을 확장**하는 이유이다.     


pre-training에 대한 자세한 내용은 Methods section에 나와 있다.    
pre-training은 360시간 동안 수행되었고, final perplexity of 2.998을 달성했다(Supplementary Fig1. 1a).      
서로 다른 어vocabulary와 validation corpus로 인해 직접 비교할 수는 없지만,     
BERT25와 RoBERTa49 저자는 각각 동일한 범위에 있는 3.99의 perplexities를 달성했다.     
우리는 또한 Supplementary Fig. 1b, c의 MLM loss 및 MLM accuracy와 같은 evaluation metrics에 대한 그래프를 제공한다.     
그런 다음 최종 pre-trained LM을 사용하여 다양한 materials science domain-specific downstream tasks을 평가했으며, 이에 대한 자세한 내용은 후속 섹션에 설명되어 있다.      
downstream tasks에서 LM의 성능을 SciBERT, BERT 및 기타 기준 모델과 비교하여  materials’ specific information를 학습하기 위해 MatSciBERT의 효과를 평가했다.     

**[Supplementary Fig1.]**
![image](https://user-images.githubusercontent.com/76824611/225911756-535c3e47-1001-412d-84aa-86d94d9cd082.png)


pre-training이 모델 성능에 미치는 영향을 이해하기 위해,     
[SOFC-slot](https://huggingface.co/datasets/sofc_materials_articles)에 대한 materials domain-specific downstream task인 NER이 pre-training의 정기적인 간격으로 모델을 사용하여 수행되었다.      
이 것으로, pre-trained model은 SOFC-slot dataset의 training set o에서 finetuned되었다.     

SOFC-slot dataset의 선택은 dataset가 세분화된  materials-specific information로 구성되어 있다는 사실에 기초했다.     
따라서 이 데이터 세트는 SciBERT의 성능을 materials-aware LMs과 구별하는 데 적합하다.      
이러한 finetuned 모델의 성능은 test 세트에서 평가되었다.   


LM-CRF architecture는 이 연구의 후반부에서 보여주듯이  downstream task에 대해 지속적으로 최상의 성능을 제공하기 때문에 분석에 사용되었다.      


![image](https://user-images.githubusercontent.com/76824611/225914989-9358c11b-7357-41a8-aeba-fe14a9b298c9.png)

macro-F1 averages( across three seeds exhibited)은 증가 추세를 보인다( Supplementary Fig. 2a)     
➡ **longer durations train의 중요성**을 시사함    

우리는 또한 abstract classification task에 대한 유사한 그래프를 보여준다(Supplementary Fig. 2b).



















