---
title: "MatSciBERT: A materials domain language model for text mining and information extraction ì •ë¦¬"
date:   2023-02-15
excerpt: "Specializing Multi-domain NMT via Penalizing Low Mutual Information paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



materials scienceì— ëŒ€í•œ knowledgeê°€ ë§ì´ ìƒì„±ë˜ë©°, í…ìŠ¤íŠ¸ë¡œ ì €ì¥ë˜ì–´ì™”ë‹¤.    

<span style="background-color:#F5F5F5">**[ë¬¸ì œ]**</span>         
Bidirectional Encoder Representations from Transformers (BERT)ê³¼ ê°™ì€ NLP ëª¨ë¸ì€ ìœ ìš©í•œ ì •ë³´ ì¶”ì¶œ ë„êµ¬ì´ì§€ë§Œ, ì´ëŸ¬í•œ ëª¨ë¸ì€ <span style="background-color:#fff5b1">**materials science íŠ¹ì • í‘œê¸°ë²•(materials science specific notations)**ê³¼ **ì „ë¬¸ ìš©ì–´(jargons)**ì— ëŒ€í•´ trainë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—</span> materials scienceì— ì ìš©í•  ë•Œ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í•œë‹¤.         

<span style="background-color:#F5F5F5">**[í•´ê²°: MatSciBERT]**</span>     
* <span style="background-color:#FFE6E6">**peer-reviewed materials science publicationsì˜ large corpus**ì— ëŒ€í•´ train</span>ëœ materials-aware language modelì´ë‹¤.        
* ì´ ëª¨ë¸ì€ MatSciBERTê°€  science corpusì— ëŒ€í•´ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì¸ **[SciBERT](https://yerimoh.github.io/LAN22/)ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ë‹¤.**    
* downstream tasks, named entity recognition, relation classification, and abstract classificationì—ì„œ SOTAì„±ëŠ¥ì´ ë‚œë‹¤.   


ë³¸ ë…¼ë¬¸ì€ MatSciBERTì˜ pre-trained weightsë¥¼ ê³µê°œì ìœ¼ë¡œ ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.      

----
---


# INTRODUCTION
<span style="background-color:#fff5b1">**materialsë¥¼ ë°œê²¬í•˜ê³  ì‹¤ìš©ì ì¸ ì‘ìš©ì— í™œìš©**í•˜ëŠ” ê²ƒì€ ìˆ˜ì‹­ ë…„ ë‹¨ìœ„ë¡œ ê±¸ë ¤ **ì‹œê°„ì´ ë§¤ìš° ë§ì´ë“ ë‹¤**.</span>       
**ì´ ê³¼ì •ì„ ê°€ì†í™”**í•˜ê¸° ìœ„í•´ì„œëŠ” ì—„ê²©í•œ ê³¼í•™ì  ì ˆì°¨ë¥¼ í†µí•´ 
<span style="background-color:#fff5b1"**ìˆ˜ì„¸ê¸°ì— ê±¸ì³ ê°œë°œëœ ì¬ë£Œì— ëŒ€í•œ ì§€ì‹ì„ ì‘ì§‘ë ¥ ìˆëŠ” ë°©ì‹ìœ¼ë¡œ í™œìš©í•˜ê³  í™œìš©**</span>í•´ì•¼ í•œë‹¤.       

Textbooks, scientific publications, reports, handbooks, websites ë“±ì€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ì±„êµ´í•  ìˆ˜ ìˆëŠ” í° ë°ì´í„° ì €ì¥ì†Œì˜ ì—­í• ì„ í•œë‹¤.     
ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ ê³¼í•™ ë°ì´í„°ëŠ” the form of text, paragraphs with cross reference, image captions, and tablesì˜ í˜•íƒœë¡œ **ë°˜êµ¬ì¡°í™”ë˜ê±°ë‚˜ êµ¬ì¡°í™”ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸**ì— ì´ëŸ¬í•œ í…ìŠ¤íŠ¸ì—ì„œ **ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ì‘ì—…**ì´ë‹¤.       

âš  ì´ëŸ¬í•œ ì •ë³´ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ê²ƒì€, **ì‹œê°„ê³¼ ìì›ì´ ë§¤ìš° ë§ì´ ì†Œìš”**ë˜ë©° **domain ì „ë¬¸ê°€ì˜ í•´ì„ì— ì˜ì¡´**í•œë‹¤.   




---

<span style="background-color:#F5F5F5">**[NLP ëª¨ë¸ ë™í–¥]**</span>      
ì¸ê³µì§€ëŠ¥ì˜ í•˜ìœ„ ë„ë©”ì¸ì¸ ìì—°ì–´ ì²˜ë¦¬(NLP)ëŠ” í…ìŠ¤íŠ¸ì—ì„œ **ì •ë³´ ì¶”ì¶œì„ ìë™í™”**í•  ìˆ˜ ìˆëŠ” ëŒ€ì²´ ì ‘ê·¼ë²•ì„ ì œì‹œí•œë‹¤.     
* **word2vec ë° GloVe:** Neural pretrained embeddingsìœ¼ë¡œ, ê½¤ ì¸ê¸°ê°€ ìˆì§€ë§Œ **domain-specific knowledgeì´ ë¶€ì¡±**í•˜ê³  **ìƒí™©ë³„ ì„ë² ë”©ì„ ìƒì„±í•˜ì§€ ì•ŠëŠ”ë‹¤**.          
* ìµœê·¼ NLPì˜ ë°œì „ì€ **domain-specific tasksì— ëŒ€í•´ pre-trained language model (LM)ì´ finetunedë˜ëŠ” ê³„ì‚° íŒ¨ëŸ¬ë‹¤ì„ì˜ ê°œë°œ**ë¡œ ì´ì–´ì¡Œë‹¤.        
* ì—°êµ¬ì— ë”°ë¥´ë©´ ì´ëŸ¬í•œ pretrain-finetune paradigmì€ 19-23ë…„ ì‚¬ì´ì— SOTAì˜€ë‹¤.    
* ìµœê·¼ì—, ë§ì€ ì–‘ì˜ í…ìŠ¤íŠ¸ì™€ ë†’ì€ ì»´í“¨íŒ… ëŠ¥ë ¥ì˜ ê°€ìš©ì„± ë•Œë¬¸ì—, ì—°êµ¬ìë“¤ì€ ì´ëŸ¬í•œ **í° ì‹ ê²½ ì–¸ì–´ ëª¨ë¸ì„ pretrainì‹œí‚¬ ìˆ˜ ìˆì—ˆë‹¤**.          
ì˜ˆë¥¼ ë“¤ì–´, Bidirectional Encoder Representations from Transformers (BERT)ì€ BookCorpus ë° English Wikipediaì—ì„œ í›ˆë ¨ë˜ì–´  question answeringê³¼ entity recognitionê³¼ ê°™ì€ ì—¬ëŸ¬ NLP ì‘ì—…ì—ì„œ SOTAë¥¼ ë°œíœ˜í•œë‹¤.         


---


<span style="background-color:#F5F5F5">**[materials science domainì—ì„œì˜ NLP applications]**</span>       
ì—°êµ¬ì›ë“¤ì€ **materials science domain**ì—ì„œ ML ì‘ìš© í”„ë¡œê·¸ë¨ì„ ìœ„í•œ **ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±ì„ ìë™í™”**í•˜ê¸° ìœ„í•´ **NLP toolsë¥¼ ì‚¬ìš©**í–ˆë‹¤. ì•„ë˜ëŠ” í™œìš©ì˜ ì˜ˆì‹œì´ë‹¤.          
ì•„ë˜ ì˜ˆì‹œë“¤ì€ ëª¨ë‘ NLP íŒŒì´í”„ë¼ì¸ì¸ [ChemDataExtractor](https://pubs.acs.org/doi/10.1021/acs.jcim.6b00207)ë¥¼ ì‚¬ìš©í–ˆë‹¤.    
* [create databases of battery materials](https://www.nature.com/articles/s41597-020-00602-2)       
* [Curie and NÃ©el temperatures of magnetic materials](https://www.nature.com/articles/sdata2018111)              
* [inorganic material synthesis routes](https://www.nature.com/articles/s41597-019-0224-1)       
* [composition and dissolution rate of calcium aluminosilicate glassy materials](https://ceramics.onlinelibrary.wiley.com/doi/10.1111/jace.17631)ì˜ ì¡°ì„±     
* [ìš©í•´ìœ¨ê³¼ zeolite í•©ì„±ê²½ë¡œë¥¼ ìˆ˜ì§‘í•˜ì—¬ zeolite](https://pubs.acs.org/doi/10.1021/acscentsci.9b00193)ë¥¼ í•¨ìœ í•˜ëŠ” ê²Œë¥´ë§ˆëŠ„ì„ í•©ì„±     
* [process and testing parameters of oxide glasses ë° testing parametersë¥¼ ì¶”ì¶œí•˜ì—¬ Vickers
hardnessì˜ í–¥ìƒëœ ì˜ˆì¸¡ì„ ê°€ëŠ¥í•˜ê²Œ í•¨](https://web.iitd.ac.in/~krishnan/publications/Krishnan/102_Extracting%20processing%20and%20test.html)              
* [computational materials science research papersì—ì„œ ì¶”ì¶œí•œ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë§Œë“œëŠ” ìë™í™”ëœ NLP](https://www.researchgate.net/publication/349533689_MatScIE_An_automated_tool_for_the_generation_of_databases_of_methods_and_parameters_used_in_the_computational_materials_science_literature) ë„êµ¬ë¥¼ ë§Œë“¦              
* [topic modeling in glasses](http://www.gstatic.com/generate_204): unsupervised fashionìœ¼ë¡œ ë¬¸í•™ì„ ë‹¤ë¥¸ ì£¼ì œë¡œ ê·¸ë£¹í™”í•¨, specific queries(elements present, synthesis, or characterization techniques, and applications)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì°¾ëŠ” ë° ì‚¬ìš©ë˜ì—ˆë‹¤.           




**[Olivetti et al.(2019)](https://aip.scitation.org/doi/abs/10.1063/5.0021106)ì˜ í¬ê´„ì  ê²€í† **ëŠ” NLPê°€ materials scienceì— ì´ìµì„ ì¤„ ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ë°©ë²•ì„ ì„¤ëª…í•œë‹¤.    
* [OSCAR4](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-3-41): insights into chemical parsing toolsì œê³µ, í™”í•™ì  êµ¬ë¬¸ ë¶„ì„ ë„êµ¬ì— ëŒ€í•œ í†µì°°ë ¥ì„ ì œê³µ        
* [Artificial Chemis](https://onlinelibrary.wiley.com/doi/10.1002/adma.202001626): ì „êµ¬ì²´ ì •ë³´ì˜ ì…ë ¥ì„ ë°›ì•„ ëª©í‘œ ëŒ€ì—­ ê°„ê²©ì„ ê°€ì§„ ê´‘ì „ì ë°˜ë„ì²´ë¥¼ ì œì¡°í•˜ê¸° ìœ„í•œ í•©ì„± ê²½ë¡œë¥¼ ìƒì„±          
* [robotic system](https://www.science.org/doi/10.1126/sciadv.aaz8867): ë³´ë‹¤ ê¹¨ë—í•˜ê³  ì§€ì† ê°€ëŠ¥í•œ ì—ë„ˆì§€ ì†”ë£¨ì…˜ì„ ìƒì‚°í•˜ê¸° ìœ„í•´ thin filmsì„ ë§Œë“¦     
â¡ 37ê³¼ 8ì²œë§Œ ê°œ ì´ìƒì˜ materials science domain-specific named entitiesë¥¼ ì‹ë³„í•˜ëŠ” ì—°êµ¬ëŠ” MLê³¼ NLP ê¸°ìˆ ì˜ ê²°í•©ì„ í†µí•´ **ë‹¤ì–‘í•œ ì‘ìš©ì„ ìœ„í•œ ì¬ë£Œì˜ ê°€ì†í™”ë¥¼ ì´‰ì§„**í–ˆë‹¤.      


---


<span style="background-color:#F5F5F5">**[Word2vecì™€ BERTì˜ í™•ì¥]**</span>       
ì—°êµ¬ìë“¤ì€ Word2vecì™€ BERTì˜ ë„ë©”ì¸ ì ì‘ ëŠ¥ë ¥ì„ ì•„ë˜ì™€ ê°™ì´ í™•ì¥í•˜ì˜€ë‹¤.    
* [BioWordVec](https://www.nature.com/articles/s41597-019-0055-0), [BioBERT](https://arxiv.org/abs/1901.08746): ìƒë¬¼ê³¼í•™ ë¶„ì•¼ì—ì„œ í™•ì¥            
* [SciBERT](https://yerimoh.github.io/LAN22/): scientific and biomedical corpusìœ¼ë¡œ train        
* [clinicalBERT](https://arxiv.org/abs/1904.05342): 2 million clinical notes in [MIMIC-III v1.4 database](https://www.nature.com/articles/sdata201635)ì—ì„œ í›ˆë ¨ë¨    
* [mBERT](https://aclanthology.org/2020.findings-emnlp.150/): multilingual machine translations tasks    
* [PatentBERT](http://www.digital.ntu.edu.tw/hsiang/pdf/WPI%20Patent%20classification%20by%20fine-tuning%20BERT.PDF): patent classification    
* [FinBERT](https://arxiv.org/abs/1908.10063): financial tasks        



âœ¨ ì´ëŠ” <span style="background-color:#fff5b1">**materials-aware LM**ì´ downstream tasksì— ì¶”ê°€ë¡œ ì ì‘í•¨ìœ¼ë¡œì¨ **í•´ë‹¹ ë¶„ì•¼ì˜ ì—°êµ¬ë¥¼ í¬ê²Œ ê°€ì†í™”í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬**</span>í•œë‹¤.       


ì´ ì—°êµ¬ ì´ì „ì—ëŠ” ì¬ë£Œ ì¸ì‹ ì–¸ì–´ ëª¨ë¸ ê°œë°œì— ëŒ€í•œ ë…¼ë¬¸ì´ ì—†ì—ˆì§€ë§Œ, ìµœê·¼ [a recent preprint](https://www.researchgate.net/publication/355846376_The_Impact_of_Domain-Specific_Pre-Training_on_Named_Entity_Recognition_Tasks_in_Materials_Science)ì€  materials scienceì—ì„œ domain-specific language modelsì´ named entity recognition (NER) ì‘ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê°•ì¡°í•œë‹¤.       

---

<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸]**</span>       
ì´ ì—°êµ¬ì—ì„œ, ìš°ë¦¬ëŠ” materials science domain-specific BERT, ì¦‰ **MatSciBERT**ë¥¼ í›ˆë ¨ì‹œí‚¨ë‹¤.      
Fig. 1ì€ materials science ë§ë­‰ì¹˜ ìƒì„±, MatSciBERT í›ˆë ¨ ë° ë‹¤ì–‘í•œ downstream tasks í‰ê°€ë¥¼ í¬í•¨í•˜ëŠ” ë³¸ ì—°êµ¬ì— ì±„íƒëœ ë°©ë²•ë¡ ì˜ ê·¸ë˜í”½ ìš”ì•½ì„ ë³´ì—¬ì¤€ë‹¤.              
ìš°ë¦¬ëŠ” ì•„ë˜ ë‚˜ì—´ëœ ê²ƒì²˜ëŸ¼ **domain-specific tasksì— ëŒ€í•œ SoTA**ë¥¼ ë‹¬ì„±í•œë‹¤.      
* **a.**  [NER on SOFC, SOFC Slot dataset by Friedrich et al. (2020)45     
 Matscholar dataset by Weston et al](https://pubmed.ncbi.nlm.nih.gov/31361962/)                    
* **b.** [Glass vs. Non-Glass classification of paper abstracts](https://www.sciencedirect.com/science/article/pii/S2666389921001239)             
* **c.**  [Relation Classification on MSPT corpus](https://aclanthology.org/W19-4007/)            


í˜„ì¬ ì—°êµ¬ëŠ” materials domain language modelì˜ ê°€ìš©ì„± ê²©ì°¨ë¥¼ í•´ì†Œí•˜ì—¬ ì—°êµ¬ìê°€ ì •ë³´ ì¶”ì¶œ, knowledge graph completion ë° ê¸°íƒ€  downstream tasksì„ ìë™í™”í•˜ì—¬ **materials ë°œê²¬ì„ ê°€ì†í™”**í•  ìˆ˜ ìˆë‹¤.        

**[Fig. 1 Methodology for training MatSciBERT]**
![image](https://user-images.githubusercontent.com/76824611/225840065-ddbf1a52-fd41-4e8f-9f53-55d4f966fb06.png)   
* ìš°ë¦¬ëŠ” ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸ì„ ì„ íƒí•œ í›„ query searchì„ í†µí•´ Materials Science Corpus (MSC)ë¥¼ ë§Œë“ ë‹¤.     
* MSCì—ì„œ pre-trainedëœ MatSciBERTëŠ” ë‹¤ì–‘í•œ downstream tasksì—ì„œ í‰ê°€ëœë‹¤.       


---

<span style="background-color:#F5F5F5">**[open source]**</span>       
* [huggingface/matscibert](https://huggingface.co/m3rg-iitd/matscibert): pre-trained weightsì„ ì œê³µ    
[github/MatSciBERT](https://github.com/M3RG-IITD/MatSciBERT): MatSciBERTì˜ pretrainingê³¼ finetuning on downstream tasks ì½”ë“œ ê³µê°œ            
* [zenodo](https://doi.org/ 10.5281/zenodo.6413296): downstream tasksì„ ìœ„í•œfinetuned
modelsì´ ìˆëŠ” ì½”ë“œ      




---
---

# RESULTS AND DISCUSSION
## Dataset
Textual datasetsëŠ” LM í›ˆë ¨ì˜ í•„ìˆ˜ì ì¸ ë¶€ë¶„ì´ë‹¤.     

<span style="background-color:#F5F5F5">**[materials domainì— ì í•©í•˜ì§€ ì•Šì€ corpora]**</span>       
* **BookCorpus, ìœ„í‚¤í”¼ë””ì•„**ê³¼ ì˜ì–´ì™€ ê°™ì€ ë§ì€ ë²”ìš© ë§ë­‰ì¹˜     
* **biomedical corpus, clinical database**: ê°™ì€ domain-specific corpora   


<span style="background-color:#F5F5F5">**[materials domainì— ì í•©í•œ corporaë¡œ data ë§Œë“¤ê¸°]**</span>       
* materials specific LMì„ ì œê³µí•˜ê¸° ìœ„í•´ **4ê°œì˜ important materials ì œí’ˆêµ°ì— ê±¸ì¹œ ë§ë­‰ì¹˜**ë¥¼ ìƒì„±    
   * inorganic glasses    
   * metallic glasses     
   * alloys    
   * cement & concrete     
* ìœ„ì™€ ê°™ì€ ê´‘ë²”ìœ„í•œ ë²”ì£¼ê°€ ì–¸ê¸‰ë˜ì—ˆì§€ë§Œ, 2ì°¨ì› ë¬¼ì§ˆì„ í¬í•¨í•œ **ëª‡ ê°€ì§€ ë‹¤ë¥¸ ë²”ì£¼ì˜ ë¬¼ì§ˆë„ ë§ë­‰ì¹˜ì— ì¡´ì¬í–ˆë‹¤ëŠ” ì ì— ìœ ì˜**í•´ì•¼ í•œë‹¤.         
* ì´ ì—°êµ¬ë¥¼ ìœ„í•´ ê°œë°œëœ materials science corpusëŠ” ~285M ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°,
 ì´ëŠ” SciBERT(3.17B ë‹¨ì–´)ì™€ BERT(3.3B ë‹¨ì–´)ë¥¼ ì‚¬ì „ í›ˆë ¨í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ì˜ ê±°ì˜ 9%ì´ë‹¤.    
 â¡ <span style="background-color:#fff5b1"> **SciBERT pre-trainì„ ê³„ì†**í•˜ê¸° ë•Œë¬¸ì—</span>        MatSciBERTëŠ” 3.17 + 0.28 = 3.45B ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ë§ë­‰ì¹˜ì— ëŒ€í•´ **íš¨ê³¼ì ìœ¼ë¡œ í›ˆë ¨**ëœë‹¤.         


<details>
<summary>ğŸ“œ Supplementary Table 1</summary>
<div markdown="1"> 
 
**[Supplementary Table 1]**      
![image](https://user-images.githubusercontent.com/76824611/225872626-6ca1a9b1-fd9a-49fb-b015-8e0ca1d22b83.png)
* ë§ë­‰ì¹˜ë¥¼ ë§Œë“œëŠ” ë‹¨ê³„ëŠ” ë°©ë²•ì€  Methods sectionì— ë‚˜ì™€ ìˆë‹¤.     
* ë‹¨ì–´ì˜ 40%ëŠ” norganic glasses ë° ceramicsê³¼ ê´€ë ¨ëœ ì—°êµ¬ ë…¼ë¬¸ì—ì„œ ë‚˜ì™”ë‹¤.        
* ë‹¨ì–´ì˜ ê°ê° 20%ëŠ” bulk metallic glasses(BMG), alloysì—ì„œ ë‚˜ì˜¨ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.      
* 'cement and concrete'ì˜ ì—°êµ¬ ë…¼ë¬¸ ìˆ˜ëŠ” 'inorganic glasses and ceramics'ë³´ë‹¤ ë§ì§€ë§Œ,   
 'inorganic glasses and ceramics'ê°€ ë‹¨ì–´ ìˆ˜ê°€ ë” ë§ë‹¤.     
â¡ ì´ëŠ” 'inorganic glasses and ceramics'ì˜ ë²”ì£¼ì™€ ê´€ë ¨í•˜ì—¬ ê²€ìƒ‰ëœ ë§ì€ ìˆ˜ì˜ full-text documentsê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤.    
* êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” Elsevier Science Direct Databaseì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ì•½ 1ë°±ë§Œ ê°œì˜ ë…¼ë¬¸ ì¤‘ì—ì„œ ì•½ 150ë§Œ ê°œì˜ ë…¼ë¬¸ì„ ì„ íƒí–ˆë‹¤.   

</div>
</details>

<details>
<summary>ğŸ“œ Supplementary Table 2</summary>
<div markdown="1">
 
**[Supplementary Table 2]**     
![image](https://user-images.githubusercontent.com/76824611/225872675-f316ca7f-b015-4ec4-95f1-20f6b527545a.png)
* materials science ë¶„ì•¼ì™€ ê´€ë ¨ëœ ì¤‘ìš”í•œ ë¬¸ìì—´ì˜ ë‹¨ì–´ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.    
* ë§ë­‰ì¹˜ëŠ” thermoelectric(ì—´ì „), nanomaterials(ë‚˜ë…¸ ì¬ë£Œ), polymers(ê³ ë¶„ì), and biomaterials(ìƒì²´ ì¬ë£Œ)ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ë¥¼ í¬ê´„í•œë‹¤ëŠ” ì ì— ìœ ì˜í•´ì•¼ í•œë‹¤.     
* ë˜í•œ ì–¸ì–´ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë§ë­‰ì¹˜ëŠ” ì‹¤í—˜(experimental) ë° ê³„ì‚°(computational) taskë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ **ëª¨ë‘ ë¬¼ì§ˆ ë°˜ì‘(material response)ì„ ì´í•´**í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.    
* ì´ ë§ë­‰ì¹˜ì˜ í‰ê·  ë…¼ë¬¸ ê¸¸ì´ëŠ” ~1848 ë‹¨ì–´ì´ë©°,    
ì´ëŠ” **SciBERT** ë§ë­‰ì¹˜ì˜ í‰ê·  ë…¼ë¬¸ ê¸¸ì´ 2769 ë‹¨ì–´ì˜ **3ë¶„ì˜ 2**ë‹¤.     
* í‰ê·  ë…¼ë¬¸ ê¸¸ì´ê°€ ì ì€ 2ê°€ì§€ ì´ìœ    
    * **(a)** ì¼ë°˜ì ìœ¼ë¡œ **materials science ë…¼ë¬¸**ì€ biomedical ë…¼ë¬¸ë³´ë‹¤ **ì§§ë‹¤**.         
    * **(b)** ìš°ë¦¬ ë§ë­‰ì¹˜ì—ë„ full textê°€ ì—†ëŠ” ë…¼ë¬¸ì´ ìˆë‹¤. ê·¸ëŸ¬í•œ ê²½ìš°, ìš°ë¦¬ëŠ” ìµœì¢… ë§ë­‰ì¹˜ì— ë„ë‹¬í•˜ê¸° ìœ„í•´ ê·¸ëŸ¬í•œ **ë…¼ë¬¸ì˜ ìš”ì•½ì„ ì‚¬ìš©**í–ˆë‹¤.            

</div>
</details>



---



## Pre-training of MatSciBERT


<span style="background-color:#F5F5F5">**[domain adaptive pretraining]**</span>       
MatSciBERT pre-trainingì˜ ê²½ìš°,    
ë³¸ ë…¼ë¬¸ì€ <span style="background-color:#fff5b1">[domain adaptive pretraining](https://aclanthology.org/2020.acl-main.740/)ì„ ë”°ë¥¸ë‹¤</span>.              
* **ë°©ë²•**      
ì´ ì—°êµ¬ì—ì„œ ì €ìë“¤ì€ domain-specific textì˜ ë§ë­‰ì¹˜ì— ëŒ€í•œ initial LMì˜ pre-trainingì„ ê³„ì†í–ˆë‹¤.(ì²˜ìŒë¶€í„° ë‹¤ì‹œ í•˜ì§€ ì•Šê³  ê¸°ì¡´ pretrainëœ ê²ƒì— ì¶”ê°€ë¡œ í–ˆë‹¤ëŠ” ëœ»)           
* **ê²°ê³¼**      
ê·¸ë“¤ì€ initial LM ì–´íœ˜ì™€ domainë³„ ì–´íœ˜ ì‚¬ì´ì˜ ì¤‘ë³µì´ 54.1% ë¯¸ë§Œì„ì—ë„ ë¶ˆêµ¬í•˜ê³ ,            
4ê°œ domain ëª¨ë‘ì— ëŒ€í•œ **domainë³„ downstream tasksì˜ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒ**ë˜ëŠ” ê²ƒì„ ê´€ì°°í–ˆë‹¤.           
* **ë¹„ìŠ·í•œ ë°©ì‹ì˜ ëª¨ë¸**    
**BioBERT19ì™€ FinBERT22:** vanilla BERT ëª¨ë¸ì— domain-specific textë¥¼ ì¶”ê°€ë¡œ pretrainedí•œ ë¹„ìŠ·í•œ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê°œë°œë¨(í† í°í™”ëŠ” ì›ë˜ BERT ì–´íœ˜ë¥¼ ì‚¬ìš©)      



<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì—ì˜ ì ìš©]**</span>       
* **MatSciBERT weights:**    
ì¼ë¶€ ì í•©í•œ LMì˜ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”í•œ ë‹¤ìŒ, MSCì—ì„œ pre-trainí•œë‹¤.         
MatSciBERTì— ëŒ€í•œ ì ì ˆí•œ ì´ˆê¸° ê°€ì¤‘ì¹˜ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” tokenizers library(48)ì„ ì‚¬ìš©í•˜ì—¬   
Materials Science Corpus(MSC)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ uncased wordpiece(47)ë¥¼ í›ˆë ¨ì‹œì¼°ë‹¤.     
* **MSC ì–´íœ˜ì˜ ì¤‘ë³µ:**     
uncased SciBERT21 ì–´íœ˜ì˜ ê²½ìš° 53.64%, uncasedëŠ” BERT ì–´íœ˜ì˜ ê²½ìš° 38.90%ì´ë‹¤.    
SciBERTì˜ ì–´íœ˜ì™€ ì¤‘ë³µì´ ì»¤ì„œ, ìš°ë¦¬ëŠ” <span style="background-color:#fff5b1">**SciBERT ì–´íœ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ë­‰ì¹˜ë¥¼ í† í°í™”**</span>í•˜ê³ ,   
Beltagy et al. (2019)(21)ì— ì˜í•´ ê³µê°œëœ <span style="background-color:#fff5b1">**SciBERTì˜ ê°€ì¤‘ì¹˜ë¡œ MatSciBERT ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”**</span>í•œë‹¤.     


<details>
<summary>ğŸ“œ materials domain-specific vacabì´ ì•„ë‹Œ SciBERT vocabularyë¡œ í† í°í™” í•˜ëŠ” ì´ìœ </summary>
<div markdown="1">

 
materials scienceë³„ ì–´íœ˜ëŠ” ë§ë­‰ì¹˜ë¥¼ **ë” ì ì€ ìˆ˜ì˜ ë‹¨ì–´ë¡œ í‘œí˜„**í•˜ê³  **ì ì¬ì ìœ¼ë¡œ ë” ë‚˜ì€ ì–¸ì–´ ëª¨ë¸ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ**ì„ ì–¸ê¸‰í•  ê°€ì¹˜ê°€ ìˆë‹¤.      
ì˜ˆë¥¼ ë“¤ì–´,    
<center>â€œyttria-stabilized zirconiaâ€</center>         
* SciBERT vocabularyë¡œ í† í°í™”: ```[ â€œytâ€, â€œ##triâ€, â€œ##aâ€, â€œ-â€, â€œstabilizedâ€, â€œzirconâ€, â€œ##iaâ€]```     
* domain-specific í† í°í™”: ```["yttria", "-", "stabilized", "zircon"]```       
âš ï¸ ê·¸ëŸ¬ë‚˜, domain-specific í† í°í™”ë¥¼ ì‚¬ìš©í•˜ë©´ SciBERT ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë©°,    
SciBERTê°€ ì´ë¯¸ í•™ìŠµí•œ ê³¼í•™ ì§€ì‹ì„ í™œìš©í•œë‹¤.      
âš ï¸ ë˜í•œ  deep neural language models ì€ ê¸°ì¡´ í† í°í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ **ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°˜ë³µ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—**, **ì¬ë£Œ ì˜ì—­ì— SciBERT ì–´íœ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°˜ë“œì‹œ í•´ë¡œìš´ ê²ƒì€ ì•„ë‹ˆë‹¤**.     

ì˜ˆë¥¼ ë“¤ì–´, ë‹¨ì–´ ì¡°ê° ```"yt"```, ```"##tri"```, ê·¸ë¦¬ê³  ```"##a"```ê°€ ì—°ì†ì ìœ¼ë¡œ ë°œìƒí•  ë•Œ,   
SciBERTëŠ” downstream tasksì—ì„œ ì…ì¦ëœ ê²ƒì²˜ëŸ¼ ì–´ë–¤ ë¬¼ì§ˆì´ ë…¼ì˜ë˜ê³  ìˆìŒì„ ì¸ì‹í•œë‹¤.  
ì´ëŸ¬ë©´ domain-specificì´ ```"yttria"```ë¼ê³  í† í°í™” í•œê²ƒê³¼ ë¹„ìŠ·í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.     
 
â¡ ì´ê²Œ ì™œ BERT-based LMs(FinBERT22, BioBERT19, and ClinicalBERT40)ì´ <span style="background-color:#fff5b1">**domain-specific tokenizersë¥¼ ì•ˆì“°ê³  ê¸°ì¡´ pre-trainingì„ í™•ì¥**í•˜ëŠ” ì´ìœ ì´ë‹¤.</span> 
 
</div>
</details>  

         

<span style="background-color:#F5F5F5">**[pretraining ê²°ê³¼: Supplementary Fig1.]**</span>       
![image](https://user-images.githubusercontent.com/76824611/225911756-535c3e47-1001-412d-84aa-86d94d9cd082.png)
* pre-trainingì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ Methods sectionì— ë‚˜ì™€ ìˆë‹¤.    
* pre-trainingì€ 360ì‹œê°„ ë™ì•ˆ ìˆ˜í–‰ë˜ì—ˆê³ , final perplexity of 2.998ì„ ë‹¬ì„±í–ˆë‹¤(Supplementary Fig1. 1a).      
* ì„œë¡œ ë‹¤ë¥¸ vocabularyì™€ validation corpusë¡œ ì¸í•´ ì§ì ‘ ë¹„êµí•  ìˆ˜ëŠ” ì—†ì§€ë§Œ,     
BERT25ì™€ RoBERTa49 ì €ìëŠ” ê°ê° ë™ì¼í•œ ë²”ìœ„ì— ìˆëŠ” 3.99ì˜ perplexitiesë¥¼ ë‹¬ì„±í–ˆë‹¤.     
* ìš°ë¦¬ëŠ” ë˜í•œ Supplementary Fig. 1b, cì˜ MLM loss ë° MLM accuracyì™€ ê°™ì€ evaluation metricsì— ëŒ€í•œ ê·¸ë˜í”„ë¥¼ ì œê³µí•œë‹¤.     
ê·¸ëŸ° ë‹¤ìŒ ìµœì¢… pre-trained LMì„ ì‚¬ìš©í•˜ì—¬ **ë‹¤ì–‘í•œ materials science domain-specific downstream tasksì„ í‰ê°€**í–ˆìœ¼ë©°, ì´ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ í›„ì† ì„¹ì…˜ì— ì„¤ëª…ë˜ì–´ ìˆë‹¤.      
* **downstream tasksì—ì„œ LMì˜ ì„±ëŠ¥ì„ SciBERT, BERT ë° ê¸°íƒ€ ê¸°ì¤€ ëª¨ë¸ê³¼ ë¹„êµ**í•˜ì—¬  **materialsâ€™ specific information**ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ **MatSciBERTì˜ íš¨ê³¼**ë¥¼ í‰ê°€í–ˆë‹¤.     



<span style="background-color:#F5F5F5">**[pretraining ì´ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì´í•´]**</span>       
* pre-trainingì´ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì´í•´í•˜ê¸° ìœ„í•´,     
[SOFC-slot](https://huggingface.co/datasets/sofc_materials_articles)ì— ëŒ€í•œ materials domain-specific downstream taskì¸ **NERì´ pre-trainingì˜ ì •ê¸°ì ì¸ ê°„ê²©ìœ¼ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë˜**ì—ˆë‹¤.      
* ì´ê²ƒìœ¼ë¡œ, pre-trained modelì€ SOFC-slot datasetì˜ training setì—ì„œ finetunedë˜ì—ˆë‹¤.     
* **SOFC-slot dataset**ì€ **datasetê°€ ì„¸ë¶„í™”ëœ  materials-specific informationë¡œ êµ¬ì„±**ë˜ì–´ ìˆì–´ì„œ ì´ê±¸ ì‚¬ìš©í–ˆë‹¤.     
â¡ ë”°ë¼ì„œ ì´ ë°ì´í„° ì„¸íŠ¸ëŠ”** SciBERTì˜ ì„±ëŠ¥ì„ materials-aware LMsê³¼ êµ¬ë³„í•˜ëŠ” ë° ì í•©**í•˜ë‹¤.      
* ì´ëŸ¬í•œ finetuned ëª¨ë¸ì˜ ì„±ëŠ¥ì€ test ì„¸íŠ¸ì—ì„œ í‰ê°€ë˜ì—ˆë‹¤.       


<span style="background-color:#F5F5F5">**[longer durations trainì˜ ì¤‘ìš”ì„±]**</span>       
![image](https://user-images.githubusercontent.com/76824611/225914989-9358c11b-7357-41a8-aeba-fe14a9b298c9.png)
* LM-CRF architectureëŠ” ì´ ì—°êµ¬ì˜ í›„ë°˜ë¶€ì—ì„œ ë³´ì—¬ì£¼ë“¯ì´  downstream taskì— ëŒ€í•´ ì§€ì†ì ìœ¼ë¡œ ìµœìƒì˜ ì„±ëŠ¥ì„ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ë¶„ì„ì— ì‚¬ìš©ë˜ì—ˆë‹¤.        
* macro-F1 averages(across three seeds exhibited)ì€ ì¦ê°€ ì¶”ì„¸ë¥¼ ë³´ì¸ë‹¤(Supplementary Fig. 2a)     
â¡ **longer durations trainì˜ ì¤‘ìš”ì„±**ì„ ì‹œì‚¬í•¨    
* ìš°ë¦¬ëŠ” ë˜í•œ abstract classification taskì— ëŒ€í•œ ìœ ì‚¬í•œ ê·¸ë˜í”„ë¥¼ ë³´ì—¬ì¤€ë‹¤(Supplementary Fig. 2b).




---
---

## Downstream tasks
Here, we evaluate MatSciBERT on three materials science specific
downstream tasks namely, Named Entity Recognition (NER),
Relation Classification, and Paper Abstract Classification.
ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ëª…ëª…ëœ ì‹¤ì²´ ì¸ì‹(NER), ê´€ê³„ ë¶„ë¥˜ ë° ì¢…ì´ ì¶”ìƒ ë¶„ë¥˜ì˜ ì„¸ ê°€ì§€ ì¬ë£Œ ê³¼í•™ íŠ¹ì • ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•´ MatSciBERTë¥¼ í‰ê°€í•œë‹¤.


We now present the results on the three materials science NER
datasets as described in the Methods section. To the best of our
knowledge, the best Macro-F1 on solid oxide fuel cells (SOFC) and
SOFC-Slot datasets is 81.50% and 62.60%, respectively, as reported
by Friedrich et al. (2020), who introduced the dataset44. We run
the experiments on the same train-validation-test splits as done
by Friedrich et al. (2020) for a fair comparison of results. Moreover,
since the authors reported results averaged over 17 entities (the
extra entity is â€œThicknessâ€) for the SOFC-Slot dataset, we also
report the results taking the â€˜Thicknessâ€™ entity into account
ì´ì œ ë°©ë²• ì„¹ì…˜ì— ì„¤ëª…ëœ ëŒ€ë¡œ ì„¸ ê°€ì§€ ì¬ë£Œ ê³¼í•™ NER ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì œì‹œí•œë‹¤. ìš°ë¦¬ê°€ ì•„ëŠ” í•œ, ê³ ì²´ ì‚°í™”ë¬¼ ì—°ë£Œ ì „ì§€(SOFC)ì™€ SOFC-ìŠ¬ë¡¯ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ìµœìƒì˜ ë§¤í¬ë¡œ-F1ì€ ë°ì´í„° ì„¸íŠ¸ 44ë¥¼ ë„ì…í•œ Friedrich et al.(2020)ì´ ë³´ê³ í•œ ë°”ì™€ ê°™ì´ ê°ê° 81.50%ì™€ 62.60%ì´ë‹¤. ìš°ë¦¬ëŠ” ê²°ê³¼ì˜ ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ Friedrich ë“±(2020)ì´ ìˆ˜í–‰í•œ ê²ƒê³¼ ë™ì¼í•œ ì—´ì°¨ ìœ íš¨ì„± ê²€ì‚¬ ë¶„í• ì— ëŒ€í•œ ì‹¤í—˜ì„ ì‹¤í–‰í•œë‹¤. ë”ìš±ì´ ì €ìë“¤ì€ SOFC-Slot ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ 17ê°œ ì—”í‹°í‹°(ì¶”ê°€ ì—”í‹°í‹°ëŠ” "ë‘ê»˜")ì— ê±¸ì³ í‰ê·  ê²°ê³¼ë¥¼ ë³´ê³ í–ˆê¸° ë•Œë¬¸ì—, ìš°ë¦¬ëŠ” ë˜í•œ 'ë‘ê»˜" ì—”í‹°í‹°ë¥¼ ê³ ë ¤í•œ ê²°ê³¼ë¥¼ ë³´ê³ í•œë‹¤


Table 1 shows the Macro-F1 scores for the NER task on the
SOFC-Slot and SOFC datasets by MatSciBERT, SciBERT, and BERT.
We observe that LM-CRF always performs better than LM-Linear.
This can be attributed to the fact that the CRF layer can model the
BIO tags accurately. Also, all SciBERT architectures perform better
than the corresponding BERT architecture. We obtained an
improvement of ~6.3 Macro F1 and ~3.2 Micro F1 (see
Supplementary Table 3) on the SOFC-Slot test set for MatSciBERT
vs. SciBERT while using the LM-CRF architecture. For the SOFC test
dataset, MatSciBERT-BiLSTM-CRF performs better than SciBERTBiLSTM-CRF by ~2.1 Macro F1 and ~2.1 Micro F1. Similar
improvements can be seen for other architectures as well. These
MatSciBERT results also surpass the current best results on SOFCSlot and SOFC datasets by ~3.35 and ~0.9 Macro-F1, respectively

í‘œ 1ì€ MatSciBERT, SciBERT ë° BERTì˜ SOFC-Slot ë° SOFC ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ NER ì‘ì—…ì— ëŒ€í•œ ë§¤í¬ë¡œ F1 ì ìˆ˜ë¥¼ ë³´ì—¬ì¤€ë‹¤. ìš°ë¦¬ëŠ” LM-CRFê°€ í•­ìƒ LM-Linearë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•œë‹¤. ì´ëŠ” CRF ê³„ì¸µì´ BIO íƒœê·¸ë¥¼ ì •í™•í•˜ê²Œ ëª¨ë¸ë§í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë˜í•œ ëª¨ë“  SciBERT ì•„í‚¤í…ì²˜ëŠ” í•´ë‹¹ BERT ì•„í‚¤í…ì²˜ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” LM-CRF ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë™ì•ˆ MatSciBERT ëŒ€ SciBERTì— ëŒ€í•œ SOFC-Slot í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ~6.3 Macro F1 ë° ~3.2 Micro F1(ë³´ì¡° í‘œ 3 ì°¸ì¡°)ì˜ ê°œì„ ì„ ì–»ì—ˆë‹¤. SOFC í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ì˜ ê²½ìš°, MatSciBERT-BiLSTM-CRFëŠ” SciBERTBiLSTM-CRFë³´ë‹¤ ~2.1 ë§¤í¬ë¡œ F1 ë° ~2.1 ë§ˆì´í¬ë¡œ F1ë§Œí¼ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤. ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ì—ì„œë„ ìœ ì‚¬í•œ ê°œì„ ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ MatSciBERT ê²°ê³¼ëŠ” SOFC ìŠ¬ë¡¯ ë° SOFC ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ í˜„ì¬ ìµœê³ ì˜ ê²°ê³¼ë¥¼ ê°ê° ~3.35 ë° ~0.9 ë§¤í¬ë¡œ-F1ë¡œ ëŠ¥ê°€í•œë‹¤

It is worth noting that the SOFC-slot dataset consists of 17 entity
types and hence has more fine-grained information regarding the
materials. On the other hand, SOFC has only four entity types
representing coarse-grained information. We notice that the
performance of MatSciBERT on SOFC-slot is significantly better
than that of SciBERT. To further evaluate this aspect, we analyzed
the F1-score of both SciBERT and MatSciBERT on all the 17 entity
types of the SOFC-slot data individually, as shown in Fig. 2.
Interestingly, we observe that for all the materials related entity
types, namely anode material, cathode material, electrolyte
material, interlayer material, and support material, MatSciBERT
performs better than SciBERT. In addition, for materials related
properties such as open circuit voltage and degradation rate,
MatSciBERT is able to significantly outperform SciBERT. This
suggests that MatSciBERT is indeed able to capitalize on the
additional information learned from the MSC to deliver better
performance on complex problems specific to the materials
domain
SOFC-ìŠ¬ë¡¯ ë°ì´í„° ì„¸íŠ¸ëŠ” 17ê°œì˜ ì—”í‹°í‹° ìœ í˜•ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¬ë£Œì™€ ê´€ë ¨ëœ ë³´ë‹¤ ì„¸ë¶„í™”ëœ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‹¤ëŠ” ì ì— ì£¼ëª©í•  í•„ìš”ê°€ ìˆë‹¤. ë°˜ë©´ SOFCëŠ” ê±°ì¹œ ì„¸ë¶„í™”ëœ ì •ë³´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 4ê°œì˜ ì—”í‹°í‹° ìœ í˜•ë§Œ ê°€ì§€ê³  ìˆë‹¤. SOFC ìŠ¬ë¡¯ì—ì„œ MatSciBERTì˜ ì„±ëŠ¥ì´ SciBERTì˜ ì„±ëŠ¥ë³´ë‹¤ í›¨ì”¬ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì¸¡ë©´ì„ ì¶”ê°€ë¡œ í‰ê°€í•˜ê¸° ìœ„í•´ ê·¸ë¦¼ 2ì™€ ê°™ì´ SOFC ìŠ¬ë¡¯ ë°ì´í„°ì˜ ëª¨ë“  17ê°œ ì—”í‹°í‹° ìœ í˜•ì— ëŒ€í•œ SciBERTì™€ MatSciBERTì˜ F1 ì ìˆ˜ë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„í–ˆë‹¤. í¥ë¯¸ë¡­ê²Œë„, ìš°ë¦¬ëŠ” ìŒê·¹ ì¬ë£Œ, ì–‘ê·¹ ì¬ë£Œ, ì „í•´ì§ˆ ì¬ë£Œ, ì¸µê°„ ì¬ë£Œ ë° ì§€ì§€ ì¬ë£Œì™€ ê°™ì€ ëª¨ë“  ì¬ë£Œ ê´€ë ¨ ì—”í‹°í‹° ìœ í˜•ì—ì„œ MatSciBERTê°€ SciBERTë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•œë‹¤. ë˜í•œ ê°œë°© íšŒë¡œ ì „ì•• ë° ì—´í™” ì†ë„ì™€ ê°™ì€ ì¬ë£Œ ê´€ë ¨ íŠ¹ì„±ì˜ ê²½ìš° MatSciBERTê°€ SciBERTë¥¼ í¬ê²Œ ëŠ¥ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” MatSciBERTê°€ ì‹¤ì œë¡œ MSCì—ì„œ í•™ìŠµí•œ ì¶”ê°€ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì¬ë£Œ ì˜ì—­ì— íŠ¹ì •ëœ ë³µì¡í•œ ë¬¸ì œì— ëŒ€í•´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì œê³µí•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤



Now, we present the results for the Matscholar dataset9 in Table 2.
For this dataset too, MatSciBERT outperforms SciBERT, BERT as well
as the current best results, as can be seen in the case of LM-CRF
architecture. The authors obtained Macro-F1 of 85.41% on the
validation set and 85.10% on the test set, and Micro-F1 of 87.09%
and 87.04% (see Supplementary Table 4). We observe that our
best model MatSciBERT-CRF has Macro-F1 values of 88.66% and
86.38%, both better than the existing state of the art.
ì´ì œ, ìš°ë¦¬ëŠ” í‘œ 2ì— Matscholar ë°ì´í„° ì„¸íŠ¸ 9ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì œì‹œí•œë‹¤. ì´ ë°ì´í„° ì„¸íŠ¸ì˜ ê²½ìš°ì—ë„ MatSciBERTëŠ” LM-CRF ì•„í‚¤í…ì²˜ì˜ ê²½ìš°ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ SciBERT, BERTëŠ” í˜„ì¬ ìµœìƒì˜ ê²°ê³¼ë¿ë§Œ ì•„ë‹ˆë¼ SciBERTë¥¼ ëŠ¥ê°€í•œë‹¤. ì €ìë“¤ì€ ìœ íš¨ì„± ê²€ì‚¬ ì„¸íŠ¸ì—ì„œ 85.41%, ì‹œí—˜ ì„¸íŠ¸ì—ì„œ 85.10%ì˜ Macro-F1ê³¼ 87.09%ì™€ 87.04%ì˜ Micro-F1ì„ ì–»ì—ˆë‹¤(ë³´ì¡° í‘œ 4 ì°¸ì¡°). ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ìµœê³  ëª¨ë¸ì¸ MatSciBERT-CRFê°€ ë§¤í¬ë¡œ F1 ê°’ì´ 88.66%ì™€ 86.38%ë¡œ ê¸°ì¡´ì˜ ìµœì²¨ë‹¨ ëª¨ë¸ë³´ë‹¤ ë” ë‚«ë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•œë‹¤.


In order to demonstrate the performance of MatSciBERT, we
demonstrate an example from the validation set of the dataset in
Supplementary Figs. 3 and 4. The overall superior performance of
MatSciBERT is evident from Table 2.

Table 3 shows the results for the Relation Classification task
performed on the Materials Synthesis Procedures dataset46. We
also compare the results with two recent baseline models,
MaxPool and MaxAtt50, details of which can be found in the
Methods section. Even in this task, we observe that MatSciBERT
performs better than SciBERT, BERT, and baseline models
consistently, although with a lower margin.


In Paper Abstract Classification downstream task, we consider
the ability of LMs to classify a manuscript into glass vs. non-glass
topics based on an in-house dataset10. This is a binary
classification problem, with the input being the abstract of a
manuscript. Here too, we use the same baseline models MaxPool
and MaxAtt50. Table 4 shows the comparison of accuracies
achieved by MatSciBERT, SciBERT, BERT, and baselines. It can be
clearly seen that MatSciBERT outperforms SciBERT by more than
2.75% accuracy on the test set.

Altogether, we demonstrate that the MatSciBERT, pre-trained on
a materials science corpus, can perform better than SciBERT for all
the downstream tasks such as NER, abstract classification, and
relation classification on materials datasets. These results also
suggest that the scientific literature in the materials domain, on
which MatSciBERT is pre-trained, is significantly different from the
computer science and biomedical domains on which SciBERT is

trained. Specifically, each scientific discipline exhibits significant
variability in terms of ontology, vocabulary, and domain-specific
notations. Thus, the development of a domain-specific language
model, even within the scientific literature, can significantly
enhance the performance in downstream tasks related to text
mining and information extraction from literature






































