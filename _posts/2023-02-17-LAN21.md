---
title: "Rethinking Style Transformer with Energy-based Interpretation: Adversarial Unsupervised Style Transfer using a Pretrained Modelì •ë¦¬"
date:   2023-02-17
excerpt: "Rethinking Style Transformer with Energy-based Interpretation: Adversarial Unsupervised Style Transfer using a Pretrained Model paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# Abstract
<span style="background-color:#F5F5F5">**[í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œ]**</span>        
Style control, content preservation ë° fluencyëŠ” text style transfer modelsì˜ í’ˆì§ˆì„ ê²°ì •í•œë‹¤.      
nonparallel ë§ë­‰ì¹˜ì— ëŒ€í•´ trainí•˜ê¸° ìœ„í•´, ëª‡ ê°€ì§€ ê¸°ì¡´ ì ‘ê·¼ë²•ì€ **adversarial lossë¡œ  style discriminatorë¥¼ ì†ì´ëŠ” ê²ƒì„ ëª©í‘œ**ë¡œ í•œë‹¤.     
ê·¸ëŸ¬ë‚˜ <span style="background-color:#FFE6E6">adversarial trainingì€ ë‹¤ë¥¸ ë‘  metricsì— ë¹„í•´ **fluencyë¥¼ í¬ê²Œ ì €í•˜**</span>ì‹œí‚¨ë‹¤.     


<details>
<summary>ğŸ“œ Adversarial lossë€? </summary>
<div markdown="1">
  
Adversarial lossëŠ” Generatorë¡œ í•˜ì—¬ê¸ˆ ì§„ì§œì²˜ëŸ¼ ë³´ì¼ ì •ë„ë¡œ ì‚¬ì‹¤ì ì¸ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë„ë¡ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜

</div>
</details>


<details>
<summary>ğŸ“œ adversarial trainingë€? </summary>
<div markdown="1">
  
ì ëŒ€ì  í›ˆë ¨ Adversarial trainingì€ regularizationì˜ í•œ ë°©ë²•ì´ë‹¤.

ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ ë¶„ì•¼ì—ì„œ regularizationì€ overfittingì„ ë§‰ê³  ëª¨ë¸ì„ robustí•˜ê²Œ ë§Œë“¤ì–´ì£¼ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤.

ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ Input dataë¥¼ í•™ìŠµí•˜ì—¬ ì ì ˆí•œ labelì„ ë°˜í™˜í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í›ˆë ¨ì„ ì§„í–‰í•œë‹¤.

Adversarial attackì€ ë°˜ëŒ€ë¡œ ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ì— ëŒ€í•˜ì—¬ ì…ë ¥ ë°ì´í„°ë¥¼ ì˜ëª» ì˜ˆì¸¡í•˜ë„ë¡ Inputì„ ì¡°ì‘í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤ .

ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ inputìœ¼ë¡œ ë°›ëŠ” CNNì˜ ê²½ìš° ì •ë‹µ Yì—ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì˜ˆì¸¡ê°’Y'ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì¡°ê¸ˆì”© ì´ë¯¸ì§€ì˜ í”½ì…€ì„ ìˆ˜ì •í•œë‹¤.
  
</div>
</details>


<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ ëª©ì ]**</span>        
* ë³¸ ì—°êµ¬ì—ì„œëŠ” <span style="background-color:#fff5b1">energy-based interpretationì„ ì‚¬ìš©í•˜ì—¬ **ì´ í˜„ìƒì„ ì„¤ëª…**</span>í•˜ê³ ,        
<span style="background-color:#fff5b1">pretrained language modelì„ í™œìš©í•˜ì—¬ **fluencyì„ í–¥ìƒ**</span>ì‹œí‚¨ë‹¤.      
* êµ¬ì²´ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” <span style="background-color:#fff5b1">**discriminatorì™€ ëª¨ë¸ ìì²´ë¥¼ ì¬êµ¬ì„±**</span>í•˜ê³ ,    
<span style="background-color:#fff5b1">**pretrained language modelì„ text style transfer frameworkì— ì ìš©**</span>í•˜ì—¬ **generatorì™€  discriminatorê°€ pretrained modelì˜ í˜ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆ**í•œë‹¤.         


ë³¸ ë…¼ë¬¸ì€ GYAFC, Amazon ë° Yelpì˜ ì„¸ ê°€ì§€ ê³µê°œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•˜ê³  **ì „ì²´ ë©”íŠ¸ë¦­ì—ì„œ SOTA ë‹¬ì„±**í–ˆë‹¤.


---
---

# 1 Introduction

**Text style transfer**ì€ **ìŠ¤íƒ€ì¼ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” ì˜ë¯¸ë¡ ì„ ìœ ì§€**í•˜ë©´ì„œ, **í•œ ìŠ¤íƒ€ì¼ì—ì„œ ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ë¡œ ë¬¸ì¥ì„ ë³€í™˜í•˜ëŠ” ì‘ì—…**ì´ë‹¤.          

Text style transfer taskì„ í•´ê²°í•  ë•ŒëŠ” **ì„¸ ê°€ì§€ ê¸°ì¤€**ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.            
1) ***style control***, styleì´ ì›ë˜ ë¬¸ì¥ì—ì„œ ìƒì„±ëœ ë¬¸ì¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì˜ transferë˜ëŠ”ì§€,        
2) ***content preservation***,  ìƒì„±ëœ ë¬¸ì¥ì´ ì›ë³¸ì˜ ì˜ë¯¸ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ìœ ì§€í–ˆëŠ”ì§€,       
3) ***fluency***,ìƒì„±ëœ ë¬¸ì¥ì´ ì–¼ë§ˆë‚˜ ìì—°ìŠ¤ëŸ¬ìš´ì§€.      


**ë¬¸ì¥ì˜ styleì„ fluentlyí•˜ê²Œ ë³€í™˜**í•˜ëŠ” ê²ƒì€ ì¢…ì¢… **content preservationê³¼ ì¶©ëŒ**í•˜ê¸° ë•Œë¬¸ì— Text style transferì€ ì–´ë µë‹¤.          

ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´,   
ì—¬ëŸ¬ supervised text style transfer methodì´ ì‹œë„ë˜ì—ˆë‹¤.    
ê·¸ëŸ¬ë‚˜ **style-labeledì´ ìˆëŠ” ë¬¸ì¥ ìŒì„ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°**ê°€ ë§ê¸° ë•Œë¬¸ì— ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì´ ì‹¤ìš©ì ì´ì§€ ì•Šë‹¤.(ë°ì´í„°ê°€ ì ë‹¤.)        



<span style="background-color:#F5F5F5">**[ì„ í–‰ ì—°êµ¬ & í•œê³„]**</span>        
* autoencoder, back-translation ë° reinforcement learningì„ í¬í•¨í•œ ë‹¤ì–‘í•œ unsupervised text style transferì— ëŒ€í•œ ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ì´ ëŒ€ì¤‘í™”ë˜ì—ˆë‹¤.     
* ì´ì „ ì—°êµ¬ ì¤‘ Style TransformerëŠ” **adversarial training**ì„ í†µí•´ style discriminatorë¥¼ ì†ì„ìœ¼ë¡œì¨  **fine-grained style controlë¥¼ ë‹¬ì„±**í–ˆë‹¤.     
* ê·¸ëŸ¬ë‚˜, ê·¸ë“¤ì˜ ê°•ì ê³¼ëŠ” ë³„ê°œë¡œ, <span style="background-color:#fff5b1">style transformerë¥¼ í¬í•¨í•œ adversarial trainingì€ ìƒì„±ëœ ë¬¸ì¥ì˜ **fluencyì„ ì €í•˜**</span>ì‹œí‚¨ë‹¤.   


<span style="background-color:#F5F5F5">**[fluency ì €í•˜ì˜ ì´ìœ  ì¦ëª…: energy]**</span>     
* ë³¸ ë…¼ë¬¸ì—ì„œ, ìš°ë¦¬ëŠ” adversarial trainingì˜ fluency ì €í•˜ì˜ ì´ìœ ë¥¼ ì¡°ì‚¬í•˜ê¸° ìœ„í•´ style transformerë¥¼ ê²€í† í•œë‹¤.      
* ê·¸ ì „ì— **fluency ë¬´ì—‡ì¸ì§€ ë³´ë‹¤ ì •í™•í•˜ê²Œ í•´ì„**í•˜ê¸° ìœ„í•´ ë³€ìˆ˜ì˜ ì—”íŠ¸ë¡œí”¼ì¸ [*energy*ì˜ ê°œë…](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)ì„ ì†Œê°œí•œë‹¤.    
íŠ¹ì • styleì— ëŒ€í•œ ì…ë ¥ ë³€ìˆ˜ì˜ ì—ë„ˆì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ì—ë„ˆì§€ í•¨ìˆ˜ëŠ”,    
í•´ë‹¹ styleì—ì„œ **ì…ë ¥ì´ ê³µí†µì´ë©´ ë‚®ì€ ì—ë„ˆì§€ë¥¼ ì¶œë ¥**í•˜ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë†’ì€ ì—ë„ˆì§€ë¥¼ ì¶œë ¥í•œë‹¤.      
     * ì˜ˆë¥¼ ë“¤ì–´, formal/informal ë¬¸ì¥ì€ **formal ë§ë­‰ì¹˜**ì—ì„œ ì—ë„ˆì§€ê°€ ë‚®ë‹¤.(ì…ë ¥ì´ ê³µí†µì´ë‹ˆê¹Œ)    
     * ê·¸ëŸ°ë° formal(ì˜ˆ: ì •ì¹˜ì  í‘œí˜„)ê³¼ ë¬´ê´€í•œ ë¬¸ì¥ì€ ë§ë­‰ì¹˜ì—ì„œ ì—ë„ˆì§€ê°€ ë†’ì„ ê²ƒì´ë‹¤.     
* ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” ìœ ì°½í•œ ë¬¸ì¥ì´ ë§ë­‰ì¹˜ì˜ style ì¤‘ í•˜ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” **íŠ¹ì • ë§ë­‰ì¹˜**ì—ì„œ **ë‚®ì€ ì—ë„ˆì§€ë¥¼ ê°–ëŠ” ê²ƒìœ¼ë¡œ ìœ ì°½ì„±ì„ ì •ì˜**í•œë‹¤.     
* [Figure 1]ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´, adversarial trainingì€ source styleì— ëŒ€í•œ ì—ë„ˆì§€ë¥¼ ìµœëŒ€í™”í•˜ê³  ìƒì„±ëœ ë¬¸ì¥ì„ ë§ë­‰ì¹˜ì˜ ë¶„í¬ì—ì„œ ë©€ë¦¬ ëª°ì•„ë‚´ê¸° ë•Œë¬¸ì—, deceiving the discriminatorë¥¼ í•˜ëŠ” ë™ì•ˆ fluencyê°€ ì €í•˜ëœë‹¤.    
* ìœ ì°½ì„± ì €í•˜ì— ëŒ€ì‘í•˜ê¸° ìœ„í•´ ì–¸ì–´ ëª¨ë¸(LM)ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë¬¸ì¥ì„ ë§ë­‰ì¹˜ì˜ ë¶„í¬ì— ìœ ì§€í•˜ëŠ” ì •ê·œí™”ê¸°ë¥¼ ë„ì…í•œë‹¤.
â¡ **ë³¸ ë…¼ë¬¸ì˜ í•´ê²°**: ì´ <span style="background-color:#fff5b1">LM-based regularizerëŠ” ë¬¸ì¥ì„  target corpusë¡œ ëŒì–´ë‚´ì–´ ìƒì„±ëœ ë¬¸ì¥ì„ ë§ë­‰ì¹˜ì— ìœ ì§€</span>í•œë‹¤.

**[Figure 1]** fluency ì €í•˜ì— ëŒ€í•œ *energy* ê¸°ë°˜ í•´ì„
![image](https://user-images.githubusercontent.com/76824611/224559727-77c00525-9907-44c8-89f7-4027ccac0a2f.png)
* energy-based discriminator Dë¥¼ ì†ì´ë ¤ë©´,        
   * 1) ì „ë‹¬ëœ ë¬¸ì¥ $$ğ‘¥ Ì‚^â€²$$ì™€ target ìŠ¤íƒ€ì¼ $$sâ€²$$ ì‚¬ì´ì˜ ì—ë„ˆì§€ Eë¥¼ ìµœì†Œí™”í•˜ê³ ,   
   * 2) ë¬¸ì¥ $$ğ‘¥ Ì‚^â€²$$ì™€ **ì›ë˜ ìŠ¤íƒ€ì¼ ì‚¬ì´ì˜ ì—ë„ˆì§€ë¥¼ ìµœëŒ€í™”**í•´ì•¼ í•œë‹¤.     
   * ê·¸ëŸ¬ë‚˜ style $$s$$ê³¼ $$sâ€²$$ëŠ” **ë™ì¼í•œ ë§ë­‰ì¹˜ì—ì„œ ê¸°ì›**í•˜ë¯€ë¡œ $$E(ğ‘¥ Ì‚^â€², s)$$ë¥¼ ìµœëŒ€í™”í•˜ë©´ ì „ì²´ fluencyê°€ ì €í•˜ëœë‹¤. ê·¸ê²ƒì€ Eq.6ì˜ í•´ì„ì´ë‹¤.     



To apply the LM-based regularizers, we can
leverage pretrained models such as GPT-2 to generate fluent sentences. Moreover, fluency is expected to further improve when the generator and
the discriminator are also replaced with a pretrained
model. However, as shown in Figure 2, the generator, discriminator, and LM must share the same
vocabulary and tokenizer in order to propagate gradients successfully. Thus, inefficiency can arise,
in that two of the three modules may need to be
re-trained from scratch because the existing pretrained models are based on different tokenizers.
To address this issue, we restructured the discriminator and LM such that a single pretrained model is
applied to all three modules: the generator, discriminator, and LM. By using a single pretrained model,
our method not only solves the inconsistent vocabulary problem but also has advantages when applying further pretraining for domain adaptation (Gururangan et al., 2020) or additional dataset (Lai
et al., 2021a). This is because additional pretraining is necessary when only a single model is used
to improve style transfer performance.
Our contributions can be summarized as follows:


<span style="background-color:#F5F5F5">**[LM-based regularizers ì ìš©]**</span>     
* LM-based regularizersë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ GPT-2ì™€ ê°™ì€ pretrained models ëª¨ë¸ì„ í™œìš©í•˜ì—¬ fluent ë¬¸ì¥ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤.      
* generator ì™€ discriminatorë„ pretrained modelsë¡œ êµì²´ë˜ë©´ fluent ë”ìš± í–¥ìƒë  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ë¦¼ 2ì— í‘œì‹œëœ ê²ƒì²˜ëŸ¼, ê·¸ë ˆì´ë””ì–¸íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì „íŒŒí•˜ê¸° ìœ„í•´ì„œëŠ” ìƒì„±ê¸°, íŒë³„ê¸° ë° LMì´ ë™ì¼í•œ ì–´íœ˜ì™€ í† í°í™”ê¸°ë¥¼ ê³µìœ í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ, ê¸°ì¡´ì˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì´ ì„œë¡œ ë‹¤ë¥¸ í† í°í™”ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê¸° ë•Œë¬¸ì— ì„¸ ê°œì˜ ëª¨ë“ˆ ì¤‘ ë‘ ê°œë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ í›ˆë ¨í•´ì•¼ í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ë¹„íš¨ìœ¨ì„±ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì´ ë°œìƒê¸°, íŒë³„ê¸° ë° LMì˜ ì„¸ ëª¨ë“ˆì— ëª¨ë‘ ì ìš©ë˜ë„ë¡ íŒë³„ê¸°ì™€ LMì„ ì¬êµ¬ì„±í–ˆë‹¤. ì‚¬ì „ í›ˆë ¨ëœ ë‹¨ì¼ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, ìš°ë¦¬ì˜ ë°©ë²•ì€ ì¼ê´€ì„± ì—†ëŠ” ì–´íœ˜ ë¬¸ì œë¥¼ í•´ê²°í•  ë¿ë§Œ ì•„ë‹ˆë¼ ë„ë©”ì¸ ì ì‘ì„ ìœ„í•œ ì¶”ê°€ ì‚¬ì „ í›ˆë ¨(Gururangan et al., 2020) ë˜ëŠ” ì¶”ê°€ ë°ì´í„° ì„¸íŠ¸(Lai et al., 2021a)ë¥¼ ì ìš©í•  ë•Œ ì´ì ì´ ìˆë‹¤. ìŠ¤íƒ€ì¼ ì „ì†¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë‹¨ì¼ ëª¨ë¸ë§Œ ì‚¬ìš©í•  ê²½ìš° ì¶”ê°€ì ì¸ ì‚¬ì „ êµìœ¡ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤. NATì˜ ê¸°ì—¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìš”ì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:



â€¢ ìš°ë¦¬ëŠ” ì—ë„ˆì§€ ê¸°ë°˜ í•´ì„ìœ¼ë¡œ ì ëŒ€ì  í›ˆë ¨ì˜ ìœ ì°½ì„± ì €í•˜ë¥¼ ë¶„ì„í•˜ê³  ìœ ì°½ì„± ì €í•˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•˜ëŠ” ì •ê·œí™”ê¸°ë¥¼ ì œì•ˆí•œë‹¤.

â€¢ ìš°ë¦¬ëŠ” í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ ì „ì†¡ í”„ë ˆì„ì›Œí¬ì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ ë‹¨ì¼ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ íŒë³„ê¸°ì™€ ì–¸ì–´ ëª¨ë¸ì„ ì¬êµ¬ì„±í•œë‹¤.

â€¢ ìš°ë¦¬ëŠ” GYAFC, Amazon ë° Yelp ë°ì´í„° ì„¸íŠ¸ì—ì„œ ìƒˆë¡œìš´ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ê³  ëª¨ë¸ì˜ ê° êµ¬ì„± ìš”ì†Œì˜ ê¸°ì—¬ë„ë¥¼ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•œë‹¤.


â€¢ We analyze the fluency degradation in adversarial training with an energy-based interpretation, and propose a regularizer leveraging a
language model to prevent fluency degradation.


â€¢ We reconstruct the discriminator and language
model such that the single pretrained language
model can be employed in the text style transfer framework.


â€¢ We achieve new state-of-the-art results on
GYAFC, Amazon, and Yelp datasets and carefully analyze the contribution of each component of our model.


---
---

# 2 Related Work

## 2.1 Unsupervised style transfer
Many of the previous studies have attempted to
learn disentangled representations of text by separating representations of content and style in a latent space. For instance, Shen et al. (2017) trained
a cross-aligned autoencoder to learn a shared latent space for contents, while learning a separate
representation for styles using adversarial learning. Yang et al. (2018) further extended this crossaligned approach by leveraging LM as a discriminator to enhance the informativeness and stability
of adversarial training. Yi et al. (2020) leveraged
multiple instances of the same style to model the
latent space of underlying stylistic characteristics,
and samples extracted from this space were fed into
the decoder to balance with contents. These works
using disentangled representations exhibited reasonable performance with high interpretability, but
disentangled content representations can still contain style-relevant information, as pointed out by
Lample et al. (2019). In addition, there is a limitation in that the meaning of the input sentence must
be expressed in a fixed-size vector with a limited
capacity (Dai et al., 2019).


In contrast, there are methods without disentangled representations that do not explicitly disentan
gle the content and style of text using reinforcement learning (Xu et al., 2018; Luo et al., 2019)
and back-translation (Lample et al., 2019; Prabhumoye et al., 2018). Dai et al. (2019) proposed a
novel style transfer model based on the transformer
architecture without disentangled representations,
and Wang et al. (2019) also utilized a transformer
for an unsupervised framework by editing entangled latent representations. These models are novel
in their model architecture or training strategy, but
they do not utilize pretraining models, which results in lower performance than the state-of-the-art
methods. On the other hand, our work proposes a
novel approach to effectively leverage a pretrained
LM in an unsupervised text style transfer task.



## 2.2 Style transfer with pretrained models
Recently, pretrained models have achieved great
success on various NLP tasks such as machine
translation (Chronopoulou et al., 2020; CONNEAU
and Lample, 2019) and text summarization (Liu
and Lapata, 2019). The pretrained models are also
being used for text style transfer tasks. Sudhakar
et al. (2019) used two variants of â€˜decoder-onlyâ€™
transformer to generate sentences in a target style
and leveraged the power of GPT (Radford et al.,
2018). Malmi et al. (2020) used a padded masked
language model (Mallinson et al., 2020) variant,
whose architecture and the trained corpus were
identical to those of BERT (Devlin et al., 2019).
Although these studies exploited the power of pretrained models, our approach differs in that we train
our model adversarially in an end-to-end manner.


In addition, several works have focused on style
transfer in a specific domain, or for leveraging an
additional corpus. To transfer writing styles between authors, Syed et al. (2020) pretrained LM on
the author corpus from scratch using masked language modeling. Laugier et al. (2021) detoxified
toxic texts by fine tuning a pretrained T5 (Raffel et al., 2020) using additional denoising and
cycle-consistency objectives. However, these studies only focused on a specific domain. Lai et al.
(2021a) built a pseudo-parallel dataset by leveraging generic resources including WordNet (Baccianella et al., 2010) and Parabank (Hu et al., 2019)
to fine tune BART on style transfer tasks. Lai et al.
(2021b) fine tuned BART (Lewis et al., 2020) using
parallel data with a policy gradient (Sutton et al.,
1999) which maximized the style classifier reward
and the BLEU score reward. Our work incorporates
pretrained models with adversarial training on various domains, while trained only on a non-parallel
corpus.


## Energy-based model
The conventional probabilistic model outputs the
normalized probability p(x) for input variable x.
In contrast, the energy-based model outputs the
non-normalized scalar value E(x) denoted as energy (Hinton, 2002; Lecun et al., 2006) Using the
energy-based model, we can classify x by comparing the energy of each label, or generate x by
optimizing arg minx E(x).


Several works have leveraged the energy-based
model for image generation (Ngiam et al., 2011;
Zhao et al., 2017), text generation (Deng et al.,
2020; Bakhtin et al., 2021), and reinforcement
learning (Haarnoja et al., 2017). We borrow the
main idea of the energy-based model, which expresses the classifier in the form of an energy function. In the work of Che et al. (2020), the authors
interpreted the GAN discriminator (Goodfellow
et al., 2014) using the energy-based model, but
we apply this interpretation to the style transfer
task. We show that Style Transformer can be interpreted as an energy-based model by decomposing
the discriminator, and provide the reason why fluency degradation occurs when we try to deceive
the style discriminator.

---
---


5 Limitations
As our model uses adversarial learning, training is
somewhat unstable, like GAN (Goodfellow et al.,
2014; Arjovsky et al., 2017). For this reason, continuing training does not guarantee good results,
and the text style transfer performance fluctuates.
Although we have paid attention to model selection to compensate for the unstable training, the
instability of adversarial learning remains an issue.
In addition, we have only conducted experiments
on widely used datasets, to compare our work with
previous studies. These datasets are composed of
binary style classes, such as positive and negative
sentiments. Therefore, conducting experiments
using multi-class datasets (Lample et al., 2019)
should be considered.
6 Ethical consideration
Our model may generate negative and rude expressions about a specific person or a commercial site
because of the data distribution of the Yelp, Amazon, and GYAFC datasets. However, we propose
our work in anticipation of positive applicability as
shown in previous studies.
7 Conclusion
Using energy-based interpretation, we found that
fluency is inevitably degraded when deceiving
the discriminator in Style Transformer (Dai et al.,
2019). The problem is solved by adding an LMbased regularizer and training the pretrained generator, discriminator and LM together. Our model
shows comparable performance in text style transfer and content preservation while preserving fluency, and we demonstrated the robustness of our
model by conducting extensive experiments on various styles in raw text.




