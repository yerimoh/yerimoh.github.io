---
title: "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP ì •ë¦¬"
date:   2022-07-12
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## í•µì‹¬  

## ì½ê¸° ìœ„í•´ í•„ìš”í•œ ì§€ì‹
* [word2vec](https://yerimoh.github.io/DL14/): baseline ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ê¼­ ì•Œì•„ì•¼ í•œë‹¤.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec ì†ë„ ê°œì„ ìœ¼ë¡œ ì´ í¬ìŠ¤íŒ…ë„ ê¼­ ì•Œì•„ì•¼ í•œë‹¤.      

## ì› ë…¼ë¬¸
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)


---

# ëª©ì°¨  


---


# **Abstract**
ìš°ë¦¬ê°€ ëª¨ë¸ë§í•˜ê³ ì í•˜ëŠ” í…ìŠ¤íŠ¸ì˜ ë‹¨ìœ„ëŠ”  
bytesì—ì„œ ë‹¤ì¤‘ ë‹¨ì–´ í‘œí˜„ì‹(multi-word expressions)ì— ì´ë¥´ê¸°ê¹Œì§€, í…ìŠ¤íŠ¸ëŠ” ë‹¤ì–‘í•œ ì„¸ë¶„ì„±ìœ¼ë¡œ ë¶„ì„ë˜ê³  ìƒì„±ë  ìˆ˜ ìˆë‹¤.      

ìµœê·¼ê¹Œì§€ ëŒ€ë¶€ë¶„ì˜ ìì—°ì–´ ì²˜ë¦¬(NLP) ëª¨ë¸ì€ ë‹¨ì–´ë¥¼ í†µí•´ ì‘ë™í•˜ì—¬ discrete ë°  atomic tokensìœ¼ë¡œ ì²˜ë¦¬í–ˆì§€ë§Œ,   
byte-pair encoding(BPE)ì„ ì‹œì‘ìœ¼ë¡œ ë§ì€ ì˜ì—­ì—ì„œ í•˜ìœ„ ë‹¨ì–´ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì´ ìš°ì„¸í•˜ì—¬ **ì‘ì€ ì–´íœ˜ë¥¼ ì‚¬ìš©**í•˜ë©´ì„œë„ **ë¹ ë¥¸ ì¶”ë¡ **ì„ í—ˆìš©í•˜ê³  ìˆë‹¤.      

ì´ í† í¬ë‚˜ì´ì§• ë°©ë²•ë“¤ì— ëŒ€í•´ í•™ìŠµëœ ì„¸ë¶„í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ **subword-based approaches**ë¿ë§Œ ì•„ë‹ˆë¼ **ë‹¨ì–´ì™€ ë¬¸ìì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ë°©ì‹**ì´ **ì–´ë–»ê²Œ ì œì•ˆë˜ê³  í‰ê°€**ë˜ì—ˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤Œìœ¼ë¡œì¨,     
ì‚¬ì „ ì‹ ê²½ ë° ì‹ ê²½ ì‹œëŒ€ì˜ ì—¬ëŸ¬ ì‘ì—… ë¼ì¸ì„ ì—°ê²°í•œë‹¤.     

ë³¸ ë…¼ë¬¸ì€ ëª¨ë“  ì• í”Œë¦¬ì¼€ì´ì…˜ì— ëŒ€í•´ **íŠ¹ë³„í•œ í•´ê²°ì±…**ì´ ê²°ì½” **ì—†ì„ ê²ƒ**ì´ë©°,   
í† í°í™” ë°©ë²•ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì—¬ì „íˆ ì¤‘ìš”í•˜ë‹¤ê³  ê²°ë¡ ì§“ëŠ”ë‹¤.    


-----
-----

# **1 Introduction**    


_â€œâ€˜tokensâ€™ are not a real thing. they are a computer generated illusion created by a clever engineerâ€ dril_gpt1         


ìš°ë¦¬ê°€ ì‚¬ëŒë“¤ì—ê²Œ NLP ëª¨ë¸ì„ ì²˜ìŒ ì†Œê°œí•  ë•Œ,    
ìš°ë¦¬ëŠ” ì¢…ì¢… í…ìŠ¤íŠ¸ê°€ ì»´í“¨í„°ì— ì‘ì€ ì¡°ê°ë“¤ë¡œ ì˜ë ¤ë‚˜ê°„ë‹¤ëŠ” ìƒê°ì„ ë‹¹ì—°í•˜ê²Œ ë°›ì•„ë“¤ì¸ë‹¤.    
â¡ ê²°êµ­ ê·¸ê²ƒì€ ë‹¨ì§€ ì¼ë ¨ì˜ ì •ìˆ˜ì¼ ë¿ì´ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì´ê²ƒë“¤ì„ (ë³´í†µ) ì—°ì† **sub-strings tokens**ì´ë¼ê³  ë¶€ë¥¸ë‹¤. 


êµìœ¡ í™˜ê²½ì—ì„œ, ê·¸ë¦¬ê³  ì‚¬ì‹¤ ì—­ì‚¬ì ìœ¼ë¡œ NLPì—ì„œ ì´ëŸ¬í•œ í† í°ì€ ë‹¤ì†Œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¨ì–´(ì²˜ìŒì—ëŠ” ì˜ì–´ë¡œ â€œspace-separated substringsâ€)ë¡œ ì•”ì‹œëœë‹¤. 


**[ë‹¨ì–´ì—ì„œ êµ¬ë‘ì  ë¶„ë¦¬ì˜ ì–´ë ¤ì›€]**            
* "don't"ë¼ëŠ” ë‹¨ì–´ë¥¼ ì˜ˆë¡œ ë“¤ë©´,    
* ì´ ë‹¨ì–´ì˜ êµ¬ë‘ì  ë¶„í• ì˜ ê²½ìš°, ```"don"``` ```"'"``` ```"t"```ë¼ëŠ” ë‹¤ì†Œ ë¬´ì˜ë¯¸í•œ ì„¸ ê°œì˜ í† í°ì„ ì–»ê²Œ ë  ê²ƒì´ë‹¤.            
* ê·¸ëŸ°ë° ë” íš¨ê³¼ì ì¸ í† í¬ë‚˜ì´ì§•ì„ í•˜ë ¤ë©´ ```"do"```ì™€ ```"n't"```ì˜ ë‘ ë‹¨ìœ„ë¥¼ ì‚°ì¶œí•´ì•¼ í•œë‹¤ê³  ì£¼ì¥í•  ìˆ˜ ìˆë‹¤.       

---

## ë…¼ë¬¸ ê°œìš”    
ì´ surveyëŠ” $$tokenization$$ì— ëŒ€í•œ ìœ„ì™€ ê°™ì€ ì§ˆë¬¸ì„ ë‹¤ë£¨ë©°,      
ë³¸ ë…¼ë¬¸ì€ ì˜ ê° ì±•í„°ì˜ í•µì‹¬ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ë‹¤,    

**[2: Tokens, word forms, and sub words](# 2-Tokens,-word-forms,-and-sub-words)**      
* ê·¼ë³¸ì ì¸ ì§ˆë¬¸ê³¼ ìš©ì–´ë¥¼ ìƒì„¸íˆ ì„¤ëª…     

**[3: Pre-tokenization yields word-like typographic units](# 3-Pre-tokenization-yields-word-like-typographic-units)**      
* ëª¨ë“  NLP ì‘ì—…ì—ì„œ ë‹¤ì†Œ ë§¤ë ¥ì ì´ì§€ ì•Šì€ ë¶€ë¶„ì´ ì—­ì‚¬ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë‹¤ë£¨ì–´ì¡ŒëŠ”ì§€ë¥¼ ë³´ì—¬ì¤Œ     

íŠ¹íˆ ì§€ë‚œ 5ë…„ ë™ì•ˆ, ë‹¤ì†Œ ì›ìì ì¸ ë‹¨ì–´ì™€ ê°™ì€ ê³µê°„ ë¶„ë¦¬ ë‹¨ìœ„ë¡œì„œì˜ **"í† í°"ì— ëŒ€í•œ ì§ê´€ì ì¸ ì •ì˜ë¥¼ ë„˜ì–´ì„œëŠ” ê²ƒ**ì— ëŒ€í•œ ê´€ì‹¬ì´ ìƒˆë¡œì›Œì¡Œë‹¤.  
* **4**: ì´ë¥¼ ìœ„í•œ í•œ ê°€ì§€ ë°©ë²•ì¸ **ë‹¨ì–´ ë‚´ë¶€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ì™€ ê°™ì€ ë‹¨ìœ„ë¥¼ ë³´ê°•**í•˜ëŠ” ë°©ë²•.    
* **5**: ì‹ ê²½ ëª¨ë¸ë§ì€ ì´ë¥¼ ê·¸ ì–´ëŠ ë•Œë³´ë‹¤ ì‰½ê²Œ ë§Œë“¤ì–´ ëª…ë°±í•œ í‘œì‹œ ì—†ì´ ë‹¨ì–´ ê²½ê³„ê¹Œì§€ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ë¡œ ì´ì–´ì§„ë‹¤(ì˜ˆ: ê³µë°±ì´ ìˆëŠ” ê²½ìš°).  unsupervised word segmentation or discoveryì˜ ê°œë…ì€ ìˆ˜ì‹­ ë…„ì˜ ì‘ì—…ì—ì„œ ìì²´ì ìœ¼ë¡œ ì¡´ì¬í–ˆì§€ë§Œ,    
* **6**: **subword** ë‹¨ìœ„ë¥¼ ì›ì í† í°ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” í˜„ì¬ ë„ë¦¬ í¼ì ¸ ìˆëŠ” ì•„ì´ë””ì–´ë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì‚´í´ë³¼ ë•Œ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ìœ ìš©í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œê²Œ ë  ê²ƒì´ë‹¤.      
* **7**: ë‹¤êµ­ì–´ ì–´íœ˜ì˜ ê³µìœ ì™€ ê²½ìŸì˜ ëª‡ ê°€ì§€ ë¬¸ì œ ì‚´í´ë´„     
* **8**: ë¬¸ì, ë°”ì´íŠ¸ ë˜ëŠ” ì‹¬ì§€ì–´ í”½ì…€ë¡œ ìµœëŒ€ ë¶„í•´í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ê°„ë‹¨í•œ í† í°í™”ë¥¼ ì‚¬ìš©í•´ë´„        


ì´ ë…¼ë¬¸ì´ ëë‚œ í›„,       
* ë°”ì´íŠ¸ì˜ ì†ì‰¬ìš´ ì†”ë£¨ì…˜ì´ ë„ë‹¬í•´ ë³´ì´ëŠ” 2021ë…„ í˜„ì¬ì—ë„ "ë³µì¡í•œ" í† í°í™”ê°€ ì˜ ì¤‘ìš”ì„±ì„ ë…¼ì¦í•˜ì—¬ ë§ˆë¬´ë¦¬ í•œë‹¤.     
* ìµœê·¼ì˜ ë°œì „ì€ íŠ¹ì • ë„ë©”ì¸ê³¼ ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•´ ìµœëŒ€ ë¶„í•´ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì§€ë§Œ, ê·¸ê²ƒë“¤ì€ ê´‘ë²”ìœ„í•œ NLP ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‹¤ë£¨ì§€ ì•Šê³  **ìì²´ì˜ ë‹¨ì ê³¼ í¸ê²¬**ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ì£¼ì¥í•  ê²ƒì´ë‹¤.     


![image](https://user-images.githubusercontent.com/76824611/178999180-03d114b4-8ba7-4748-afce-5115faccc2b5.png)
A taxonomy of segmentation and tokenization algorithms and research directions


------
-----











# **2 Tokens, word-forms, and sub-words** 
ì´ ì±•í„°ì—ì„œëŠ” tokenì— ëŒ€í•œ ê·¼ë³¸ì ì¸ ì§ˆë¬¸ê³¼ ìš©ì–´ë¥¼ ì„¤ëª…í•œë‹¤.      

í† í°, ë‹¨ì–´ í˜•íƒœ ë° í•˜ìœ„ ë‹¨ì–´ NLPì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” 
"ë¬¸ì¥"(sentences)ê³¼ "ë‹¨ì–´(words)"ë¡œ ë¶„í• ë˜ì—ˆë‹¤. 

**[sentences]**     
* ê±°ì‹œì  ë‹¨ìœ„(macroscopic units)ëŠ” ì¢…ì¢… ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ê³ ë ¤ë˜ë©°, ê·¸ ìì²´ê°€ ë¯¸ì‹œì  ë‹¨ìœ„ë¡œ ì„¸ë¶„ëœë‹¤.        




**[tokens]**    
* ì¼ë°˜ì ìœ¼ë¡œ í† í°ì´ë¼ê³  ë¶ˆë¦¬ëŠ” íƒ€ì´í¬ê·¸ë˜í”¼ ë‹¨ìœ„(typographic units)ëŠ” ì–¸ì–´ì ìœ¼ë¡œ ë™ê¸°í™”ëœ ë‹¨ìœ„ì— ëŒ€í•œ ê·¼ì‚¬ì¹˜ë¡œ ì‚¬ìš©ë˜ì–´ ì™”ë‹¤.     
* MAF(í˜•íƒœí•™ì  ì£¼ì„ í”„ë ˆì„ì›Œí¬)ëŠ” í† í°ì„ "â€œnon-empty contiguous sequence of graphemes or phonemes in a document."ìœ¼ë¡œ ì •ì˜í•œë‹¤.    
* ì˜ˆë¥¼ ë“¤ì–´, í˜„ì¬ ë¼í‹´ì–´ ìŠ¤í¬ë¦½íŠ¸ì™€ í•¨ê»˜ ë³´í¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” **ê³µë°±**ê³¼ ê°™ì€ **íƒ€ì´í¬ê·¸ë˜í”¼ êµ¬ë¶„ì**ë¥¼ ì‚¬ìš©í•˜ëŠ” ì“°ê¸° ì‹œìŠ¤í…œì˜ ê²½ìš°, í† í°ì€ ë„ë¦¬ ì‚¬ìš©ë˜ì—ˆìœ¼ë©° **ë¹„ë¬¸êµ¬ ë¹„ë°±ìƒ‰ ê³µê°„ ë§ˆí¬** ë˜ëŠ” **êµ¬ë‘ì ì˜ ì—°ì† ì‹œí€€ìŠ¤**ë¡œ ì •ì˜ë˜ì—ˆë‹¤.    
* íŠ¹ì • ë¬¸ì¥ ë¶€í˜¸(ì˜ˆ: í•˜ì´í”ˆ ë˜ëŠ” ì•„í¬ìŠ¤íŠ¸ë¡œí”¼)ë¥¼ í† í°ìœ¼ë¡œ ì„ì˜ë¡œ ì •í•˜ë©´, ê·¸ëŸ¬í•œ ì •ì˜ëŠ” **ë¬¸ì¥**ì„ í† í°ì„ ê¸°ì¤€ìœ¼ë¡œ **ì›ì ë‹¨ìœ„ë¡œ ë¶„í• **í•œë‹¤.     
* í† í°ê³¼ ë‹¨ì–´ í˜•íƒœ ì‚¬ì´ì— **ì¼ëŒ€ì¼ ëŒ€ì‘ì´ ì—†ë‹¤**("ë©€í‹°í†¡ ë‹¨ì–´", "ë©€í‹°ì›Œë“œ í† í°")    
    * ë‹¨ì–´ í˜•íƒœëŠ” ì—¬ëŸ¬ ê°œì˜ í† í°(ì˜ˆ:French or English sine die)ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.     
    * ë°˜ë©´, ì—¬ëŸ¬ ê°œì˜ ë‹¨ì–´ í˜•íƒœëŠ” ë™ì¼í•œ í† í°(ì˜ˆ: ì˜ì–´ don't = do + not, ìŠ¤í˜ì¸ì–´ damÃ©lo + da + lo)ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.     

**[tokensì˜ ì¬ì •ì˜ì˜ í•„ìš”ì„±: word-formsì„ ìœ„í•´]**      
* ìµœê·¼ ë¬¸ì¥ì´ ì›ì ë‹¨ìœ„ë¡œ ë¶„í• ë˜ëŠ” ë°©ì‹ì´ ì§„í™”í•˜ì—¬ **í† í°í™” ê°œë…ì´ ì¬ì •ì˜ë˜**ì—ˆë‹¤.    
* **í•„ìš”ì„±:** ê³¼í•™ì  ê²°ê³¼(ì˜ˆ: í•˜ìœ„ ë‹¨ì–´ ë¶„í• ì´ ê¸°ê³„ ë²ˆì—­ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì™€ ê¸°ìˆ ì  ìš”êµ¬ ì‚¬í•­(ì˜ˆ: ê³ ì • í¬ê¸° ì–´íœ˜ê°€ í•„ìš”í•œ BERTì™€ ê°™ì€ ì–¸ì–´ ëª¨ë¸) ëª¨ë‘ì— ê¸°ì´ˆí•˜ì—¬, ì›ì ì²˜ë¦¬ ì¥ì¹˜(ì•„ì§ í† í°ì´ë¼ê³  í•¨)ê°€ **ë‹¨ì–´ í˜•íƒœ(word-forms)ì˜ ê·¼ì‚¬ì¹˜ê°€ ë  í•„ìš”**ê°€ ìˆë‹¤.    
* âŒ **ë¬¸ì œ**: í‡´ìƒ‰í•œ ê²°ê³¼ì ìœ¼ë¡œ, í˜„ì¬ì˜ NLPì—ì„œ í† í°ì˜ ê°œë…ì€ ì—¬ì „íˆ MAF ì •ì˜ì™€ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ì§€ë§Œ, ë” ì´ìƒ **íƒ€ì´í¬ê·¸ë˜í”¼ ë‹¨ìœ„ì˜ ì „í†µì ì¸ ì •ì˜ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤**.     


**[í† í°í™”(Tokenization)]**     
* **ì •ì˜:** ë¬¸ì¥ì„ ì •í˜•ì ì´ì§€ ì•Šì€ (ê·¸ë¦¬ê³  ì‹¤ì œë¡œ ì–¸ì–´ì ì´ì§€ ì•Šì€) ë™ê¸°í™”ëœ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ì‘ì—…     
* ì´ëŠ” ì¢…ì¢… ê³ ì „ì ì¸ í† í°ê³¼ ë‹¨ì–´ í˜•íƒœë³´ë‹¤ ì‘ê¸° ë•Œë¬¸ì— ì¢…ì¢… **subword**ë¼ê³  ë¶ˆë¦°ë‹¤.     
* Typographic ë‹¨ìœ„("ì˜¤ë˜ëœ" í† í°)ëŠ” í˜„ì¬ ì¢…ì¢… "pre-tokens"ë¼ê³  ë¶ˆë¦°ë‹¤.      
â¡ ë”°ë¼ì„œ "tokenization"ë¼ê³  ë¶ˆë¦¬ë˜ ê²ƒì€ ì˜¤ëŠ˜ë‚  "pretokenization"ë¼ê³  ë¶ˆë¦°ë‹¤.      
* **pretokenization**: ì´ ìš©ì–´ëŠ” í† í°í™”ì˜ ìƒˆë¡œìš´ ê°œë…ì— ëŒ€í•œ ì²« ë²ˆì§¸ ì ‘ê·¼ë²•ì´ **resulting ë‹¨ìœ„**(ì´ì „ì—ëŠ” â€œtokensâ€, ì§€ê¸ˆì€  â€œpretokensâ€)ë¥¼ â€œsub-wordsâ€ë¡œ **ë¶„í• í•˜ê¸° ì „**ì— **ë¬¸ì¥ì„ ì ì ˆí•œ Typographic ë‹¨ìœ„**(ì˜ˆ: í† í°í™”ì˜ "ì˜›" ê°œë…)**ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì„ í¬í•¨**í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì— ì˜í•´ ë™ê¸° ë¶€ì—¬ëœë‹¤.     


-----
------


# **3 Pre-tokenization yields word-like typographic units**    
ëª¨ë“  NLP ì‘ì—…ì—ì„œ ë‹¤ì†Œ ë§¤ë ¥ì ì´ì§€ ì•Šì€ ë¶€ë¶„ì´ ì—­ì‚¬ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë‹¤ë£¨ì–´ì¡ŒëŠ”ì§€ë¥¼ ë³´ì—¬ì¤Œ

**[ì˜› í† í°ì˜ ì‚¬ìš©ë°©ì‹]**       
* purely typographic tokenì˜ ì–¸ì–´ì  ë¬´ê´€í•¨(linguistic irrelevance)     
* í…ìŠ¤íŠ¸ë¥¼ linguistically motivatedëœ ë‹¨ì–´ í˜•íƒœë¡œ ìë™ ë¶„í• í•˜ëŠ” ê²ƒì˜ ì–´ë ¤ì›€(difficulty of automatically splitting a text into linguistically motivated word-forms)     
* **ìœ„ ë‘ ë¬¸ì œì˜ ì ˆì¶©ì•ˆ:**  purely typographic tokenê³¼ purely linguistic word-formsì˜ ì¤‘ê°„ ë‹¨ìœ„ê°€ ë„ë¦¬ ì‚¬ìš©ë˜ì–´ ì™”ë‹¤.   

**[â€œpre-tokenizersâ€ì˜ ì—­ì‚¬]**         
* **ìš©ì–´ ë³€í™”**     
   * ì´ˆê¸°ì—ëŠ” â€œtokenizersâ€ë¡œ ì•Œë ¤ì§     
   * sub-word í† í°í™”ì˜ í™•ì‚° ì´í›„ â€œpretokenâ€ìœ¼ë¡œ ë°”ë€œ    
   * ì˜¤ëŠ˜ë‚ ì—ëŠ” â€œpre-tokenizersâ€ë¡œ ëª…ì¹­ ë°”ë€œ         
* **ëŒ€í‘œ ëª¨ë¸**      
   * ì´ëŸ¬í•œ â€œpre-tokenizersâ€ ì¤‘ ì¼ë¶€ëŠ” ë¹„êµì  ë‹¨ìˆœí•˜ê³  typographic tokenì— ì¶©ì‹¤í•˜ë‹¤.      
   * Hugging Faceì˜ í† í°ë¼ì´ì € íŒ¨í‚¤ì§€ì— ìˆëŠ” ì˜¤ë˜ëœ ```Moses (Kohn et al., 2007) tokenizer6``` ê°€ ì´ˆê¸°ì— ë„ë¦¬ ì‚¬ìš©ë¨           
   * ê·¸ë¦¬ê³  ë” ìµœê·¼ì€ Hugging Faceì˜ Tokenizers packageì— ìˆëŠ”```pre-tokenizers package```ê°€ ìˆë‹¤.      
 
**[â€œpre-tokenizationâ€ ê°œë…ì˜ì˜ íƒ„ìƒê³¼ ë¬¸ì œì ]**     
* ìœ„ì˜ ëŒ€í‘œ ëª¨ë¸ë“¤ì˜ ì‚¬ìš©ì€ "tokenization(í˜„ì¬ëŠ” â€œpre-tokenizationâ€)"ë¼ëŠ” ë‹¨ì–´ë¥¼ ë§Œë“¤ì–´ëƒˆë‹¤.     
* **"tokenization(pre-tokenization)"ì˜ ì˜ë¯¸:** ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì¥ì„ ì›ì ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ì‘ì—…ì„ ë‚˜íƒ€ë‚´ê²Œ ë¨.       
    * ì¦‰, "í›„ì† ì²˜ë¦¬ì—ì„œ ë¶„í•´ë  í•„ìš”ê°€ ì—†ëŠ” ê¸°ë³¸ ë‹¨ìœ„" ë‹¨ìœ„ê°€ typographic ë‹¨ìœ„ë³´ë‹¤ ë‹¨ì–´ í˜•íƒœì— ë” ê°€ê¹Œìš¸ ë•Œì—ë„ "tokenization"ì´ë¼ê³  ë¶ˆë¦°ë‹¤.       
    * ë” ë‚˜ì•„ê°€, í† í°í™”ìê°€ **ë¬¸ì¥ì„ ë¶„í• **í•  ë¿ë§Œ ì•„ë‹ˆë¼ **ì •ê·œí™”**, **ì² ì ìˆ˜ì •** ë˜ëŠ” **named entity detection** ëª©ì ê³¼ ê°™ì€ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•˜ëŠ” ë° ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ë¯€ë¡œ í˜„ì¬ í† í°ì˜ í‘œì¤€ ì •ì˜ì—ì„œ ë²—ì–´ë‚œë‹¤.      
* âŒ **ë¬¸ì œì **     
    * ì´ëŸ¬í•œ ì´ˆê¸° í† í°ì˜ ì •ì˜ì™€ ì—­í• ì€ ì •ê·œí™” ì‘ì—…ì´ë‚˜ ë‹¤ë¥¸ ê³µë°± ê¸°í˜¸ì˜ í†µí•© ë° ë³‘í•©ì€ **í† í°í™” ì „ìœ¼ë¡œ ë˜ëŒë¦´ ìˆ˜ ì—†ê²Œí•œë‹¤**.     
    * í† í°í™”ëœ ì¶œë ¥ì—ì„œ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ í™•ì‹¤í•˜ê²Œ ë³µêµ¬í•  ìˆ˜ ì—†ë‹¤.   
  
  
 
------
-----


# **4. Augmenting word-level pretokenizer tokens with character information**
íŠ¹íˆ ì§€ë‚œ 5ë…„ ë™ì•ˆ, ë‹¤ì†Œ ì›ìì ì¸ ë‹¨ì–´ì™€ ê°™ì€ ê³µê°„ ë¶„ë¦¬ ë‹¨ìœ„ë¡œì„œì˜ **"í† í°"ì— ëŒ€í•œ ì§ê´€ì ì¸ ì •ì˜ë¥¼ ë„˜ì–´ì„œëŠ” ê²ƒ**ì— ëŒ€í•œ ê´€ì‹¬ì´ ìƒˆë¡œì›Œì¡Œë‹¤.  
ì´ ì±•í„°ì—ì„œëŠ” ì´ë¥¼ ìœ„í•œ í•œ ê°€ì§€ ë°©ë²•ì¸ **ë‹¨ì–´ ë‚´ë¶€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ì™€ ê°™ì€ ë‹¨ìœ„ë¥¼ ë³´ê°•**í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œë‹¤.     

**[word-level ëª¨ë¸ì˜ íŠ¹ì§•]**           
* ê°œë…ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰¬ì›€    
* neural eraì—ì„œëŠ” í•´ì„ ê°€ëŠ¥í•œ ì„¸ë¶„í™”ëœ ê¸°ëŠ¥ì„ ì œê³µ      
* **ë‹¨ì **: íì‡„í˜• ì–´íœ˜ ëª¨ë¸: í›ˆë ¨ ì¤‘ì— ê±°ì˜ ë³´ì´ì§€ ì•ŠëŠ” í¬ê·€í•˜ê³  ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒ    
* **í•´ê²°ì±…:** ì—­ì‚¬ì ìœ¼ë¡œ í¬ê·€í•œ ë‹¨ì–´ ìœ í˜•ì€ í›ˆë ¨ ì‹œ ìƒˆë¡œìš´ ë‹¨ì–´ ìœ í˜• ```UNK(ì•Œ ìˆ˜ ì—†ìŒ)```ë¡œ ëŒ€ì²´ë¨     


**[```UNK```ëŒ€ì²´ ë°©ì‹ì˜ ë‹¨ì ]**         
* ìì—°ì–´ ìƒì„±(NLG)ì„ ìˆ˜í–‰í•  ë•Œ UNKê°€ í—ˆìš©ë˜ì§€ ì•ŠëŠ”ë‹¤   
* ELMo ë˜ëŠ” BERTì™€ ê°™ì€ large-scale ëª¨ë¸ì— ì‚¬ìš©ë  ë•Œ ì¼íšŒì„± ì´ë²¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ìœ ìš©í•œ ì˜ë¯¸ì˜ ì•µì»¤ì¸ **ìƒˆë¡œìš´ ë‹¨ì–´ì— ëŒ€í•œ ê¸°ëŠ¥**ì„ **ì¶”ì¶œí•  ìˆ˜ ì—†ë‹¤**        
* ì˜ì–´ ì´ì™¸ì˜ ì–¸ì–´, íŠ¹íˆ ë” ìƒì‚°ì ì¸ í˜•íƒœí•™ ë° ë”°ë¼ì„œ ìœ í˜• í† í° ë¹„ìœ¨ì´ ë†’ì€ ì–¸ì–´ì—ì„œ í¬ê·€ ë‹¨ì–´ë¥¼ ì œê±°í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥     

ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , **ë‹¨ì–´**ê°€ ì–¸ì–´ì˜ **ê¸°ë³¸ ë‹¨ìœ„**ì´ê¸° ë•Œë¬¸ì—,       
**ë‹¨ì–´ë¥¼ êµ¬ì„±í•˜ëŠ” ë¬¸ìë¥¼ ê¸°ë°˜**ìœ¼ë¡œ ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¨ì–´ ê¸°ë°˜ í‹€ì—ì„œ í¬ê·€í•˜ê³  ìƒˆë¡œìš´ ë‹¨ì–´ì˜ **ì²˜ë¦¬ë¥¼ ê°œì„ **í•˜ëŠ” ì—¬ëŸ¬ ì ‘ê·¼ë²•ì´ ë“±ì¥í–ˆë‹¤.      
ìš°ë¦¬ëŠ” ë‹¨ì–´ í‘œí˜„ì˜ ë³´ë‹¤ í¬ê´„ì ì¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì´ ì„¹ì…˜ì—ì„œ ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ ì¤‘ ì¼ë¶€ë¥¼ ì œì‹œí•  ê²ƒì´ë‹¤.       

-----


## 4.1 Augmenting word-level models with spelling information    

**[ë‹¨ì–´ì— ì´ˆì ]**         
* ì–¸ì–´ë¥¼ ìœ„í•œ neural modelsì—ì„œ,    
90ë…„ëŒ€ì™€ 2000ë…„ëŒ€ì˜ ì—°êµ¬ëŠ” **ë‹¨ì–´ì— ì´ˆì **ì„ ë§ì¶”ê³ , ëŒ€ì‹  ë¬¸ìì˜ ë¬¸ìì—´ì„ ì²˜ë¦¬í–ˆë‹¤.      

**[ë‹¨ì–´ ìì²´ì— ëŒ€í•œ ì •ë³´]**        
* ê·¸ëŸ¬ë‚˜ neural modelsì´ NLPì—ì„œ ì¤‘ìš”í•´ì§€ìë§ˆì, ì‹ ê²½ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ **ë‹¨ì–´ì™€ ë¬¸ì ìˆ˜ì¤€ì˜ ì •ë³´ì˜ ì¡°í•©**(word- and character-level information)ì´ ë“±ì¥.       
* ë‹¨ì–´ ì„ë² ë”© ì¶”ì •ì„ ë•ê¸° ìœ„í•´ ë‹¨ì–´ ìì²´ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚¬ìš©í•  ê²ƒì„ ì²˜ìŒ ì œì•ˆí–ˆë‹¤.      
* wordâ€™s embeddingì„ ë‹¨ì–´ì˜ ì² ìë¥¼ í†µí•´ êµ¬ì„±í•œë‹¤ëŠ” ì•„ì´ë””ì–´ë¥¼ **ëŒ€ì¤‘í™”**í–ˆë‹¤.   


**[CNN ê¸°ë°˜ ì„ë² ë”© ê¸°ëŠ¥]**     
* ì„ë² ë”© í–‰ë ¬ì„ CNN(convolutional neural network) ë ˆì´ì–´ë¡œ ëŒ€ì²´í•  ë•Œì—ë„ generative ëª¨ë¸ì€ ì—¬ì „íˆ íì‡„ ì–´íœ˜ì´ë©°, 
ì´ëŠ” í›ˆë ¨ ë°ì´í„°ì—ì„œ ë³´ì˜€ë˜ ë‹¨ì–´ë§Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì—,         
CNN êµ¬ì„±ì€ **ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ì•„ë‹Œ** **í¬ê·€ ë‹¨ì–´**ë§Œ ë•ëŠ”ë‹¤.      


**[CNN with í† í°ì˜ ì² ì]**      
* **ê° í† í°ì— ëŒ€í•œ ì² ì**ë¡œ ì„ë² ë”©ì„ êµ¬ì„±í•˜ëŠ” CNN ê¸°ë°˜ ì„ë² ë”© ê¸°ëŠ¥     
* ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì˜ˆìƒí•˜ëŠ” ëŒ€ì‹  "ìì£¼ ë‹¨ì–´ë¥¼ ì •í™•í•˜ê²Œ ì–»ë„ë¡" ì•”ë¬µì ìœ¼ë¡œ í›ˆë ¨í•œë‹¤.  
* **ì´ ê¸°ë°˜ ëª¨ë¸ì˜ ì—­í• :** POS íƒœê¹…ê³¼ ê°™ì€ ë‹¤ë¥¸ ê³ ì „ì ì¸ NLP ì‘ì—…ì˜ ë°œì „ì„ ì´ëŒì—ˆê³  ê¶ê·¹ì ìœ¼ë¡œ ìµœì´ˆì˜ í° ë¬¸ë§¥ ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ ELMoì— í˜ì„ ì‹¤ì–´ì£¼ì—ˆë‹¤.        
  * [fastText](https://yerimoh.github.io/LAN7/) ì„ë² ë”©ì€ ë¬¸ìê°€ ì•„ë‹ˆë¼ ê²¹ì¹˜ëŠ” n-gramì—ì„œ ë‹¨ì–´ ì„ë² ë”©ì„ êµ¬ì„±í•  ê²ƒì„ ì œì•ˆí•˜ì—¬,     
ìƒˆë¡œìš´ ë‹¨ì–´ì— ëŒ€í•œ ì„ë² ë”©ì„ ì–»ì„ ìˆ˜ ìˆë‹¤(generative ì˜ë¯¸ëŠ” ì•„ë‹ˆì§€ë§Œ ê·¸ëŸ° ì˜ë¯¸ì—ì„œ "open-vocabulary"ë¡œ ë§Œë“ ë‹¤).    

  * Ataman and Federicoë„ ë§ˆì°¬ê°€ì§€ë¡œ ë¬¸ì ëŒ€ì‹  (ì¤‘ì²©) ğ‘›-gramsì„ ì‚¬ìš©í•˜ì—¬ ê¸°ê³„ ë²ˆì—­ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì–»ëŠ”ë‹¤(í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€í•œ ì–¸ì–´ì—ì„œ BPEë¥¼ ëŠ¥ê°€í•¨).     
  * ìµœê·¼ì—ëŠ” ì² ì ì˜¤ë¥˜ ë˜ëŠ” ì „ì†¡ìœ¼ë¡œ ë…¸ì´ì¦ˆê°€ ë§ì€ í…ìŠ¤íŠ¸ë¥¼ ë” ì˜ ì²˜ë¦¬í•¨ìœ¼ë¡œì¨     
êµ¬ì„± ë¬¸ì ì„ë² ë”©ì— ëŒ€í•œ BPE ë‹¨ìœ„ì˜ ì„ë² ë”©ì„ í–¥ìƒì‹œì¼°ë‹¤.      


**[small Transformer]**   
* ì˜ë£Œ í…ìŠ¤íŠ¸ì™€ ê°™ì€ ìƒˆë¡œìš´ ë„ë©”ì¸; ë™ì‹œì— Aguilar ë“±(2021)ì€ ê±°ì˜ ë™ì¼í•˜ê²Œ í•˜ì§€ë§Œ CNN ëŒ€ì‹  small Transformerë¥¼ ì‚¬ìš©í•œë‹¤.     

**[êµ¬ì„± ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹]**   
* êµ¬ì„± ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì€ ì‚¬ì „ í›ˆë ¨ëœ ë‹¨ì–´ ìˆ˜ì¤€ ì…ë ¥ ëª¨ë¸ì—ë„ í†µí•©ë˜ì—ˆë‹¤.     
* íŠ¹íˆ, Pinter ë“±(2017)ì€ í…ŒìŠ¤íŠ¸ ì‹œê°„ ë™ì•ˆ ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚  ë•Œë§ˆë‹¤ í˜¸ì¶œë˜ëŠ” helper RNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ **ì² ìê°€ ì£¼ì–´ì§„ ë‹¨ì–´ì˜ ì„ë² ë”©ì„ ëª¨ë°©**í•˜ë„ë¡ í›ˆë ¨ëœ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤.      


---


## **4.2 Open-vocabulary language modeling with (tokenizer-defined) words made of characters**
**[open-vocabulary language modelingì˜ ì–´ë ¤ì›€]**    
* ```open-vocabulary language modeling```: test timeë•Œ **ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê³  ìƒì„±**í•  ìˆ˜ ìˆëŠ” ëª¨ë¸     
* ```closed-vocabulary generative models```ì„ ```open-vocabulary language modeling```ë¡œ í™•ì¥í•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤.              
* **ì–´ë ¤ìš´ ì´ìœ **: ì™„ì „íˆ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ì˜ ë¬´í•œ ì§‘í•©ì— ëŒ€í•œ í™•ë¥  ì§ˆëŸ‰ì„ ìœ ì§€í•  ìˆ˜ ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— input sideì˜ Open-vocabulary ë³´ë‹¤ ë‹¤ì†Œ ì–´ë µë‹¤.     


**[Mielkeì™€ Eisner(2018)ì˜ ëª¨ë¸]**         
* ë‹¨ì–´ ì„ë² ë”©ì„ ì •ê·œí™”í•˜ì—¬ ì‘ì€ ë¬¸ì ìˆ˜ì¤€(smaller character-level)ì˜ ```RNNLM```ì„ ì‚¬ìš©     
* smaller modelì„ ì‚¬ìš©í•˜ì—¬ ì² ìë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•¨ìœ¼ë¡œì¨ ì¼ë°˜ì ì¸ íì‡„ ìŒì„± ë‹¨ì–´ ìˆ˜ì¤€ì˜ ë°˜ë³µ ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸(```RNNLM```) ì„¤ì •ì„ ê·¼ë³¸ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” í™•ë¥ ë¡ ì  2ë‹¨ê³„ ëª¨ë¸ì„ ì œì•ˆí•œë‹¤.       
* ë‹¨ì–´ ìˆ˜ì¤€(word-level) ```RNNLM```ì´ ```UNK```ë¥¼ **ì˜ˆì¸¡í•  ë•Œë§ˆë‹¤ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì¦‰ì‹œ ìƒì„±**      
â¡ ì–¸ì–´ ê°œë…ê³¼ ì§ê´€ì  ëª¨ë¸ë§ì— ì˜í•´ ë™ê¸° ë¶€ì—¬       
â¡ ì§ˆì  ë° ì •ëŸ‰ì ìœ¼ë¡œ ì„±ê³µì ì¸ ê²ƒìœ¼ë¡œ ì…ì¦ëœ ê°œë°©í˜• ì–´íœ˜ ëª¨ë¸ì„ ì‚°ì¶œ      



**[```cache model```ì˜ í™•ì¥]**        
* Kawakami et al. (2017)ì˜ ëª¨ë¸ì€ ë‹¨ì–´ ë° ë¬¸ì ìˆ˜ì¤€ì˜ RNNì˜ ìœ ì‚¬í•œ 2ë‹¨ê³„ ì„¤ì •ì„ ë”°ë¥´ì§€ë§Œ,     
cache model(Grave et al., 2016)ì„ ì‚¬ìš©í•˜ì—¬ directly copiedí•  ìˆ˜ ì—†ëŠ” ê²½ìš°, **ê° ë‹¨ì–´ë¥¼ ë¬¸ì ìˆ˜ì¤€ì˜ RNNì„ ì‚¬ìš©í•˜ì—¬ ì² ì**í•´ì•¼ í•œë‹¤.         
â¡ ì´ë“¤ì˜ ë¶„ì„ì€ ìºì‹œ ëª¨ë¸ì´ ê·¸ë ‡ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ë¶„ëª…íˆ ë³´ì—¬ì¤€ë‹¤.      
* ```cache model```ì€ â€œburstyâ€ **ë¯¸ì§€ì˜ ë‹¨ì–´**ë§Œ ë³µì‚¬í•˜ì§€ë§Œ, ë˜í•œ ìŠì–´ë²„ë¦¬ì§€ ì•Šë„ë¡ í•˜ê¸° ìœ„í•´ ê·¹íˆ extremely **common** function wordsë“¤ë„ ë³µì‚¬í•œë‹¤.       
* ì´ ì•„ì´ë””ì–´ëŠ” ```Ataman``` ë“±(2019)ì— ì˜í•´ machine translation decoder(creating word embeddings on the encoder side from character-level ```BiRNNs``` as in ```ELMo```)ì—ì„œ ì¸ì½”ë” ì¸¡ì— ë‹¨ì–´ ì„ë² ë”©ì„ ìƒì„±)ì— ëŒ€í•´ ì„ íƒë¨    
* ë‚˜ì¤‘ì— ```Ataman``` ë“±(2020)ì— ì˜í•´ í™•ì¥ë¨          


**[ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹]**     
* ë” ë‚®ì€ ì†ë„ë¡œ ```multi-layer RNNs```ì˜ higher layerë¥¼ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ë‹¤(hidden stateë¡œì˜ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆë›´ë‹¤)     
* ì´ëŠ” El Hihi and Bengio(1995)ì—ì„œ ì²˜ìŒ ì œì‹œëœ ì˜¤ë˜ëœ ì•„ì´ë””ì–´ì´ë©°(Schmidhuber (1991, 1992)ì˜ "neural sequence chunker"ì— êµ¬ì¶•ë¨)      
* ìš°ë¦¬ëŠ” ë‹¨ì–´ ê²½ê³„ì™€ ê·¸ì— ë”°ë¥¸ segmentationsì„ ì‹¤ì œë¡œ ë°°ìš¸ ìˆ˜ ìˆë‹¤.    


----
----

# **5 Learning segmentations to find concatenative word-like pretokenizer tokens**
ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” [2](#2-Tokens,-word-forms,-and-sub-words)ì— ìš”ì•½ëœ ê°œë…ì •ì˜ì˜ ë³€í™”ì—ë„ ë¶ˆêµ¬í•˜ê³ ,    
predefinedëœ ë‹¨ì–´(ë˜ëŠ” ì‚¬ì „ í† í°í™” ì¶œë ¥) ê°œë…ì„ ê°–ëŠ” ê²ƒì— ì˜ì¡´í•´ì™”ë‹¤.              

**[ë¬¸ì œ]**      
ê·¸ëŸ¬ë‚˜ ì˜ì–´ê°€ ì•„ë‹Œ ì–¸ì–´ë‚˜ robustnessë¬¸ì œë¡œ ì¸í•´ predefinedëœ ì •ì˜ê°€ ì£¼ì–´ì§€ì§€ ì•Šê±°ë‚˜, ì–»ì„ ìˆ˜ ì—†ê±°ë‚˜, ë‹¨ìˆœíˆ ë°”ëŒì§í•˜ì§€ ì•Šë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œ?      
ìš°ë¦¬ì˜ **ë°ì´í„° ê¸°ë°˜ ë¨¸ì‹  ëŸ¬ë‹ ì ‘ê·¼ë²•**ì´ **í† í°í™”ë¥¼ í•™ìŠµ**í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ê¹Œ?     

**[í•´ê²°]**       
ì´ ì ˆì— ì„¤ëª…ëœ ëŒ€ë¶€ë¶„ì˜ ì ‘ê·¼ ë°©ì‹ì€ **ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„**ì— í•´ë‹¹í•˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ì™€ **ê²½ê³„ë¥¼ ì°¾ê¸° ìœ„í•´**,     
**ê·¼ì‚¬(approximate)** ë˜ëŠ” (ë” ë§ì€ ê°€ì •ì„ ì‚¬ìš©í•˜ì—¬) **ì •í™•í•œ ì¶”ë¡ **ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” **ì ì¬(latent) ë³€ìˆ˜ë¡œ ì•”ì‹œì  ë¶„í• (implied segmentation)ì„ ì²˜ë¦¬**í•¨ìœ¼ë¡œì¨ í† í°í™”ë¥¼ í•´ê²°í•  ê²ƒì„ ì œì•ˆí•œë‹¤.       

ì´ ì ˆì—ì„œ ì„¤ëª…í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì€ ë‹¤ì–‘í•œ í¬ê¸°ì™€ í’ˆì§ˆì˜ ë‹¨ìœ„ë¥¼ ì‚°ì¶œí•œë‹¤.


----

## 5.1 Character-level neural models that learn to skip steps at higher levels

**[Elman(1990)]**     
* ì´ë¯¸ 90ë…„ëŒ€ì— Elman(1990)ì€ character-levelì˜ RNNì„ ìˆ˜ë™ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì˜ˆì¸¡ ê¹œì§ ë¹„êµë¥¼ ë‹¨ì–´ ê²½ê³„(boundaries)ì™€ ê´€ê³„ì‹œì¼°ë‹¤.    
* ì´ ì•„ì´ë””ì–´ëŠ”  Schmidhuber(1991, 1992)ì˜ â€œneural sequence chunkerâ€ì—ì„œ í™•ì¥ë˜ì—ˆë‹¤.     


**[Doval and GÃ³mezRodrÃ­guez(2019)]**     
* ìµœê·¼ì—ëŠ” Doval and GÃ³mezRodrÃ­guez(2019)ì˜ beam search framework í•˜ì—ì„œ character-level neural models ë¿ë§Œ ì•„ë‹ˆë¼ n-gram modelsì—ë„ ë¶„ì„ì„ ì ìš©í•˜ì—¬ ê³µê°„ì´ ì‚­ì œë˜ëŠ” micro blog textsë¥¼ ë¶„í• í–ˆë‹¤.     


**[```HM-RNN```(Chung et al., 2017)]**      
* post-hoc surprisal thresholding(ì‚¬í›„ ì„ê³„ê°’)ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  [4.2](# Open-vocabulary-language-modeling-with-(tokenizer-defined)-words-made-of-characters)ì—ì„œ ë™ê¸°í™”ëœ ì—¬ëŸ¬ timescalesì˜ ì•„ì´ë””ì–´ë¥¼ ì·¨í•œë‹¤.     
* ê¸°ìš¸ê¸° í•˜ê°• ìµœì í™” ë°©ë²•        
   * skipì´ë‚˜ updateí•˜ê¸° ìœ„í•œ ì´ì§„ ê²°ì •(binary decisionbinary decision)ì„ í•™ìŠµ       
   â¡ ë‹¨ì–´ ê²½ê³„ ê°ê° ì œê³µ    
   * ì§ì„  ì¶”ì •ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ëµì ì¸ ê¸°ìš¸ê¸° í•˜ê°•ìœ¼ë¡œ ìµœì í™”        
* ê³„ì¸µë“¤ ì‚¬ì´ì˜ í†µì‹ ì€ **ì–‘ë°©í–¥ì **ìœ¼ë¡œ ì¼ì–´ë‚¨      
   * the lower network: ê·¸ê²ƒì˜ ìµœì¢… ìƒíƒœë¥¼ higher ë„¤íŠ¸ì›Œí¬ì— ë³´ê³ í•œë‹¤      
   * the higher network: ê·¸ê²ƒì˜ ìƒˆë¡œìš´ ìƒíƒœë¥¼ lower ê³„ì¸µì— ë³´ê³ í•œë‹¤.     
   * ì´ëŸ¬í•œ í†µì‹ ì„ ìŠ¤ìŠ¤ë¡œ ê³„ì† ì§„í–‰í•œë‹¤.    
 
 
**[```HM-RNN```(Chung et al., 2017)ì˜ ì—°êµ¬]**      
* **Kawakami et al. (2019):** ë°ì´í„°ì— ê³µê°„ì„ í¬í•¨í•  ë•Œ ë‹¨ì–´ ê²½ê³„ë¥¼ â€œrecover(ë³µêµ¬)â€ í•˜ì§€ë§Œ,      
Kawakami ì™¸(2019)ëŠ” **ê³µê°„ì„ í¬í•¨í•˜ì§€ ì•Šì„ ë•Œ** ì´ ëª¨ë¸ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ì„¸ê·¸ë¨¼íŠ¸(unusable segments)ë¥¼ ì–»ì—ˆë‹¤ê³  ì£¼ì¥í•œë‹¤.     
* **NMTì— ```HM-RNN```ì„ ì‚¬ìš©í•˜ë ¤ê³  í•  ë•Œ:**     
   * Cherry ë“±(2018)ì€ ê·¸ê²ƒì´ í›ˆë ¨ë˜ë„ë¡ í•˜ëŠ” ë° **ë§ì€ ìˆ˜ì • ì‘ì—…ì´ í•„ìš”**í–ˆìœ¼ë©°,      
ì‘ì—…ì—ì„œ ê·¸ê²ƒì˜ ì„±ëŠ¥ì€ ê²½ìŸì ì´ì§€ë§Œ **ìš°ìˆ˜í•˜ì§€ëŠ” ì•Šì•˜ë‹¤**ê³  ë³´ê³ í–ˆë‹¤.     
   * ì´ ë°œê²¬ì€ HM-RNNì´ ì˜ í›ˆë ¨ë˜ë„ë¡ í•˜ê³ , ì´ë¥¼ ì™„í™”í•˜ë©°,    
   í…ìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ í•˜ìœ„ ì„¸ê·¸ë¨¼íŠ¸(subpar segmentation)ë¥¼ ë³´ì—¬ì£¼ëŠ” ë…¼ë¬¸ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ”  KÃ¡dÃ¡r et al. (2018)ì˜ ì—°êµ¬ ê²°ê³¼ë¥¼ ë’·ë°›ì¹¨í•œë‹¤.      
* Kreutzer and Sokolov (2018)ëŠ” NMTì— ëŒ€í•´ **ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  í•˜ìœ„ ë ˆì´ì–´ë¡œ ìš”ì•½ì„ ìƒì„±**í•˜ëŠ” ìœ ì‚¬í•œ íŒ¨ëŸ¬ë‹¤ì„ì„ ì‚¬ìš©í•˜ë ¤ê³  ë…¸ë ¥í•˜ë©°, **ê±´ë„ˆë›°ê¸°**ëŠ” ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ìœ„í•´ **ë¶ˆí•„ìš”**í•´ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•œë‹¤.        
* **ëª¨ë¸ì˜ í™•ì¥:** ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ ëª¨ë¸ì€ Luoì™€ Zhu(2021ë…„)ì— ì˜í•´ êµ¬ì™€ ë¬¸ì¥ ìˆ˜ì¤€ì˜ ê²½ê³„ë¡œ í™•ì¥ëœë‹¤.      
* ë” coarserí•œ ê³„ì‚° ë ˆì´ì–´ë¥¼ ê°€ì§€ê³  ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ëŸ¬í•œ ëª¨ë¸ì€ ì—¬ì „íˆ ë‹¨ì–´ê°€ ìƒì„±ë  ë•Œë§ˆë‹¤  â€œspell outâ€í•´ì•¼ í•œë‹¤.       
ì¦‰, í† í°ì„ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¨ìœ„ë¡œ memoizeí•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì§€ì í•  í•„ìš”ê°€ ìˆë‹¤.       


-------


## **5.2 Marginalization over all possible segmentations**    
ë§ˆì§€ë§‰ìœ¼ë¡œ, ê°œë…ì ìœ¼ë¡œ ê°„ë‹¨í•œ ì ‘ê·¼ ë°©ì‹ì€,    
ë¬¸ìì—´ì˜ ë¶„í• ì„ trainingê³¼ test time ëª¨ë‘ì—ì„œ **marginalizedí•´ì•¼ í•˜ëŠ” ì ì¬ ë³€ìˆ˜(latent variable)** ë¡œ ë‹¤ë£¨ëŠ” ê²ƒì´ë‹¤.     

**[ë¬¸ì œ]**       
ì´ê²ƒì€ ë³¸ì§ˆì ìœ¼ë¡œ ê²¹ì¹˜ëŠ” ê¸¸ì´ê°€ ë‹¤ë¥¸ ë¬¸ìì—´, ì¦‰ "cat", "at", "foster cat"ì„ í¬í•¨í•˜ëŠ” ì–´íœ˜ë¥¼ ê°–ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°,   
"my foster cat" ë¬¸ìì—´ì€ ì ì¬ ë‹¨ìœ„ì˜ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ì— ëŒ€ì‘í•˜ì—¬ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ ë¶„í•´ë  ìˆ˜ ìˆë‹¤.    
â¡ ë¶„í• ì˜ ìˆ˜ê°€ ì‹œí€€ìŠ¤ ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì—ì„œ ê¸°í•˜ê¸‰ìˆ˜ì ì´ë‹¤.            

**[í•´ê²°]**      
* ì ì¬ ë¶„í•´ì— ëŒ€í•œ ì£¼ë³€í™”ë¥¼ ìœ„í•œ ê·¼ì‚¬ì¹˜(Â§ 5.2.1)ì— ì˜ì¡´       
* n-gram ëª¨ë¸(Â§ 5.2.2)ì„ ì‚¬ìš©í•˜ì—¬ ë…ë¦½ì„± ê°€ì •ìœ¼ë¡œ ëª¨ë¸ì„ ë‹¨ìˆœí™”í•´ì•¼í•¨       



<details>
<summary>ğŸ“œ 5.2.1 Approximate marginalization</summary>
<div markdown="1">
  
</div>
</details>  


5.2.1 Approximate marginalization
Chan et al. (2017) propose an estimator to approximate the marginal probability of observations using
approximate MAP inference through beam search.
They find that the model is very hard to train, but
manage to obtain promising results. Buckman
and Neubig (2018) confirm this modelâ€™s instability
and propose some approximate inference schemes
based on averaging RNN hidden states that produce
better results in terms of LM perplexity. Hiraoka
et al. (2020) implement a similar model, based
on a Unigram LM tokenization proposal distribution (see Â§6.4.3), whose ğ‘›-best tokenizations of a
sentence are fed into any sentence encoder model
independently and whose resulting sentence embeddings are averaged in line with their a priori
tokenization likelihood. Hiraoka et al. (2021) extend this model to sequence-to-sequence settings
by training a tokenizer and downstream model with
separate losses, the former by rewarding tokenizations that produced a low downstream loss, and the
latter using just one tokenization sampled from the
conditioned (and tempered) LM.
5.2.2 Exact marginalization using additional
independence assumptions: segmental
neural language models
The more popular solution of segmental neural language models was pioneered by Kong et al. (2016),
who cast the problem of segmentation as a monotonic13 seq2seq task, going from characters to a
covering sequence of substrings, i.e., a segmentation. By conditioning segment prediction on the
entire raw string, processed and embedded using
a BiRNN, segmentation decisions/scores can use
context, but by scoring every individual possible
substring independently as a segment using these
embeddings and then adding up individual scores
to score entire segmentations, they can find a covering of the entire input string with segments efficiently using dynamic programming. The reason
for this ability is the central independence assumption: the model does not depend on any other segments when scoring a segment, but merely on surrounding characters. Wang et al. (2017) extend
this by also having a per-segment RNN over characters for the outputs that can run without knowing
the segmentation and whose past representations
can thus be used by the individual segment generation processes, allowing for left-to-right sharing
of information about segments without breaking
dynamic programming.
The jump to LMing is now made simply by
omitting the conditioning on an input, yielding the
model of Sun and Deng (2018), who coin the term
segmental language model, training on Chinese
characters and using the unsupervisedly learned
segments to compete on Chinese Word Segmentation. To keep the computation of the quadratic number of segments feasible, they restrict the segments
to a maximum length of 4 characters (a sensible
prior for Chinese). Grave et al. (2019) make the
same jump independently, using Transformers as
the independent character-level global backbone.
When evaluating on English open-vocabulary language modeling, Grave et al. (2019) notice improved perplexity, but not using or evaluating the
obtained segmentation, most likely because they,
too, only use 4-grams that appear at least 400
times. Contemporaneously, Kawakami et al. (2019)
use the same independence idea, but have emissions of string segments come from a contextdependent mixture of a character-level model like
in Kawakami et al. (2017) (see Â§4.2) and a large
set of substrings (up to 10-grams that appear often
enough in training data) with learned embeddings.
They evaluate not only on perplexity, but also on
word segmentation performance, where they do
beat some baselines (see Â§5.3), but still perform
much worse than some previous models,14 which
they argue tuned their hyperparameters on segmentation performance instead of marginal likelihood
and thus have an unfair advantage.
Interestingly, when training on image captions,
Kawakami et al. (2019) find that both perplexity and segmentation performance improve when
the model also has access to the image that is being described, showing that learning segmentation
only from monolingual and unimodal text may be
harder than when other modalities or languages are
present. This observation is shared by He et al.
(2020), who build a similar segmental model (in
their case, a Transformer-based version that still
follows the character backbone idea to allow for
dynamic programming) as the target-side generator
in an NMT system and use it not as the final model,
but merely as a learned tokenizer. This is easy
to achieve by changing the dynamic program from
marginalization to maximization and thus obtaining
a new segmentation, called DPE, that can be used
in place of BPE or unigram LM (see Â§6.4). He et al.
(2020) proceed to show that learning to tokenize
with a small Transformer-based NMT model15 produces better segmentations than BPE for use in a
bigger model; in particular, training the tokenizing
model on the translation task produces different
segmentations depending on the source language,
and, more importantly, better segmentations (as
measured through downstream translation performance) than training on target-side language modeling alone.
The idea of conditioning on characters and pre
dicting segments is extended to the adirectional
masked language modeling setting found in Transformers and left-to-right autoregressive Transformers by Downey et al. (2021), though results do not
outperform RNN-based SNLMs consistently.
Note that many of these models can also be seen
as relatives of models based on UnigramLM, which
we will cover in Â§6.4.3.





5.3 Finding words through Bayesian
non-parametrics
In the era of ğ‘›-gram and word-based language
models, MacKay and Peto (1995) noticed that a
Bayesian view of autoregressive language models
may prove beneficial, reinterpreting smoothing and
backoff in ğ‘›-gram models as inference in a hierarchical model where higher-order distributions are
drawn from a Dirichlet distribution whose mean is
a lower-order distributions. Teh (2006) extends this
thinking, proposing a hierarchical PYP language
model where we again have ğ‘›-gram distributions
of arbitarily large orders, drawn through a hierarchy of PYP distributions that lead to a model that
still bears resemblance to ğ‘›-gram language model
smoothing, but offers a principled way to forego
the choice of ğ‘›. The pinnacle of this idea of modeling was reached in Wood et al. (2011)â€™s sequence
memoizer, which boasted great compression performance for arbitrary binary data and still performed
very well on language modeling tasks, although
neural models at this time already proved to be
strong competitors.
At the same time, Goldwater et al. (2006b) extended this Bayesian perspective to also explain
how new words are first coined and how they are
then used in running text: a process they call twostage language modeling (see Â§4.2), with the two
stages being referred to as generator (which creates
new lexemes) and adaptor (which governs reuse;
here, a Pitman-Yor Process (PYP)), relating the
resulting interaction between types and tokens to
interpolated Kneser-Ney smoothing as presented
in Chen and Goodman (1999).16 Given such a twostage model to explain text and the use of Bayesian
nonparametrics that can assign positive probability
to an infinite number of possible lexemes, it becomes possible to also try to infer word boundaries,
that is to perform unsupervised word segmentation.
Motivated more by trying to explain and model cognitive processes and in particular child language
acquisition, Goldwater et al. (2009)
17 summarize
Unigram and Bigram Dirichlet Processes (DPs)
for segmenting transcribed infant-directed speech,
showing superiority over older non-Bayesian approaches. Mochihashi et al. (2009) extend the idea
from bigram DPs to âˆ-gram nested/hierarchical
PYPs to improve word segmentation for English
written text; Elsner et al. (2013) additionally model
phonotactic processes that convert a sequence of
segments into observed realizations.
5.4 Related task: Unsupervised Chinese
Word Segmentation
Word segmentation for languages without whitespace delimiters such as Chinese, Japanese and
Vietnamese (Shao et al., 2018) is an important area
of research and can be notoriously tricky.
In Chinese word segmentation (CWS), there is
growing interest in exploring unsupervised word
segmentation involving neural language models.
Traditionally, popular unsupervised approaches
take on two primary paths: 1) discriminative models and 2) generative models. Discriminative models rely on goodness measures for candidate segmentation. These statistical measures incude Mutual Information (MI), normalized Variation of
Branching Entropy (nVBE) and Minimum Description Length (MDL), etc., see Â§6.3. Generative
models focus on designing statistical models to
find the optimal segmentation of the highest generative probability. These models include Hidden
Markov Model (HMM), Hierarchical Dirichlet Process (HDP), Nested Pitman-Yor Process (NPY),
etc., see Â§5.3. It is trivial to extend discriminative
approaches by replacing ğ‘›-gram language model
with neural language models. For generative approaches, previous work has shown that constructing a neural language model with a context encoder
and a segment decoder achieves competitive performance to its statistical counterparts (Sun and Deng,
2018, see previous subsection Â§5.2.2).


















