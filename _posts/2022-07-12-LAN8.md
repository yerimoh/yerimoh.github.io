---
title: "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP ì •ë¦¬"
date:   2022-07-12
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## í•µì‹¬  

## ì½ê¸° ìœ„í•´ í•„ìš”í•œ ì§€ì‹
* [word2vec](https://yerimoh.github.io/DL14/): baseline ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ê¼­ ì•Œì•„ì•¼ í•œë‹¤.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec ì†ë„ ê°œì„ ìœ¼ë¡œ ì´ í¬ìŠ¤íŒ…ë„ ê¼­ ì•Œì•„ì•¼ í•œë‹¤.      

## ì› ë…¼ë¬¸
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)


---

# ëª©ì°¨  


---


# **Abstract**
ìš°ë¦¬ê°€ ëª¨ë¸ë§í•˜ê³ ì í•˜ëŠ” í…ìŠ¤íŠ¸ì˜ ë‹¨ìœ„ëŠ”  
bytesì—ì„œ ë‹¤ì¤‘ ë‹¨ì–´ í‘œí˜„ì‹(multi-word expressions)ì— ì´ë¥´ê¸°ê¹Œì§€, í…ìŠ¤íŠ¸ëŠ” ë‹¤ì–‘í•œ ì„¸ë¶„ì„±ìœ¼ë¡œ ë¶„ì„ë˜ê³  ìƒì„±ë  ìˆ˜ ìˆë‹¤.      

ìµœê·¼ê¹Œì§€ ëŒ€ë¶€ë¶„ì˜ ìì—°ì–´ ì²˜ë¦¬(NLP) ëª¨ë¸ì€ ë‹¨ì–´ë¥¼ í†µí•´ ì‘ë™í•˜ì—¬ discrete ë°  atomic tokensìœ¼ë¡œ ì²˜ë¦¬í–ˆì§€ë§Œ,   
byte-pair encoding(BPE)ì„ ì‹œì‘ìœ¼ë¡œ ë§ì€ ì˜ì—­ì—ì„œ í•˜ìœ„ ë‹¨ì–´ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì´ ìš°ì„¸í•˜ì—¬ **ì‘ì€ ì–´íœ˜ë¥¼ ì‚¬ìš©**í•˜ë©´ì„œë„ **ë¹ ë¥¸ ì¶”ë¡ **ì„ í—ˆìš©í•˜ê³  ìˆë‹¤.      

ì´ í† í¬ë‚˜ì´ì§• ë°©ë²•ë“¤ì— ëŒ€í•´ í•™ìŠµëœ ì„¸ë¶„í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ **subword-based approaches**ë¿ë§Œ ì•„ë‹ˆë¼ **ë‹¨ì–´ì™€ ë¬¸ìì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ë°©ì‹**ì´ **ì–´ë–»ê²Œ ì œì•ˆë˜ê³  í‰ê°€**ë˜ì—ˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤Œìœ¼ë¡œì¨,     
ì‚¬ì „ ì‹ ê²½ ë° ì‹ ê²½ ì‹œëŒ€ì˜ ì—¬ëŸ¬ ì‘ì—… ë¼ì¸ì„ ì—°ê²°í•œë‹¤.     

ë³¸ ë…¼ë¬¸ì€ ëª¨ë“  ì• í”Œë¦¬ì¼€ì´ì…˜ì— ëŒ€í•´ **íŠ¹ë³„í•œ í•´ê²°ì±…**ì´ ê²°ì½” **ì—†ì„ ê²ƒ**ì´ë©°,   
í† í°í™” ë°©ë²•ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì—¬ì „íˆ ì¤‘ìš”í•˜ë‹¤ê³  ê²°ë¡ ì§“ëŠ”ë‹¤.    


-----
-----

# **1 Introduction**    


_â€œâ€˜tokensâ€™ are not a real thing. they are a computer generated illusion created by a clever engineerâ€ dril_gpt1         


ìš°ë¦¬ê°€ ì‚¬ëŒë“¤ì—ê²Œ NLP ëª¨ë¸ì„ ì²˜ìŒ ì†Œê°œí•  ë•Œ,    
ìš°ë¦¬ëŠ” ì¢…ì¢… í…ìŠ¤íŠ¸ê°€ ì»´í“¨í„°ì— ì‘ì€ ì¡°ê°ë“¤ë¡œ ì˜ë ¤ë‚˜ê°„ë‹¤ëŠ” ìƒê°ì„ ë‹¹ì—°í•˜ê²Œ ë°›ì•„ë“¤ì¸ë‹¤.    
â¡ ê²°êµ­ ê·¸ê²ƒì€ ë‹¨ì§€ ì¼ë ¨ì˜ ì •ìˆ˜ì¼ ë¿ì´ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì´ê²ƒë“¤ì„ (ë³´í†µ) ì—°ì† **sub-strings tokens**ì´ë¼ê³  ë¶€ë¥¸ë‹¤. 


êµìœ¡ í™˜ê²½ì—ì„œ, ê·¸ë¦¬ê³  ì‚¬ì‹¤ ì—­ì‚¬ì ìœ¼ë¡œ NLPì—ì„œ ì´ëŸ¬í•œ í† í°ì€ ë‹¤ì†Œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¨ì–´(ì²˜ìŒì—ëŠ” ì˜ì–´ë¡œ â€œspace-separated substringsâ€)ë¡œ ì•”ì‹œëœë‹¤. 


**[ë‹¨ì–´ì—ì„œ êµ¬ë‘ì  ë¶„ë¦¬ì˜ ì–´ë ¤ì›€]**            
* "don't"ë¼ëŠ” ë‹¨ì–´ë¥¼ ì˜ˆë¡œ ë“¤ë©´,    
* ì´ ë‹¨ì–´ì˜ êµ¬ë‘ì  ë¶„í• ì˜ ê²½ìš°, ```"don"``` ```"'"``` ```"t"```ë¼ëŠ” ë‹¤ì†Œ ë¬´ì˜ë¯¸í•œ ì„¸ ê°œì˜ í† í°ì„ ì–»ê²Œ ë  ê²ƒì´ë‹¤.            
* ê·¸ëŸ°ë° ë” íš¨ê³¼ì ì¸ í† í¬ë‚˜ì´ì§•ì„ í•˜ë ¤ë©´ ```"do"```ì™€ ```"n't"```ì˜ ë‘ ë‹¨ìœ„ë¥¼ ì‚°ì¶œí•´ì•¼ í•œë‹¤ê³  ì£¼ì¥í•  ìˆ˜ ìˆë‹¤.       

---

## ë…¼ë¬¸ ê°œìš”    
ì´ surveyëŠ” $$tokenization$$ì— ëŒ€í•œ ìœ„ì™€ ê°™ì€ ì§ˆë¬¸ì„ ë‹¤ë£¨ë©°,      
ë³¸ ë…¼ë¬¸ì€ ì˜ ê° ì±•í„°ì˜ í•µì‹¬ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ë‹¤,    

**[2. Tokens, word forms, and sub words](# 2-Tokens,-word-forms,-and-sub-words)**      
* ê·¼ë³¸ì ì¸ ì§ˆë¬¸ê³¼ ìš©ì–´ë¥¼ ìƒì„¸íˆ ì„¤ëª…     

**[3: Pre-tokenization yields word-like typographic units](# 3-Pre-tokenization-yields-word-like-typographic-units)**      
* ëª¨ë“  NLP ì‘ì—…ì—ì„œ ë‹¤ì†Œ ë§¤ë ¥ì ì´ì§€ ì•Šì€ ë¶€ë¶„ì´ ì—­ì‚¬ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë‹¤ë£¨ì–´ì¡ŒëŠ”ì§€ë¥¼ ë³´ì—¬ì¤Œ     

íŠ¹íˆ ì§€ë‚œ 5ë…„ ë™ì•ˆ, ë‹¤ì†Œ ì›ìì ì¸ ë‹¨ì–´ì™€ ê°™ì€ ê³µê°„ ë¶„ë¦¬ ë‹¨ìœ„ë¡œì„œì˜ **"í† í°"ì— ëŒ€í•œ ì§ê´€ì ì¸ ì •ì˜ë¥¼ ë„˜ì–´ì„œëŠ” ê²ƒ**ì— ëŒ€í•œ ê´€ì‹¬ì´ ìƒˆë¡œì›Œì¡Œë‹¤.  
* **4**: ì´ë¥¼ ìœ„í•œ í•œ ê°€ì§€ ë°©ë²•ì¸ **ë‹¨ì–´ ë‚´ë¶€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ì™€ ê°™ì€ ë‹¨ìœ„ë¥¼ ë³´ê°•**í•˜ëŠ” ë°©ë²•.    
* **5**: ì‹ ê²½ ëª¨ë¸ë§ì€ ì´ë¥¼ ê·¸ ì–´ëŠ ë•Œë³´ë‹¤ ì‰½ê²Œ ë§Œë“¤ì–´ ëª…ë°±í•œ í‘œì‹œ ì—†ì´ ë‹¨ì–´ ê²½ê³„ê¹Œì§€ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ë¡œ ì´ì–´ì§„ë‹¤(ì˜ˆ: ê³µë°±ì´ ìˆëŠ” ê²½ìš°).  unsupervised word segmentation or discoveryì˜ ê°œë…ì€ ìˆ˜ì‹­ ë…„ì˜ ì‘ì—…ì—ì„œ ìì²´ì ìœ¼ë¡œ ì¡´ì¬í–ˆì§€ë§Œ,    
* **6**: **subword** ë‹¨ìœ„ë¥¼ ì›ì í† í°ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” í˜„ì¬ ë„ë¦¬ í¼ì ¸ ìˆëŠ” ì•„ì´ë””ì–´ë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì‚´í´ë³¼ ë•Œ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ìœ ìš©í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œê²Œ ë  ê²ƒì´ë‹¤.      
* **7**: ë‹¤êµ­ì–´ ì–´íœ˜ì˜ ê³µìœ ì™€ ê²½ìŸì˜ ëª‡ ê°€ì§€ ë¬¸ì œ ì‚´í´ë´„     
* **8**: ë¬¸ì, ë°”ì´íŠ¸ ë˜ëŠ” ì‹¬ì§€ì–´ í”½ì…€ë¡œ ìµœëŒ€ ë¶„í•´í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ê°„ë‹¨í•œ í† í°í™”ë¥¼ ì‚¬ìš©í•´ë´„        


ì´ ë…¼ë¬¸ì´ ëë‚œ í›„,       
* ë°”ì´íŠ¸ì˜ ì†ì‰¬ìš´ ì†”ë£¨ì…˜ì´ ë„ë‹¬í•´ ë³´ì´ëŠ” 2021ë…„ í˜„ì¬ì—ë„ "ë³µì¡í•œ" í† í°í™”ê°€ ì˜ ì¤‘ìš”ì„±ì„ ë…¼ì¦í•˜ì—¬ ë§ˆë¬´ë¦¬ í•œë‹¤.     
* ìµœê·¼ì˜ ë°œì „ì€ íŠ¹ì • ë„ë©”ì¸ê³¼ ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•´ ìµœëŒ€ ë¶„í•´ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì§€ë§Œ, ê·¸ê²ƒë“¤ì€ ê´‘ë²”ìœ„í•œ NLP ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‹¤ë£¨ì§€ ì•Šê³  **ìì²´ì˜ ë‹¨ì ê³¼ í¸ê²¬**ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ì£¼ì¥í•  ê²ƒì´ë‹¤.     


![image](https://user-images.githubusercontent.com/76824611/178999180-03d114b4-8ba7-4748-afce-5115faccc2b5.png)
A taxonomy of segmentation and tokenization algorithms and research directions


------
-----











# **2 Tokens, word-forms, and sub-words** 
ì´ ì±•í„°ì—ì„œëŠ” tokenì— ëŒ€í•œ ê·¼ë³¸ì ì¸ ì§ˆë¬¸ê³¼ ìš©ì–´ë¥¼ ì„¤ëª…í•œë‹¤.      

í† í°, ë‹¨ì–´ í˜•íƒœ ë° í•˜ìœ„ ë‹¨ì–´ NLPì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” 
"ë¬¸ì¥"(sentences)ê³¼ "ë‹¨ì–´(words)"ë¡œ ë¶„í• ë˜ì—ˆë‹¤. 

**[sentences]**     
* ê±°ì‹œì  ë‹¨ìœ„(macroscopic units)ëŠ” ì¢…ì¢… ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ê³ ë ¤ë˜ë©°, ê·¸ ìì²´ê°€ ë¯¸ì‹œì  ë‹¨ìœ„ë¡œ ì„¸ë¶„ëœë‹¤.        




**[tokens]**    
* ì¼ë°˜ì ìœ¼ë¡œ í† í°ì´ë¼ê³  ë¶ˆë¦¬ëŠ” íƒ€ì´í¬ê·¸ë˜í”¼ ë‹¨ìœ„(typographic units)ëŠ” ì–¸ì–´ì ìœ¼ë¡œ ë™ê¸°í™”ëœ ë‹¨ìœ„ì— ëŒ€í•œ ê·¼ì‚¬ì¹˜ë¡œ ì‚¬ìš©ë˜ì–´ ì™”ë‹¤.     
* MAF(í˜•íƒœí•™ì  ì£¼ì„ í”„ë ˆì„ì›Œí¬)ëŠ” í† í°ì„ "â€œnon-empty contiguous sequence of graphemes or phonemes in a document."ìœ¼ë¡œ ì •ì˜í•œë‹¤.    
* ì˜ˆë¥¼ ë“¤ì–´, í˜„ì¬ ë¼í‹´ì–´ ìŠ¤í¬ë¦½íŠ¸ì™€ í•¨ê»˜ ë³´í¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” **ê³µë°±**ê³¼ ê°™ì€ **íƒ€ì´í¬ê·¸ë˜í”¼ êµ¬ë¶„ì**ë¥¼ ì‚¬ìš©í•˜ëŠ” ì“°ê¸° ì‹œìŠ¤í…œì˜ ê²½ìš°, í† í°ì€ ë„ë¦¬ ì‚¬ìš©ë˜ì—ˆìœ¼ë©° **ë¹„ë¬¸êµ¬ ë¹„ë°±ìƒ‰ ê³µê°„ ë§ˆí¬** ë˜ëŠ” **êµ¬ë‘ì ì˜ ì—°ì† ì‹œí€€ìŠ¤**ë¡œ ì •ì˜ë˜ì—ˆë‹¤.    
* íŠ¹ì • ë¬¸ì¥ ë¶€í˜¸(ì˜ˆ: í•˜ì´í”ˆ ë˜ëŠ” ì•„í¬ìŠ¤íŠ¸ë¡œí”¼)ë¥¼ í† í°ìœ¼ë¡œ ì„ì˜ë¡œ ì •í•˜ë©´, ê·¸ëŸ¬í•œ ì •ì˜ëŠ” **ë¬¸ì¥**ì„ í† í°ì„ ê¸°ì¤€ìœ¼ë¡œ **ì›ì ë‹¨ìœ„ë¡œ ë¶„í• **í•œë‹¤.     
* í† í°ê³¼ ë‹¨ì–´ í˜•íƒœ ì‚¬ì´ì— **ì¼ëŒ€ì¼ ëŒ€ì‘ì´ ì—†ë‹¤**("ë©€í‹°í†¡ ë‹¨ì–´", "ë©€í‹°ì›Œë“œ í† í°")    
    * ë‹¨ì–´ í˜•íƒœëŠ” ì—¬ëŸ¬ ê°œì˜ í† í°(ì˜ˆ:French or English sine die)ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.     
    * ë°˜ë©´, ì—¬ëŸ¬ ê°œì˜ ë‹¨ì–´ í˜•íƒœëŠ” ë™ì¼í•œ í† í°(ì˜ˆ: ì˜ì–´ don't = do + not, ìŠ¤í˜ì¸ì–´ damÃ©lo + da + lo)ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.     

**[tokensì˜ ì¬ì •ì˜ì˜ í•„ìš”ì„±: word-formsì„ ìœ„í•´]**      
* ìµœê·¼ ë¬¸ì¥ì´ ì›ì ë‹¨ìœ„ë¡œ ë¶„í• ë˜ëŠ” ë°©ì‹ì´ ì§„í™”í•˜ì—¬ **í† í°í™” ê°œë…ì´ ì¬ì •ì˜ë˜**ì—ˆë‹¤.    
* **í•„ìš”ì„±:** ê³¼í•™ì  ê²°ê³¼(ì˜ˆ: í•˜ìœ„ ë‹¨ì–´ ë¶„í• ì´ ê¸°ê³„ ë²ˆì—­ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì™€ ê¸°ìˆ ì  ìš”êµ¬ ì‚¬í•­(ì˜ˆ: ê³ ì • í¬ê¸° ì–´íœ˜ê°€ í•„ìš”í•œ BERTì™€ ê°™ì€ ì–¸ì–´ ëª¨ë¸) ëª¨ë‘ì— ê¸°ì´ˆí•˜ì—¬, ì›ì ì²˜ë¦¬ ì¥ì¹˜(ì•„ì§ í† í°ì´ë¼ê³  í•¨)ê°€ **ë‹¨ì–´ í˜•íƒœ(word-forms)ì˜ ê·¼ì‚¬ì¹˜ê°€ ë  í•„ìš”**ê°€ ìˆë‹¤.    
* âŒ **ë¬¸ì œ**: í‡´ìƒ‰í•œ ê²°ê³¼ì ìœ¼ë¡œ, í˜„ì¬ì˜ NLPì—ì„œ í† í°ì˜ ê°œë…ì€ ì—¬ì „íˆ MAF ì •ì˜ì™€ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ì§€ë§Œ, ë” ì´ìƒ **íƒ€ì´í¬ê·¸ë˜í”¼ ë‹¨ìœ„ì˜ ì „í†µì ì¸ ì •ì˜ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤**.     


**[í† í°í™”(Tokenization)]**     
* **ì •ì˜:** ë¬¸ì¥ì„ ì •í˜•ì ì´ì§€ ì•Šì€ (ê·¸ë¦¬ê³  ì‹¤ì œë¡œ ì–¸ì–´ì ì´ì§€ ì•Šì€) ë™ê¸°í™”ëœ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ì‘ì—…     
* ì´ëŠ” ì¢…ì¢… ê³ ì „ì ì¸ í† í°ê³¼ ë‹¨ì–´ í˜•íƒœë³´ë‹¤ ì‘ê¸° ë•Œë¬¸ì— ì¢…ì¢… **subword**ë¼ê³  ë¶ˆë¦°ë‹¤.     
* Typographic ë‹¨ìœ„("ì˜¤ë˜ëœ" í† í°)ëŠ” í˜„ì¬ ì¢…ì¢… "pre-tokens"ë¼ê³  ë¶ˆë¦°ë‹¤.      
â¡ ë”°ë¼ì„œ "tokenization"ë¼ê³  ë¶ˆë¦¬ë˜ ê²ƒì€ ì˜¤ëŠ˜ë‚  "pretokenization"ë¼ê³  ë¶ˆë¦°ë‹¤.      
* **pretokenization**: ì´ ìš©ì–´ëŠ” í† í°í™”ì˜ ìƒˆë¡œìš´ ê°œë…ì— ëŒ€í•œ ì²« ë²ˆì§¸ ì ‘ê·¼ë²•ì´ **resulting ë‹¨ìœ„**(ì´ì „ì—ëŠ” â€œtokensâ€, ì§€ê¸ˆì€  â€œpretokensâ€)ë¥¼ â€œsub-wordsâ€ë¡œ **ë¶„í• í•˜ê¸° ì „**ì— **ë¬¸ì¥ì„ ì ì ˆí•œ Typographic ë‹¨ìœ„**(ì˜ˆ: í† í°í™”ì˜ "ì˜›" ê°œë…)**ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì„ í¬í•¨**í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì— ì˜í•´ ë™ê¸° ë¶€ì—¬ëœë‹¤.     


-----
------


# **3 Pre-tokenization yields word-like typographic units**    
ëª¨ë“  NLP ì‘ì—…ì—ì„œ ë‹¤ì†Œ ë§¤ë ¥ì ì´ì§€ ì•Šì€ ë¶€ë¶„ì´ ì—­ì‚¬ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë‹¤ë£¨ì–´ì¡ŒëŠ”ì§€ë¥¼ ë³´ì—¬ì¤Œ

**[ì˜› í† í°ì˜ ì‚¬ìš©ë°©ì‹]**       
* purely typographic tokenì˜ ì–¸ì–´ì  ë¬´ê´€í•¨(linguistic irrelevance)     
* í…ìŠ¤íŠ¸ë¥¼ linguistically motivatedëœ ë‹¨ì–´ í˜•íƒœë¡œ ìë™ ë¶„í• í•˜ëŠ” ê²ƒì˜ ì–´ë ¤ì›€(difficulty of automatically splitting a text into linguistically motivated word-forms)     
* **ìœ„ ë‘ ë¬¸ì œì˜ ì ˆì¶©ì•ˆ:**  purely typographic tokenê³¼ purely linguistic word-formsì˜ ì¤‘ê°„ ë‹¨ìœ„ê°€ ë„ë¦¬ ì‚¬ìš©ë˜ì–´ ì™”ë‹¤.   

**[â€œpre-tokenizersâ€ì˜ ì—­ì‚¬]**         
* **ìš©ì–´ ë³€í™”**     
   * ì´ˆê¸°ì—ëŠ” â€œtokenizersâ€ë¡œ ì•Œë ¤ì§     
   * sub-word í† í°í™”ì˜ í™•ì‚° ì´í›„ â€œpretokenâ€ìœ¼ë¡œ ë°”ë€œ    
   * ì˜¤ëŠ˜ë‚ ì—ëŠ” â€œpre-tokenizersâ€ë¡œ ëª…ì¹­ ë°”ë€œ         
* **ëŒ€í‘œ ëª¨ë¸**      
   * ì´ëŸ¬í•œ â€œpre-tokenizersâ€ ì¤‘ ì¼ë¶€ëŠ” ë¹„êµì  ë‹¨ìˆœí•˜ê³  typographic tokenì— ì¶©ì‹¤í•˜ë‹¤.      
   * Hugging Faceì˜ í† í°ë¼ì´ì € íŒ¨í‚¤ì§€ì— ìˆëŠ” ì˜¤ë˜ëœ ```Moses (Kohn et al., 2007) tokenizer6``` ê°€ ì´ˆê¸°ì— ë„ë¦¬ ì‚¬ìš©ë¨           
   * ê·¸ë¦¬ê³  ë” ìµœê·¼ì€ Hugging Faceì˜ Tokenizers packageì— ìˆëŠ”```pre-tokenizers package```ê°€ ìˆë‹¤.      
 
**[â€œpre-tokenizationâ€ ê°œë…ì˜ì˜ íƒ„ìƒê³¼ ë¬¸ì œì ]**     
* ìœ„ì˜ ëŒ€í‘œ ëª¨ë¸ë“¤ì˜ ì‚¬ìš©ì€ "tokenization(í˜„ì¬ëŠ” â€œpre-tokenizationâ€)"ë¼ëŠ” ë‹¨ì–´ë¥¼ ë§Œë“¤ì–´ëƒˆë‹¤.     
* **"tokenization(pre-tokenization)"ì˜ ì˜ë¯¸:** ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì¥ì„ ì›ì ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ì‘ì—…ì„ ë‚˜íƒ€ë‚´ê²Œ ë¨.       
    * ì¦‰, "í›„ì† ì²˜ë¦¬ì—ì„œ ë¶„í•´ë  í•„ìš”ê°€ ì—†ëŠ” ê¸°ë³¸ ë‹¨ìœ„" ë‹¨ìœ„ê°€ typographic ë‹¨ìœ„ë³´ë‹¤ ë‹¨ì–´ í˜•íƒœì— ë” ê°€ê¹Œìš¸ ë•Œì—ë„ "tokenization"ì´ë¼ê³  ë¶ˆë¦°ë‹¤.       
    * ë” ë‚˜ì•„ê°€, í† í°í™”ìê°€ **ë¬¸ì¥ì„ ë¶„í• **í•  ë¿ë§Œ ì•„ë‹ˆë¼ **ì •ê·œí™”**, **ì² ì ìˆ˜ì •** ë˜ëŠ” **named entity detection** ëª©ì ê³¼ ê°™ì€ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•˜ëŠ” ë° ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ë¯€ë¡œ í˜„ì¬ í† í°ì˜ í‘œì¤€ ì •ì˜ì—ì„œ ë²—ì–´ë‚œë‹¤.      
* âŒ **ë¬¸ì œì **     
    * ì´ëŸ¬í•œ ì´ˆê¸° í† í°ì˜ ì •ì˜ì™€ ì—­í• ì€ ì •ê·œí™” ì‘ì—…ì´ë‚˜ ë‹¤ë¥¸ ê³µë°± ê¸°í˜¸ì˜ í†µí•© ë° ë³‘í•©ì€ **í† í°í™” ì „ìœ¼ë¡œ ë˜ëŒë¦´ ìˆ˜ ì—†ê²Œí•œë‹¤**.     
    * í† í°í™”ëœ ì¶œë ¥ì—ì„œ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ í™•ì‹¤í•˜ê²Œ ë³µêµ¬í•  ìˆ˜ ì—†ë‹¤.   
  
  
 
------
-----


# **4. Augmenting word-level pretokenizer tokens with character information**


[word-level ëª¨ë¸ì˜ íŠ¹ì§•]       
* ê°œë…ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰¬ì›€    
* neural eraì—ì„œëŠ” í•´ì„ ê°€ëŠ¥í•œ ì„¸ë¶„í™”ëœ ê¸°ëŠ¥ì„ ì œê³µ      
* **ë‹¨ì **: íì‡„í˜• ì–´íœ˜ ëª¨ë¸: í›ˆë ¨ ì¤‘ì— ê±°ì˜ ë³´ì´ì§€ ì•ŠëŠ” í¬ê·€í•˜ê³  ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒ    
* **í•´ê²°ì±…:** ì—­ì‚¬ì ìœ¼ë¡œ í¬ê·€í•œ ë‹¨ì–´ ìœ í˜•ì€ í›ˆë ¨ ì‹œ ìƒˆë¡œìš´ ë‹¨ì–´ ìœ í˜• ```UNK(ì•Œ ìˆ˜ ì—†ìŒ)```ë¡œ ëŒ€ì²´ë¨     


**[```UNK```ëŒ€ì²´ ë°©ì‹ì˜ ë‹¨ì ]**         
* ìì—°ì–´ ìƒì„±(NLG)ì„ ìˆ˜í–‰í•  ë•Œ UNKê°€ í—ˆìš©ë˜ì§€ ì•ŠëŠ”ë‹¤   
* ELMo ë˜ëŠ” BERTì™€ ê°™ì€ large-scale ëª¨ë¸ì— ì‚¬ìš©ë  ë•Œ ì¼íšŒì„± ì´ë²¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ìœ ìš©í•œ ì˜ë¯¸ì˜ ì•µì»¤ì¸ **ìƒˆë¡œìš´ ë‹¨ì–´ì— ëŒ€í•œ ê¸°ëŠ¥**ì„ **ì¶”ì¶œí•  ìˆ˜ ì—†ë‹¤**        
* ì˜ì–´ ì´ì™¸ì˜ ì–¸ì–´, íŠ¹íˆ ë” ìƒì‚°ì ì¸ í˜•íƒœí•™ ë° ë”°ë¼ì„œ ìœ í˜• í† í° ë¹„ìœ¨ì´ ë†’ì€ ì–¸ì–´ì—ì„œ í¬ê·€ ë‹¨ì–´ë¥¼ ì œê±°í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥     

ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , **ë‹¨ì–´**ê°€ ì–¸ì–´ì˜ **ê¸°ë³¸ ë‹¨ìœ„**ì´ê¸° ë•Œë¬¸ì—,       
**ë‹¨ì–´ë¥¼ êµ¬ì„±í•˜ëŠ” ë¬¸ìë¥¼ ê¸°ë°˜**ìœ¼ë¡œ ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¨ì–´ ê¸°ë°˜ í‹€ì—ì„œ í¬ê·€í•˜ê³  ìƒˆë¡œìš´ ë‹¨ì–´ì˜ **ì²˜ë¦¬ë¥¼ ê°œì„ **í•˜ëŠ” ì—¬ëŸ¬ ì ‘ê·¼ë²•ì´ ë“±ì¥í–ˆë‹¤.      
ìš°ë¦¬ëŠ” ë‹¨ì–´ í‘œí˜„ì˜ ë³´ë‹¤ í¬ê´„ì ì¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì´ ì„¹ì…˜ì—ì„œ ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ ì¤‘ ì¼ë¶€ë¥¼ ì œì‹œí•  ê²ƒì´ë‹¤.       

-----


## 4.1 Augmenting word-level models with spelling information    
The idea of somehow using information about
the spellings of a word to inform the wordâ€™s representations of course is decades old. In neural models for language, research in the 90s and
2000s often forewent the focus on words altogether
and processed strings of characters instead (see
Â§8 and Â§4.2), but as soon as neural models became important in NLP, combinations of word- and
character-level information for use in neural networks emerged there, too.
Dos Santos and Zadrozny (2014) first proposed
to use information about the words themselves to
aid word embedding estimation. Soon thereafter
Ling et al. (2015), Kim et al. (2016), and Jozefowicz et al. (2016) popularized the idea of deterministically constructing a wordâ€™s embedding from its
spelling,10 both for textual input as well as for generative language modeling, that is, prediction of
strings. However, even when replacing embedding
matrices with convolutional neural network (CNN)
layers, their generative models are still closedvocabulary, meaning they can only predict words
that were seen (often enough) in training data, so
the CNN construction only helps with rare words,
not novel words. Furthermore, constructing embeddings from spellings for each token (as opposed
to every type like Mielke and Eisner (2018), see
Â§4.2) implicitly trains the CNN-powered embedding function to â€œget frequent words rightâ€ instead
of anticipating novel words, an issue discussed in
Mielke and Eisner (2018). Similar constructions
led to advances in other classic NLP tasks like POS
tagging (Plank et al., 2016) and ultimately powered the first big contextual word embedding model
ELMo (Peters et al., 2018).
The popular fastText embeddings (Bojanowski
et al., 2017) propose constructing word embeddings not from characters, but from overlapping
ğ‘›-grams, allowing one to obtain embeddings for
novel words (making it â€œopen-vocabularyâ€ in that
sense, though not in the generative sense). Ataman
and Federico (2018a) likewise obtain better performance on machine translation by using (overlapping) ğ‘›-grams instead of characters (also beating
BPE on morphologically rich languages).
In more recent times, El Boukkouri et al. (2020,
CharacterBERT) and Ma et al. (2020, CharBERT)
use the same CNN construction as in Kim et al.
(2016) on a modern BERT-style model, this time
enhancing the BPE unitsâ€™ embedding with their
constituent charactersâ€™ embedding, motivated by
better handling noisy texts with spelling errors or
transferring to new domains like medical text; concurrently, Aguilar et al. (2021) do almost the same,
but using a small Transformer instead of CNNs.
Finally, construction-based approaches have also
been integrated into pretrained word-level input
models. Specifically, Pinter et al. (2017) learn a
model that is trained to mimic the embedding of a
word given its spelling using a helper RNN model
that is called whenever an unknown word appears
during test time.


---


## **4.2 Open-vocabulary language modeling with (tokenizer-defined) words made of characters**
Extending closed-vocabulary generative models to
open-vocabulary models, i.e., those that can predict
and generate novel words at test time, is somewhat
more difficult than being open-vocabulary on the
input side because it must be possible to hold out
probability mass for the infinite set of sentences
that contain completely novel words.
Inspired by Luong and Manning (2016), Mielke
and Eisner (2018) propose a probabilistic twostage model that essentially augments the ordinary
closed-vocab word-level recurrent neural network
language model (RNNLM) setup by regularizing
word embeddings to be predictive of their spellings
using a smaller character-level RNNLM and using that smaller model to generate novel words
on the fly whenever the word-level RNNLM pre
dicts UNK, yielding an open-vocabulary model motivated by linguistic notions and intuitive modeling
and proven successful qualitatively and quantitatively.
Independently developed, the model of
Kawakami et al. (2017) follows a similar two-level
setup of word- and character-level RNN, but
where each word has to be spelled out using
a character-level RNN if it cannot be directly
copied from the recent past using a cache model
(Grave et al., 2016).11 Their analysis shows clearly
that the cache model not only copies â€œburstyâ€
unknown words like Noriega (Church, 2000),
but also extremely common function words like
the in an attempt to keep itself from forgetting
them. The idea is picked up by Ataman et al.
(2019) for a machine translation decoder (creating
word embeddings on the encoder side from
character-level BiRNNs as in ELMo (Peters et al.,
2018, see Â§4.1)) and later extended by Ataman
et al. (2020) with some additional stochasticity that
is intended to pick up on lemmata and inflections
unsupervisedly.
A different approach is having higher layers of
multi-layer RNNs run at lower speed (skipping updates to the hidden state) This is an old idea, first
present in El Hihi and Bengio (1995) (building
on Schmidhuber (1991, 1992)â€™s â€œneural sequence
chunkerâ€) and revived in Koutnik et al. (2014) for
fixed-frequency skipping and Hwang and Sung
(2017) for skipping on word boundaries (which are
assumed to be observed).12 This approach leads
to the first of a number of ways in which we can
actually learn word boundaries and thus segmentations.
















