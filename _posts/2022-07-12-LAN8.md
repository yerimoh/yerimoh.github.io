---
title: "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP ì •ë¦¬"
date:   2022-07-12
excerpt: "Between words and characters:A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---

# intro
 
      

## í•µì‹¬  

## ì½ê¸° ìœ„í•´ í•„ìš”í•œ ì§€ì‹
* [word2vec](https://yerimoh.github.io/DL14/): baseline ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ê¼­ ì•Œì•„ì•¼ í•œë‹¤.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec ì†ë„ ê°œì„ ìœ¼ë¡œ ì´ í¬ìŠ¤íŒ…ë„ ê¼­ ì•Œì•„ì•¼ í•œë‹¤.      

## ì› ë…¼ë¬¸
[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/2112.10508)


---

# ëª©ì°¨  


---


# **Abstract**
ìš°ë¦¬ê°€ ëª¨ë¸ë§í•˜ê³ ì í•˜ëŠ” í…ìŠ¤íŠ¸ì˜ ë‹¨ìœ„ëŠ”  
bytesì—ì„œ ë‹¤ì¤‘ ë‹¨ì–´ í‘œí˜„ì‹(multi-word expressions)ì— ì´ë¥´ê¸°ê¹Œì§€, í…ìŠ¤íŠ¸ëŠ” ë‹¤ì–‘í•œ ì„¸ë¶„ì„±ìœ¼ë¡œ ë¶„ì„ë˜ê³  ìƒì„±ë  ìˆ˜ ìˆë‹¤.      

ìµœê·¼ê¹Œì§€ ëŒ€ë¶€ë¶„ì˜ ìì—°ì–´ ì²˜ë¦¬(NLP) ëª¨ë¸ì€ ë‹¨ì–´ë¥¼ í†µí•´ ì‘ë™í•˜ì—¬ discrete ë°  atomic tokensìœ¼ë¡œ ì²˜ë¦¬í–ˆì§€ë§Œ,   
byte-pair encoding(BPE)ì„ ì‹œì‘ìœ¼ë¡œ ë§ì€ ì˜ì—­ì—ì„œ í•˜ìœ„ ë‹¨ì–´ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì´ ìš°ì„¸í•˜ì—¬ **ì‘ì€ ì–´íœ˜ë¥¼ ì‚¬ìš©**í•˜ë©´ì„œë„ **ë¹ ë¥¸ ì¶”ë¡ **ì„ í—ˆìš©í•˜ê³  ìˆë‹¤.      

ì´ í† í¬ë‚˜ì´ì§• ë°©ë²•ë“¤ì— ëŒ€í•´ í•™ìŠµëœ ì„¸ë¶„í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ **subword-based approaches**ë¿ë§Œ ì•„ë‹ˆë¼ **ë‹¨ì–´ì™€ ë¬¸ìì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ë°©ì‹**ì´ **ì–´ë–»ê²Œ ì œì•ˆë˜ê³  í‰ê°€**ë˜ì—ˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤Œìœ¼ë¡œì¨,     
ì‚¬ì „ ì‹ ê²½ ë° ì‹ ê²½ ì‹œëŒ€ì˜ ì—¬ëŸ¬ ì‘ì—… ë¼ì¸ì„ ì—°ê²°í•œë‹¤.     

ë³¸ ë…¼ë¬¸ì€ ëª¨ë“  ì• í”Œë¦¬ì¼€ì´ì…˜ì— ëŒ€í•´ **íŠ¹ë³„í•œ í•´ê²°ì±…**ì´ ê²°ì½” **ì—†ì„ ê²ƒ**ì´ë©°,   
í† í°í™” ë°©ë²•ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì—¬ì „íˆ ì¤‘ìš”í•˜ë‹¤ê³  ê²°ë¡ ì§“ëŠ”ë‹¤.    


-----
-----

# **1 Introduction**    


_â€œâ€˜tokensâ€™ are not a real thing. they are a computer generated illusion created by a clever engineerâ€ dril_gpt1         


ìš°ë¦¬ê°€ ì‚¬ëŒë“¤ì—ê²Œ NLP ëª¨ë¸ì„ ì²˜ìŒ ì†Œê°œí•  ë•Œ,    
ìš°ë¦¬ëŠ” ì¢…ì¢… í…ìŠ¤íŠ¸ê°€ ì»´í“¨í„°ì— ì‘ì€ ì¡°ê°ë“¤ë¡œ ì˜ë ¤ë‚˜ê°„ë‹¤ëŠ” ìƒê°ì„ ë‹¹ì—°í•˜ê²Œ ë°›ì•„ë“¤ì¸ë‹¤.    
â¡ ê²°êµ­ ê·¸ê²ƒì€ ë‹¨ì§€ ì¼ë ¨ì˜ ì •ìˆ˜ì¼ ë¿ì´ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì´ê²ƒë“¤ì„ (ë³´í†µ) ì—°ì† **sub-strings tokens**ì´ë¼ê³  ë¶€ë¥¸ë‹¤. 


êµìœ¡ í™˜ê²½ì—ì„œ, ê·¸ë¦¬ê³  ì‚¬ì‹¤ ì—­ì‚¬ì ìœ¼ë¡œ NLPì—ì„œ ì´ëŸ¬í•œ í† í°ì€ ë‹¤ì†Œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¨ì–´(ì²˜ìŒì—ëŠ” ì˜ì–´ë¡œ â€œspace-separated substringsâ€)ë¡œ ì•”ì‹œëœë‹¤. 


**[ë‹¨ì–´ì—ì„œ êµ¬ë‘ì  ë¶„ë¦¬ì˜ ì–´ë ¤ì›€]**            
* "don't"ë¼ëŠ” ë‹¨ì–´ë¥¼ ì˜ˆë¡œ ë“¤ë©´,    
* ì´ ë‹¨ì–´ì˜ êµ¬ë‘ì  ë¶„í• ì˜ ê²½ìš°, ```"don"``` ```"'"``` ```"t"```ë¼ëŠ” ë‹¤ì†Œ ë¬´ì˜ë¯¸í•œ ì„¸ ê°œì˜ í† í°ì„ ì–»ê²Œ ë  ê²ƒì´ë‹¤.            
* ê·¸ëŸ°ë° ë” íš¨ê³¼ì ì¸ í† í¬ë‚˜ì´ì§•ì„ í•˜ë ¤ë©´ ```"do"```ì™€ ```"n't"```ì˜ ë‘ ë‹¨ìœ„ë¥¼ ì‚°ì¶œí•´ì•¼ í•œë‹¤ê³  ì£¼ì¥í•  ìˆ˜ ìˆë‹¤.       

---

## ë…¼ë¬¸ ê°œìš”    
ì´ surveyëŠ” $$tokenization$$ì— ëŒ€í•œ ìœ„ì™€ ê°™ì€ ì§ˆë¬¸ì„ ë‹¤ë£¨ë©°,      
ë³¸ ë…¼ë¬¸ì€ ì˜ ê° ì±•í„°ì˜ í•µì‹¬ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ë‹¤,    

**[2: Tokens, word forms, and sub words](# 2-Tokens,-word-forms,-and-sub-words)**      
* ê·¼ë³¸ì ì¸ ì§ˆë¬¸ê³¼ ìš©ì–´ë¥¼ ìƒì„¸íˆ ì„¤ëª…     

**[3: Pre-tokenization yields word-like typographic units](# 3-Pre-tokenization-yields-word-like-typographic-units)**      
* ëª¨ë“  NLP ì‘ì—…ì—ì„œ ë‹¤ì†Œ ë§¤ë ¥ì ì´ì§€ ì•Šì€ ë¶€ë¶„ì´ ì—­ì‚¬ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë‹¤ë£¨ì–´ì¡ŒëŠ”ì§€ë¥¼ ë³´ì—¬ì¤Œ     

íŠ¹íˆ ì§€ë‚œ 5ë…„ ë™ì•ˆ, ë‹¤ì†Œ ì›ìì ì¸ ë‹¨ì–´ì™€ ê°™ì€ ê³µê°„ ë¶„ë¦¬ ë‹¨ìœ„ë¡œì„œì˜ **"í† í°"ì— ëŒ€í•œ ì§ê´€ì ì¸ ì •ì˜ë¥¼ ë„˜ì–´ì„œëŠ” ê²ƒ**ì— ëŒ€í•œ ê´€ì‹¬ì´ ìƒˆë¡œì›Œì¡Œë‹¤.  
* **4**: ì´ë¥¼ ìœ„í•œ í•œ ê°€ì§€ ë°©ë²•ì¸ **ë‹¨ì–´ ë‚´ë¶€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ì™€ ê°™ì€ ë‹¨ìœ„ë¥¼ ë³´ê°•**í•˜ëŠ” ë°©ë²•.    
* **5**: ì‹ ê²½ ëª¨ë¸ë§ì€ ì´ë¥¼ ê·¸ ì–´ëŠ ë•Œë³´ë‹¤ ì‰½ê²Œ ë§Œë“¤ì–´ ëª…ë°±í•œ í‘œì‹œ ì—†ì´ ë‹¨ì–´ ê²½ê³„ê¹Œì§€ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ë¡œ ì´ì–´ì§„ë‹¤(ì˜ˆ: ê³µë°±ì´ ìˆëŠ” ê²½ìš°).  unsupervised word segmentation or discoveryì˜ ê°œë…ì€ ìˆ˜ì‹­ ë…„ì˜ ì‘ì—…ì—ì„œ ìì²´ì ìœ¼ë¡œ ì¡´ì¬í–ˆì§€ë§Œ,    
* **6**: **subword** ë‹¨ìœ„ë¥¼ ì›ì í† í°ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” í˜„ì¬ ë„ë¦¬ í¼ì ¸ ìˆëŠ” ì•„ì´ë””ì–´ë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì‚´í´ë³¼ ë•Œ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ìœ ìš©í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œê²Œ ë  ê²ƒì´ë‹¤.      
* **7**: ë‹¤êµ­ì–´ ì–´íœ˜ì˜ ê³µìœ ì™€ ê²½ìŸì˜ ëª‡ ê°€ì§€ ë¬¸ì œ ì‚´í´ë´„     
* **8**: ë¬¸ì, ë°”ì´íŠ¸ ë˜ëŠ” ì‹¬ì§€ì–´ í”½ì…€ë¡œ ìµœëŒ€ ë¶„í•´í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ê°„ë‹¨í•œ í† í°í™”ë¥¼ ì‚¬ìš©í•´ë´„        


ì´ ë…¼ë¬¸ì´ ëë‚œ í›„,       
* ë°”ì´íŠ¸ì˜ ì†ì‰¬ìš´ ì†”ë£¨ì…˜ì´ ë„ë‹¬í•´ ë³´ì´ëŠ” 2021ë…„ í˜„ì¬ì—ë„ "ë³µì¡í•œ" í† í°í™”ê°€ ì˜ ì¤‘ìš”ì„±ì„ ë…¼ì¦í•˜ì—¬ ë§ˆë¬´ë¦¬ í•œë‹¤.     
* ìµœê·¼ì˜ ë°œì „ì€ íŠ¹ì • ë„ë©”ì¸ê³¼ ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•´ ìµœëŒ€ ë¶„í•´ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì§€ë§Œ, ê·¸ê²ƒë“¤ì€ ê´‘ë²”ìœ„í•œ NLP ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‹¤ë£¨ì§€ ì•Šê³  **ìì²´ì˜ ë‹¨ì ê³¼ í¸ê²¬**ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ì£¼ì¥í•  ê²ƒì´ë‹¤.     


![image](https://user-images.githubusercontent.com/76824611/178999180-03d114b4-8ba7-4748-afce-5115faccc2b5.png)
A taxonomy of segmentation and tokenization algorithms and research directions


------
-----











# **2 Tokens, word-forms, and sub-words** 
ì´ ì±•í„°ì—ì„œëŠ” tokenì— ëŒ€í•œ ê·¼ë³¸ì ì¸ ì§ˆë¬¸ê³¼ ìš©ì–´ë¥¼ ì„¤ëª…í•œë‹¤.      

í† í°, ë‹¨ì–´ í˜•íƒœ ë° í•˜ìœ„ ë‹¨ì–´ NLPì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” 
"ë¬¸ì¥"(sentences)ê³¼ "ë‹¨ì–´(words)"ë¡œ ë¶„í• ë˜ì—ˆë‹¤. 

**[sentences]**     
* ê±°ì‹œì  ë‹¨ìœ„(macroscopic units)ëŠ” ì¢…ì¢… ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ê³ ë ¤ë˜ë©°, ê·¸ ìì²´ê°€ ë¯¸ì‹œì  ë‹¨ìœ„ë¡œ ì„¸ë¶„ëœë‹¤.        




**[tokens]**    
* ì¼ë°˜ì ìœ¼ë¡œ í† í°ì´ë¼ê³  ë¶ˆë¦¬ëŠ” íƒ€ì´í¬ê·¸ë˜í”¼ ë‹¨ìœ„(typographic units)ëŠ” ì–¸ì–´ì ìœ¼ë¡œ ë™ê¸°í™”ëœ ë‹¨ìœ„ì— ëŒ€í•œ ê·¼ì‚¬ì¹˜ë¡œ ì‚¬ìš©ë˜ì–´ ì™”ë‹¤.     
* MAF(í˜•íƒœí•™ì  ì£¼ì„ í”„ë ˆì„ì›Œí¬)ëŠ” í† í°ì„ "â€œnon-empty contiguous sequence of graphemes or phonemes in a document."ìœ¼ë¡œ ì •ì˜í•œë‹¤.    
* ì˜ˆë¥¼ ë“¤ì–´, í˜„ì¬ ë¼í‹´ì–´ ìŠ¤í¬ë¦½íŠ¸ì™€ í•¨ê»˜ ë³´í¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” **ê³µë°±**ê³¼ ê°™ì€ **íƒ€ì´í¬ê·¸ë˜í”¼ êµ¬ë¶„ì**ë¥¼ ì‚¬ìš©í•˜ëŠ” ì“°ê¸° ì‹œìŠ¤í…œì˜ ê²½ìš°, í† í°ì€ ë„ë¦¬ ì‚¬ìš©ë˜ì—ˆìœ¼ë©° **ë¹„ë¬¸êµ¬ ë¹„ë°±ìƒ‰ ê³µê°„ ë§ˆí¬** ë˜ëŠ” **êµ¬ë‘ì ì˜ ì—°ì† ì‹œí€€ìŠ¤**ë¡œ ì •ì˜ë˜ì—ˆë‹¤.    
* íŠ¹ì • ë¬¸ì¥ ë¶€í˜¸(ì˜ˆ: í•˜ì´í”ˆ ë˜ëŠ” ì•„í¬ìŠ¤íŠ¸ë¡œí”¼)ë¥¼ í† í°ìœ¼ë¡œ ì„ì˜ë¡œ ì •í•˜ë©´, ê·¸ëŸ¬í•œ ì •ì˜ëŠ” **ë¬¸ì¥**ì„ í† í°ì„ ê¸°ì¤€ìœ¼ë¡œ **ì›ì ë‹¨ìœ„ë¡œ ë¶„í• **í•œë‹¤.     
* í† í°ê³¼ ë‹¨ì–´ í˜•íƒœ ì‚¬ì´ì— **ì¼ëŒ€ì¼ ëŒ€ì‘ì´ ì—†ë‹¤**("ë©€í‹°í†¡ ë‹¨ì–´", "ë©€í‹°ì›Œë“œ í† í°")    
    * ë‹¨ì–´ í˜•íƒœëŠ” ì—¬ëŸ¬ ê°œì˜ í† í°(ì˜ˆ:French or English sine die)ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.     
    * ë°˜ë©´, ì—¬ëŸ¬ ê°œì˜ ë‹¨ì–´ í˜•íƒœëŠ” ë™ì¼í•œ í† í°(ì˜ˆ: ì˜ì–´ don't = do + not, ìŠ¤í˜ì¸ì–´ damÃ©lo + da + lo)ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.     

**[tokensì˜ ì¬ì •ì˜ì˜ í•„ìš”ì„±: word-formsì„ ìœ„í•´]**      
* ìµœê·¼ ë¬¸ì¥ì´ ì›ì ë‹¨ìœ„ë¡œ ë¶„í• ë˜ëŠ” ë°©ì‹ì´ ì§„í™”í•˜ì—¬ **í† í°í™” ê°œë…ì´ ì¬ì •ì˜ë˜**ì—ˆë‹¤.    
* **í•„ìš”ì„±:** ê³¼í•™ì  ê²°ê³¼(ì˜ˆ: í•˜ìœ„ ë‹¨ì–´ ë¶„í• ì´ ê¸°ê³„ ë²ˆì—­ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì™€ ê¸°ìˆ ì  ìš”êµ¬ ì‚¬í•­(ì˜ˆ: ê³ ì • í¬ê¸° ì–´íœ˜ê°€ í•„ìš”í•œ BERTì™€ ê°™ì€ ì–¸ì–´ ëª¨ë¸) ëª¨ë‘ì— ê¸°ì´ˆí•˜ì—¬, ì›ì ì²˜ë¦¬ ì¥ì¹˜(ì•„ì§ í† í°ì´ë¼ê³  í•¨)ê°€ **ë‹¨ì–´ í˜•íƒœ(word-forms)ì˜ ê·¼ì‚¬ì¹˜ê°€ ë  í•„ìš”**ê°€ ìˆë‹¤.    
* âŒ **ë¬¸ì œ**: í‡´ìƒ‰í•œ ê²°ê³¼ì ìœ¼ë¡œ, í˜„ì¬ì˜ NLPì—ì„œ í† í°ì˜ ê°œë…ì€ ì—¬ì „íˆ MAF ì •ì˜ì™€ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ì§€ë§Œ, ë” ì´ìƒ **íƒ€ì´í¬ê·¸ë˜í”¼ ë‹¨ìœ„ì˜ ì „í†µì ì¸ ì •ì˜ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤**.     


**[í† í°í™”(Tokenization)]**     
* **ì •ì˜:** ë¬¸ì¥ì„ ì •í˜•ì ì´ì§€ ì•Šì€ (ê·¸ë¦¬ê³  ì‹¤ì œë¡œ ì–¸ì–´ì ì´ì§€ ì•Šì€) ë™ê¸°í™”ëœ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ì‘ì—…     
* ì´ëŠ” ì¢…ì¢… ê³ ì „ì ì¸ í† í°ê³¼ ë‹¨ì–´ í˜•íƒœë³´ë‹¤ ì‘ê¸° ë•Œë¬¸ì— ì¢…ì¢… **subword**ë¼ê³  ë¶ˆë¦°ë‹¤.     
* Typographic ë‹¨ìœ„("ì˜¤ë˜ëœ" í† í°)ëŠ” í˜„ì¬ ì¢…ì¢… "pre-tokens"ë¼ê³  ë¶ˆë¦°ë‹¤.      
â¡ ë”°ë¼ì„œ "tokenization"ë¼ê³  ë¶ˆë¦¬ë˜ ê²ƒì€ ì˜¤ëŠ˜ë‚  "pretokenization"ë¼ê³  ë¶ˆë¦°ë‹¤.      
* **pretokenization**: ì´ ìš©ì–´ëŠ” í† í°í™”ì˜ ìƒˆë¡œìš´ ê°œë…ì— ëŒ€í•œ ì²« ë²ˆì§¸ ì ‘ê·¼ë²•ì´ **resulting ë‹¨ìœ„**(ì´ì „ì—ëŠ” â€œtokensâ€, ì§€ê¸ˆì€  â€œpretokensâ€)ë¥¼ â€œsub-wordsâ€ë¡œ **ë¶„í• í•˜ê¸° ì „**ì— **ë¬¸ì¥ì„ ì ì ˆí•œ Typographic ë‹¨ìœ„**(ì˜ˆ: í† í°í™”ì˜ "ì˜›" ê°œë…)**ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì„ í¬í•¨**í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì— ì˜í•´ ë™ê¸° ë¶€ì—¬ëœë‹¤.     


-----
------


# **3 Pre-tokenization yields word-like typographic units**    
ëª¨ë“  NLP ì‘ì—…ì—ì„œ ë‹¤ì†Œ ë§¤ë ¥ì ì´ì§€ ì•Šì€ ë¶€ë¶„ì´ ì—­ì‚¬ì ìœ¼ë¡œ ì–´ë–»ê²Œ ë‹¤ë£¨ì–´ì¡ŒëŠ”ì§€ë¥¼ ë³´ì—¬ì¤Œ

**[ì˜› í† í°ì˜ ì‚¬ìš©ë°©ì‹]**       
* purely typographic tokenì˜ ì–¸ì–´ì  ë¬´ê´€í•¨(linguistic irrelevance)     
* í…ìŠ¤íŠ¸ë¥¼ linguistically motivatedëœ ë‹¨ì–´ í˜•íƒœë¡œ ìë™ ë¶„í• í•˜ëŠ” ê²ƒì˜ ì–´ë ¤ì›€(difficulty of automatically splitting a text into linguistically motivated word-forms)     
* **ìœ„ ë‘ ë¬¸ì œì˜ ì ˆì¶©ì•ˆ:**  purely typographic tokenê³¼ purely linguistic word-formsì˜ ì¤‘ê°„ ë‹¨ìœ„ê°€ ë„ë¦¬ ì‚¬ìš©ë˜ì–´ ì™”ë‹¤.   

**[â€œpre-tokenizersâ€ì˜ ì—­ì‚¬]**         
* **ìš©ì–´ ë³€í™”**     
   * ì´ˆê¸°ì—ëŠ” â€œtokenizersâ€ë¡œ ì•Œë ¤ì§     
   * sub-word í† í°í™”ì˜ í™•ì‚° ì´í›„ â€œpretokenâ€ìœ¼ë¡œ ë°”ë€œ    
   * ì˜¤ëŠ˜ë‚ ì—ëŠ” â€œpre-tokenizersâ€ë¡œ ëª…ì¹­ ë°”ë€œ         
* **ëŒ€í‘œ ëª¨ë¸**      
   * ì´ëŸ¬í•œ â€œpre-tokenizersâ€ ì¤‘ ì¼ë¶€ëŠ” ë¹„êµì  ë‹¨ìˆœí•˜ê³  typographic tokenì— ì¶©ì‹¤í•˜ë‹¤.      
   * Hugging Faceì˜ í† í°ë¼ì´ì € íŒ¨í‚¤ì§€ì— ìˆëŠ” ì˜¤ë˜ëœ ```Moses (Kohn et al., 2007) tokenizer6``` ê°€ ì´ˆê¸°ì— ë„ë¦¬ ì‚¬ìš©ë¨           
   * ê·¸ë¦¬ê³  ë” ìµœê·¼ì€ Hugging Faceì˜ Tokenizers packageì— ìˆëŠ”```pre-tokenizers package```ê°€ ìˆë‹¤.      
 
**[â€œpre-tokenizationâ€ ê°œë…ì˜ì˜ íƒ„ìƒê³¼ ë¬¸ì œì ]**     
* ìœ„ì˜ ëŒ€í‘œ ëª¨ë¸ë“¤ì˜ ì‚¬ìš©ì€ "tokenization(í˜„ì¬ëŠ” â€œpre-tokenizationâ€)"ë¼ëŠ” ë‹¨ì–´ë¥¼ ë§Œë“¤ì–´ëƒˆë‹¤.     
* **"tokenization(pre-tokenization)"ì˜ ì˜ë¯¸:** ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì¥ì„ ì›ì ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ì‘ì—…ì„ ë‚˜íƒ€ë‚´ê²Œ ë¨.       
    * ì¦‰, "í›„ì† ì²˜ë¦¬ì—ì„œ ë¶„í•´ë  í•„ìš”ê°€ ì—†ëŠ” ê¸°ë³¸ ë‹¨ìœ„" ë‹¨ìœ„ê°€ typographic ë‹¨ìœ„ë³´ë‹¤ ë‹¨ì–´ í˜•íƒœì— ë” ê°€ê¹Œìš¸ ë•Œì—ë„ "tokenization"ì´ë¼ê³  ë¶ˆë¦°ë‹¤.       
    * ë” ë‚˜ì•„ê°€, í† í°í™”ìê°€ **ë¬¸ì¥ì„ ë¶„í• **í•  ë¿ë§Œ ì•„ë‹ˆë¼ **ì •ê·œí™”**, **ì² ì ìˆ˜ì •** ë˜ëŠ” **named entity detection** ëª©ì ê³¼ ê°™ì€ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•˜ëŠ” ë° ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ë¯€ë¡œ í˜„ì¬ í† í°ì˜ í‘œì¤€ ì •ì˜ì—ì„œ ë²—ì–´ë‚œë‹¤.      
* âŒ **ë¬¸ì œì **     
    * ì´ëŸ¬í•œ ì´ˆê¸° í† í°ì˜ ì •ì˜ì™€ ì—­í• ì€ ì •ê·œí™” ì‘ì—…ì´ë‚˜ ë‹¤ë¥¸ ê³µë°± ê¸°í˜¸ì˜ í†µí•© ë° ë³‘í•©ì€ **í† í°í™” ì „ìœ¼ë¡œ ë˜ëŒë¦´ ìˆ˜ ì—†ê²Œí•œë‹¤**.     
    * í† í°í™”ëœ ì¶œë ¥ì—ì„œ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ í™•ì‹¤í•˜ê²Œ ë³µêµ¬í•  ìˆ˜ ì—†ë‹¤.   
  
  
 
------
-----


# **4. Augmenting word-level pretokenizer tokens with character information**
íŠ¹íˆ ì§€ë‚œ 5ë…„ ë™ì•ˆ, ë‹¤ì†Œ ì›ìì ì¸ ë‹¨ì–´ì™€ ê°™ì€ ê³µê°„ ë¶„ë¦¬ ë‹¨ìœ„ë¡œì„œì˜ **"í† í°"ì— ëŒ€í•œ ì§ê´€ì ì¸ ì •ì˜ë¥¼ ë„˜ì–´ì„œëŠ” ê²ƒ**ì— ëŒ€í•œ ê´€ì‹¬ì´ ìƒˆë¡œì›Œì¡Œë‹¤.  
ì´ ì±•í„°ì—ì„œëŠ” ì´ë¥¼ ìœ„í•œ í•œ ê°€ì§€ ë°©ë²•ì¸ **ë‹¨ì–´ ë‚´ë¶€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ì™€ ê°™ì€ ë‹¨ìœ„ë¥¼ ë³´ê°•**í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œë‹¤.     

**[word-level ëª¨ë¸ì˜ íŠ¹ì§•]**           
* ê°œë…ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰¬ì›€    
* neural eraì—ì„œëŠ” í•´ì„ ê°€ëŠ¥í•œ ì„¸ë¶„í™”ëœ ê¸°ëŠ¥ì„ ì œê³µ      
* **ë‹¨ì **: íì‡„í˜• ì–´íœ˜ ëª¨ë¸: í›ˆë ¨ ì¤‘ì— ê±°ì˜ ë³´ì´ì§€ ì•ŠëŠ” í¬ê·€í•˜ê³  ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒ    
* **í•´ê²°ì±…:** ì—­ì‚¬ì ìœ¼ë¡œ í¬ê·€í•œ ë‹¨ì–´ ìœ í˜•ì€ í›ˆë ¨ ì‹œ ìƒˆë¡œìš´ ë‹¨ì–´ ìœ í˜• ```UNK(ì•Œ ìˆ˜ ì—†ìŒ)```ë¡œ ëŒ€ì²´ë¨     


**[```UNK```ëŒ€ì²´ ë°©ì‹ì˜ ë‹¨ì ]**         
* ìì—°ì–´ ìƒì„±(NLG)ì„ ìˆ˜í–‰í•  ë•Œ UNKê°€ í—ˆìš©ë˜ì§€ ì•ŠëŠ”ë‹¤   
* ELMo ë˜ëŠ” BERTì™€ ê°™ì€ large-scale ëª¨ë¸ì— ì‚¬ìš©ë  ë•Œ ì¼íšŒì„± ì´ë²¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ìœ ìš©í•œ ì˜ë¯¸ì˜ ì•µì»¤ì¸ **ìƒˆë¡œìš´ ë‹¨ì–´ì— ëŒ€í•œ ê¸°ëŠ¥**ì„ **ì¶”ì¶œí•  ìˆ˜ ì—†ë‹¤**        
* ì˜ì–´ ì´ì™¸ì˜ ì–¸ì–´, íŠ¹íˆ ë” ìƒì‚°ì ì¸ í˜•íƒœí•™ ë° ë”°ë¼ì„œ ìœ í˜• í† í° ë¹„ìœ¨ì´ ë†’ì€ ì–¸ì–´ì—ì„œ í¬ê·€ ë‹¨ì–´ë¥¼ ì œê±°í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥     

ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , **ë‹¨ì–´**ê°€ ì–¸ì–´ì˜ **ê¸°ë³¸ ë‹¨ìœ„**ì´ê¸° ë•Œë¬¸ì—,       
**ë‹¨ì–´ë¥¼ êµ¬ì„±í•˜ëŠ” ë¬¸ìë¥¼ ê¸°ë°˜**ìœ¼ë¡œ ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¨ì–´ ê¸°ë°˜ í‹€ì—ì„œ í¬ê·€í•˜ê³  ìƒˆë¡œìš´ ë‹¨ì–´ì˜ **ì²˜ë¦¬ë¥¼ ê°œì„ **í•˜ëŠ” ì—¬ëŸ¬ ì ‘ê·¼ë²•ì´ ë“±ì¥í–ˆë‹¤.      
ìš°ë¦¬ëŠ” ë‹¨ì–´ í‘œí˜„ì˜ ë³´ë‹¤ í¬ê´„ì ì¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì´ ì„¹ì…˜ì—ì„œ ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ ì¤‘ ì¼ë¶€ë¥¼ ì œì‹œí•  ê²ƒì´ë‹¤.       

-----


## 4.1 Augmenting word-level models with spelling information    

**[ë‹¨ì–´ì— ì´ˆì ]**         
* ì–¸ì–´ë¥¼ ìœ„í•œ neural modelsì—ì„œ,    
90ë…„ëŒ€ì™€ 2000ë…„ëŒ€ì˜ ì—°êµ¬ëŠ” **ë‹¨ì–´ì— ì´ˆì **ì„ ë§ì¶”ê³ , ëŒ€ì‹  ë¬¸ìì˜ ë¬¸ìì—´ì„ ì²˜ë¦¬í–ˆë‹¤.      

**[ë‹¨ì–´ ìì²´ì— ëŒ€í•œ ì •ë³´]**        
* ê·¸ëŸ¬ë‚˜ neural modelsì´ NLPì—ì„œ ì¤‘ìš”í•´ì§€ìë§ˆì, ì‹ ê²½ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ **ë‹¨ì–´ì™€ ë¬¸ì ìˆ˜ì¤€ì˜ ì •ë³´ì˜ ì¡°í•©**(word- and character-level information)ì´ ë“±ì¥.       
* ë‹¨ì–´ ì„ë² ë”© ì¶”ì •ì„ ë•ê¸° ìœ„í•´ ë‹¨ì–´ ìì²´ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‚¬ìš©í•  ê²ƒì„ ì²˜ìŒ ì œì•ˆí–ˆë‹¤.      
* wordâ€™s embeddingì„ ë‹¨ì–´ì˜ ì² ìë¥¼ í†µí•´ êµ¬ì„±í•œë‹¤ëŠ” ì•„ì´ë””ì–´ë¥¼ **ëŒ€ì¤‘í™”**í–ˆë‹¤.   


**[CNN ê¸°ë°˜ ì„ë² ë”© ê¸°ëŠ¥]**     
* ì„ë² ë”© í–‰ë ¬ì„ CNN(convolutional neural network) ë ˆì´ì–´ë¡œ ëŒ€ì²´í•  ë•Œì—ë„ generative ëª¨ë¸ì€ ì—¬ì „íˆ íì‡„ ì–´íœ˜ì´ë©°, 
ì´ëŠ” í›ˆë ¨ ë°ì´í„°ì—ì„œ ë³´ì˜€ë˜ ë‹¨ì–´ë§Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì—,         
CNN êµ¬ì„±ì€ **ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ì•„ë‹Œ** **í¬ê·€ ë‹¨ì–´**ë§Œ ë•ëŠ”ë‹¤.      


**[CNN with í† í°ì˜ ì² ì]**      
* **ê° í† í°ì— ëŒ€í•œ ì² ì**ë¡œ ì„ë² ë”©ì„ êµ¬ì„±í•˜ëŠ” CNN ê¸°ë°˜ ì„ë² ë”© ê¸°ëŠ¥     
* ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì˜ˆìƒí•˜ëŠ” ëŒ€ì‹  "ìì£¼ ë‹¨ì–´ë¥¼ ì •í™•í•˜ê²Œ ì–»ë„ë¡" ì•”ë¬µì ìœ¼ë¡œ í›ˆë ¨í•œë‹¤.  
* **ì´ ê¸°ë°˜ ëª¨ë¸ì˜ ì—­í• :** POS íƒœê¹…ê³¼ ê°™ì€ ë‹¤ë¥¸ ê³ ì „ì ì¸ NLP ì‘ì—…ì˜ ë°œì „ì„ ì´ëŒì—ˆê³  ê¶ê·¹ì ìœ¼ë¡œ ìµœì´ˆì˜ í° ë¬¸ë§¥ ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ ELMoì— í˜ì„ ì‹¤ì–´ì£¼ì—ˆë‹¤.        
  * [fastText](https://yerimoh.github.io/LAN7/) ì„ë² ë”©ì€ ë¬¸ìê°€ ì•„ë‹ˆë¼ ê²¹ì¹˜ëŠ” n-gramì—ì„œ ë‹¨ì–´ ì„ë² ë”©ì„ êµ¬ì„±í•  ê²ƒì„ ì œì•ˆí•˜ì—¬,     
ìƒˆë¡œìš´ ë‹¨ì–´ì— ëŒ€í•œ ì„ë² ë”©ì„ ì–»ì„ ìˆ˜ ìˆë‹¤(generative ì˜ë¯¸ëŠ” ì•„ë‹ˆì§€ë§Œ ê·¸ëŸ° ì˜ë¯¸ì—ì„œ "open-vocabulary"ë¡œ ë§Œë“ ë‹¤).    

  * Ataman and Federicoë„ ë§ˆì°¬ê°€ì§€ë¡œ ë¬¸ì ëŒ€ì‹  (ì¤‘ì²©) ğ‘›-gramsì„ ì‚¬ìš©í•˜ì—¬ ê¸°ê³„ ë²ˆì—­ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì–»ëŠ”ë‹¤(í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€í•œ ì–¸ì–´ì—ì„œ BPEë¥¼ ëŠ¥ê°€í•¨).     
  * ìµœê·¼ì—ëŠ” ì² ì ì˜¤ë¥˜ ë˜ëŠ” ì „ì†¡ìœ¼ë¡œ ë…¸ì´ì¦ˆê°€ ë§ì€ í…ìŠ¤íŠ¸ë¥¼ ë” ì˜ ì²˜ë¦¬í•¨ìœ¼ë¡œì¨     
êµ¬ì„± ë¬¸ì ì„ë² ë”©ì— ëŒ€í•œ BPE ë‹¨ìœ„ì˜ ì„ë² ë”©ì„ í–¥ìƒì‹œì¼°ë‹¤.      


**[small Transformer]**   
* ì˜ë£Œ í…ìŠ¤íŠ¸ì™€ ê°™ì€ ìƒˆë¡œìš´ ë„ë©”ì¸; ë™ì‹œì— Aguilar ë“±(2021)ì€ ê±°ì˜ ë™ì¼í•˜ê²Œ í•˜ì§€ë§Œ CNN ëŒ€ì‹  small Transformerë¥¼ ì‚¬ìš©í•œë‹¤.     

**[êµ¬ì„± ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹]**   
* êµ¬ì„± ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì€ ì‚¬ì „ í›ˆë ¨ëœ ë‹¨ì–´ ìˆ˜ì¤€ ì…ë ¥ ëª¨ë¸ì—ë„ í†µí•©ë˜ì—ˆë‹¤.     
* íŠ¹íˆ, Pinter ë“±(2017)ì€ í…ŒìŠ¤íŠ¸ ì‹œê°„ ë™ì•ˆ ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚  ë•Œë§ˆë‹¤ í˜¸ì¶œë˜ëŠ” helper RNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ **ì² ìê°€ ì£¼ì–´ì§„ ë‹¨ì–´ì˜ ì„ë² ë”©ì„ ëª¨ë°©**í•˜ë„ë¡ í›ˆë ¨ëœ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤.      


---


## **4.2 Open-vocabulary language modeling with (tokenizer-defined) words made of characters**
**[open-vocabulary language modelingì˜ ì–´ë ¤ì›€]**    
* ```open-vocabulary language modeling```: test timeë•Œ **ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê³  ìƒì„±**í•  ìˆ˜ ìˆëŠ” ëª¨ë¸     
* ```closed-vocabulary generative models```ì„ ```open-vocabulary language modeling```ë¡œ í™•ì¥í•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤.              
* **ì–´ë ¤ìš´ ì´ìœ **: ì™„ì „íˆ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ì˜ ë¬´í•œ ì§‘í•©ì— ëŒ€í•œ í™•ë¥  ì§ˆëŸ‰ì„ ìœ ì§€í•  ìˆ˜ ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— input sideì˜ Open-vocabulary ë³´ë‹¤ ë‹¤ì†Œ ì–´ë µë‹¤.     


**[Mielkeì™€ Eisner(2018)ì˜ ëª¨ë¸]**         
* ë‹¨ì–´ ì„ë² ë”©ì„ ì •ê·œí™”í•˜ì—¬ ì‘ì€ ë¬¸ì ìˆ˜ì¤€(smaller character-level)ì˜ ```RNNLM```ì„ ì‚¬ìš©     
* smaller modelì„ ì‚¬ìš©í•˜ì—¬ ì² ìë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•¨ìœ¼ë¡œì¨ ì¼ë°˜ì ì¸ íì‡„ ìŒì„± ë‹¨ì–´ ìˆ˜ì¤€ì˜ ë°˜ë³µ ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸(```RNNLM```) ì„¤ì •ì„ ê·¼ë³¸ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” í™•ë¥ ë¡ ì  2ë‹¨ê³„ ëª¨ë¸ì„ ì œì•ˆí•œë‹¤.       
* ë‹¨ì–´ ìˆ˜ì¤€(word-level) ```RNNLM```ì´ ```UNK```ë¥¼ **ì˜ˆì¸¡í•  ë•Œë§ˆë‹¤ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì¦‰ì‹œ ìƒì„±**      
â¡ ì–¸ì–´ ê°œë…ê³¼ ì§ê´€ì  ëª¨ë¸ë§ì— ì˜í•´ ë™ê¸° ë¶€ì—¬       
â¡ ì§ˆì  ë° ì •ëŸ‰ì ìœ¼ë¡œ ì„±ê³µì ì¸ ê²ƒìœ¼ë¡œ ì…ì¦ëœ ê°œë°©í˜• ì–´íœ˜ ëª¨ë¸ì„ ì‚°ì¶œ      



**[```cache model```ì˜ í™•ì¥]**        
* Kawakami et al. (2017)ì˜ ëª¨ë¸ì€ ë‹¨ì–´ ë° ë¬¸ì ìˆ˜ì¤€ì˜ RNNì˜ ìœ ì‚¬í•œ 2ë‹¨ê³„ ì„¤ì •ì„ ë”°ë¥´ì§€ë§Œ,     
cache model(Grave et al., 2016)ì„ ì‚¬ìš©í•˜ì—¬ directly copiedí•  ìˆ˜ ì—†ëŠ” ê²½ìš°, **ê° ë‹¨ì–´ë¥¼ ë¬¸ì ìˆ˜ì¤€ì˜ RNNì„ ì‚¬ìš©í•˜ì—¬ ì² ì**í•´ì•¼ í•œë‹¤.         
â¡ ì´ë“¤ì˜ ë¶„ì„ì€ ìºì‹œ ëª¨ë¸ì´ ê·¸ë ‡ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ë¶„ëª…íˆ ë³´ì—¬ì¤€ë‹¤.      
* ```cache model```ì€ â€œburstyâ€ **ë¯¸ì§€ì˜ ë‹¨ì–´**ë§Œ ë³µì‚¬í•˜ì§€ë§Œ, ë˜í•œ ìŠì–´ë²„ë¦¬ì§€ ì•Šë„ë¡ í•˜ê¸° ìœ„í•´ ê·¹íˆ extremely **common** function wordsë“¤ë„ ë³µì‚¬í•œë‹¤.       
* ì´ ì•„ì´ë””ì–´ëŠ” ```Ataman``` ë“±(2019)ì— ì˜í•´ machine translation decoder(creating word embeddings on the encoder side from character-level ```BiRNNs``` as in ```ELMo```)ì—ì„œ ì¸ì½”ë” ì¸¡ì— ë‹¨ì–´ ì„ë² ë”©ì„ ìƒì„±)ì— ëŒ€í•´ ì„ íƒë¨    
* ë‚˜ì¤‘ì— ```Ataman``` ë“±(2020)ì— ì˜í•´ í™•ì¥ë¨          


**[ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹]**     
* ë” ë‚®ì€ ì†ë„ë¡œ ```multi-layer RNNs```ì˜ higher layerë¥¼ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ë‹¤(hidden stateë¡œì˜ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆë›´ë‹¤)     
* ì´ëŠ” El Hihi and Bengio(1995)ì—ì„œ ì²˜ìŒ ì œì‹œëœ ì˜¤ë˜ëœ ì•„ì´ë””ì–´ì´ë©°(Schmidhuber (1991, 1992)ì˜ "neural sequence chunker"ì— êµ¬ì¶•ë¨)      
* ìš°ë¦¬ëŠ” ë‹¨ì–´ ê²½ê³„ì™€ ê·¸ì— ë”°ë¥¸ segmentationsì„ ì‹¤ì œë¡œ ë°°ìš¸ ìˆ˜ ìˆë‹¤.    


----
----

# **5 Learning segmentations to find concatenative word-like pretokenizer tokens**
ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” [2](#2-Tokens,-word-forms,-and-sub-words)ì— ìš”ì•½ëœ ê°œë…ì •ì˜ì˜ ë³€í™”ì—ë„ ë¶ˆêµ¬í•˜ê³ ,    
predefinedëœ ë‹¨ì–´(ë˜ëŠ” ì‚¬ì „ í† í°í™” ì¶œë ¥) ê°œë…ì„ ê°–ëŠ” ê²ƒì— ì˜ì¡´í•´ì™”ë‹¤.              

**[ë¬¸ì œ]**      
ê·¸ëŸ¬ë‚˜ ì˜ì–´ê°€ ì•„ë‹Œ ì–¸ì–´ë‚˜ robustnessë¬¸ì œë¡œ ì¸í•´ predefinedëœ ì •ì˜ê°€ ì£¼ì–´ì§€ì§€ ì•Šê±°ë‚˜, ì–»ì„ ìˆ˜ ì—†ê±°ë‚˜, ë‹¨ìˆœíˆ ë°”ëŒì§í•˜ì§€ ì•Šë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œ?      
ìš°ë¦¬ì˜ **ë°ì´í„° ê¸°ë°˜ ë¨¸ì‹  ëŸ¬ë‹ ì ‘ê·¼ë²•**ì´ **í† í°í™”ë¥¼ í•™ìŠµ**í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆì„ê¹Œ?     

**[í•´ê²°]**       
ì´ ì ˆì— ì„¤ëª…ëœ ëŒ€ë¶€ë¶„ì˜ ì ‘ê·¼ ë°©ì‹ì€ **ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„**ì— í•´ë‹¹í•˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ì™€ **ê²½ê³„ë¥¼ ì°¾ê¸° ìœ„í•´**,     
**ê·¼ì‚¬(approximate)** ë˜ëŠ” (ë” ë§ì€ ê°€ì •ì„ ì‚¬ìš©í•˜ì—¬) **ì •í™•í•œ ì¶”ë¡ **ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” **ì ì¬(latent) ë³€ìˆ˜ë¡œ ì•”ì‹œì  ë¶„í• (implied segmentation)ì„ ì²˜ë¦¬**í•¨ìœ¼ë¡œì¨ í† í°í™”ë¥¼ í•´ê²°í•  ê²ƒì„ ì œì•ˆí•œë‹¤.       

ì´ ì ˆì—ì„œ ì„¤ëª…í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì€ ë‹¤ì–‘í•œ í¬ê¸°ì™€ í’ˆì§ˆì˜ ë‹¨ìœ„ë¥¼ ì‚°ì¶œí•œë‹¤.


----

## 5.1 Character-level neural models that learn to skip steps at higher levels

**[Elman(1990)]**     
* ì´ë¯¸ 90ë…„ëŒ€ì— Elman(1990)ì€ character-levelì˜ RNNì„ ìˆ˜ë™ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì˜ˆì¸¡ ê¹œì§ ë¹„êµë¥¼ ë‹¨ì–´ ê²½ê³„(boundaries)ì™€ ê´€ê³„ì‹œì¼°ë‹¤.    
* ì´ ì•„ì´ë””ì–´ëŠ”  Schmidhuber(1991, 1992)ì˜ â€œneural sequence chunkerâ€ì—ì„œ í™•ì¥ë˜ì—ˆë‹¤.     


**[Doval and GÃ³mezRodrÃ­guez(2019)]**     
* ìµœê·¼ì—ëŠ” Doval and GÃ³mezRodrÃ­guez(2019)ì˜ beam search framework í•˜ì—ì„œ character-level neural models ë¿ë§Œ ì•„ë‹ˆë¼ n-gram modelsì—ë„ ë¶„ì„ì„ ì ìš©í•˜ì—¬ ê³µê°„ì´ ì‚­ì œë˜ëŠ” micro blog textsë¥¼ ë¶„í• í–ˆë‹¤.     


**[```HM-RNN```(Chung et al., 2017)]**      
* post-hoc surprisal thresholding(ì‚¬í›„ ì„ê³„ê°’)ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  [4.2](# Open-vocabulary-language-modeling-with-(tokenizer-defined)-words-made-of-characters)ì—ì„œ ë™ê¸°í™”ëœ ì—¬ëŸ¬ timescalesì˜ ì•„ì´ë””ì–´ë¥¼ ì·¨í•œë‹¤.     
* ê¸°ìš¸ê¸° í•˜ê°• ìµœì í™” ë°©ë²•        
   * skipì´ë‚˜ updateí•˜ê¸° ìœ„í•œ ì´ì§„ ê²°ì •(binary decisionbinary decision)ì„ í•™ìŠµ       
   â¡ ë‹¨ì–´ ê²½ê³„ ê°ê° ì œê³µ    
   * ì§ì„  ì¶”ì •ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ëµì ì¸ ê¸°ìš¸ê¸° í•˜ê°•ìœ¼ë¡œ ìµœì í™”        
* ê³„ì¸µë“¤ ì‚¬ì´ì˜ í†µì‹ ì€ **ì–‘ë°©í–¥ì **ìœ¼ë¡œ ì¼ì–´ë‚¨      
   * the lower network: ê·¸ê²ƒì˜ ìµœì¢… ìƒíƒœë¥¼ higher ë„¤íŠ¸ì›Œí¬ì— ë³´ê³ í•œë‹¤      
   * the higher network: ê·¸ê²ƒì˜ ìƒˆë¡œìš´ ìƒíƒœë¥¼ lower ê³„ì¸µì— ë³´ê³ í•œë‹¤.     
   * ì´ëŸ¬í•œ í†µì‹ ì„ ìŠ¤ìŠ¤ë¡œ ê³„ì† ì§„í–‰í•œë‹¤.    
 
 
**[```HM-RNN```(Chung et al., 2017)ì˜ ì—°êµ¬]**      
* **Kawakami et al. (2019):** ë°ì´í„°ì— ê³µê°„ì„ í¬í•¨í•  ë•Œ ë‹¨ì–´ ê²½ê³„ë¥¼ â€œrecover(ë³µêµ¬)â€ í•˜ì§€ë§Œ,      
Kawakami ì™¸(2019)ëŠ” **ê³µê°„ì„ í¬í•¨í•˜ì§€ ì•Šì„ ë•Œ** ì´ ëª¨ë¸ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ì„¸ê·¸ë¨¼íŠ¸(unusable segments)ë¥¼ ì–»ì—ˆë‹¤ê³  ì£¼ì¥í•œë‹¤.     
* **NMTì— ```HM-RNN```ì„ ì‚¬ìš©í•˜ë ¤ê³  í•  ë•Œ:**     
   * Cherry ë“±(2018)ì€ ê·¸ê²ƒì´ í›ˆë ¨ë˜ë„ë¡ í•˜ëŠ” ë° **ë§ì€ ìˆ˜ì • ì‘ì—…ì´ í•„ìš”**í–ˆìœ¼ë©°,      
ì‘ì—…ì—ì„œ ê·¸ê²ƒì˜ ì„±ëŠ¥ì€ ê²½ìŸì ì´ì§€ë§Œ **ìš°ìˆ˜í•˜ì§€ëŠ” ì•Šì•˜ë‹¤**ê³  ë³´ê³ í–ˆë‹¤.     
   * ì´ ë°œê²¬ì€ HM-RNNì´ ì˜ í›ˆë ¨ë˜ë„ë¡ í•˜ê³ , ì´ë¥¼ ì™„í™”í•˜ë©°,    
   í…ìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ í•˜ìœ„ ì„¸ê·¸ë¨¼íŠ¸(subpar segmentation)ë¥¼ ë³´ì—¬ì£¼ëŠ” ë…¼ë¬¸ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ”  KÃ¡dÃ¡r et al. (2018)ì˜ ì—°êµ¬ ê²°ê³¼ë¥¼ ë’·ë°›ì¹¨í•œë‹¤.      
* Kreutzer and Sokolov (2018)ëŠ” NMTì— ëŒ€í•´ **ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  í•˜ìœ„ ë ˆì´ì–´ë¡œ ìš”ì•½ì„ ìƒì„±**í•˜ëŠ” ìœ ì‚¬í•œ íŒ¨ëŸ¬ë‹¤ì„ì„ ì‚¬ìš©í•˜ë ¤ê³  ë…¸ë ¥í•˜ë©°, **ê±´ë„ˆë›°ê¸°**ëŠ” ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ìœ„í•´ **ë¶ˆí•„ìš”**í•´ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•œë‹¤.        
* **ëª¨ë¸ì˜ í™•ì¥:** ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ ëª¨ë¸ì€ Luoì™€ Zhu(2021ë…„)ì— ì˜í•´ êµ¬ì™€ ë¬¸ì¥ ìˆ˜ì¤€ì˜ ê²½ê³„ë¡œ í™•ì¥ëœë‹¤.      
* ë” coarserí•œ ê³„ì‚° ë ˆì´ì–´ë¥¼ ê°€ì§€ê³  ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ëŸ¬í•œ ëª¨ë¸ì€ ì—¬ì „íˆ ë‹¨ì–´ê°€ ìƒì„±ë  ë•Œë§ˆë‹¤  â€œspell outâ€í•´ì•¼ í•œë‹¤.       
ì¦‰, í† í°ì„ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¨ìœ„ë¡œ memoizeí•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì§€ì í•  í•„ìš”ê°€ ìˆë‹¤.       


-------


## **5.2 Marginalization over all possible segmentations**    
ë§ˆì§€ë§‰ìœ¼ë¡œ, ê°œë…ì ìœ¼ë¡œ ê°„ë‹¨í•œ ì ‘ê·¼ ë°©ì‹ì€,    
ë¬¸ìì—´ì˜ ë¶„í• ì„ trainingê³¼ test time ëª¨ë‘ì—ì„œ **marginalizedí•´ì•¼ í•˜ëŠ” ì ì¬ ë³€ìˆ˜(latent variable)** ë¡œ ë‹¤ë£¨ëŠ” ê²ƒì´ë‹¤.     

**[ë¬¸ì œ]**       
ì´ê²ƒì€ ë³¸ì§ˆì ìœ¼ë¡œ ê²¹ì¹˜ëŠ” ê¸¸ì´ê°€ ë‹¤ë¥¸ ë¬¸ìì—´, ì¦‰ "cat", "at", "foster cat"ì„ í¬í•¨í•˜ëŠ” ì–´íœ˜ë¥¼ ê°–ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°,   
"my foster cat" ë¬¸ìì—´ì€ ì ì¬ ë‹¨ìœ„ì˜ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ì— ëŒ€ì‘í•˜ì—¬ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ ë¶„í•´ë  ìˆ˜ ìˆë‹¤.    
â¡ ë¶„í• ì˜ ìˆ˜ê°€ ì‹œí€€ìŠ¤ ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì—ì„œ ê¸°í•˜ê¸‰ìˆ˜ì ì´ë‹¤.            

**[í•´ê²°]**      
* ì ì¬ ë¶„í•´ì— ëŒ€í•œ ì£¼ë³€í™”ë¥¼ ìœ„í•œ ê·¼ì‚¬ì¹˜(Â§ 5.2.1)ì— ì˜ì¡´       
* n-gram ëª¨ë¸(Â§ 5.2.2)ì„ ì‚¬ìš©í•˜ì—¬ ë…ë¦½ì„± ê°€ì •ìœ¼ë¡œ ëª¨ë¸ì„ ë‹¨ìˆœí™”í•´ì•¼í•¨       



<details>
<summary>ğŸ“œ 5.2.1 Approximate marginalization</summary>
<div markdown="1">
  
* **Chan et al. (2017)**     
    * beam searchì„ í†µí•´ **approximate MAP ì¶”ë¡ **ì„ ì‚¬ìš©í•˜ì—¬ ê´€ì¸¡ì˜ í•œê³„ í™•ë¥ ì„ ê·¼ì‚¬í™”í•˜ëŠ” ì¶”ì •ê¸°ë¥¼ ì œì•ˆí•œë‹¤.     
    * ê·¸ë“¤ì€ ê·¸ ëª¨ë¸ì´ í›ˆë ¨ì‹œí‚¤ê¸°ê°€ ë§¤ìš° í˜ë“¤ì§€ë§Œ ìœ ë§í•œ ê²°ê³¼ë¥¼ ì–»ëŠ” ë° ì„±ê³µí•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.    
* **Buckmanê³¼ Neubig(2018)**   
    * ì´ ëª¨ë¸ì˜ **ë¶ˆì•ˆì •ì„±ì„ í™•ì¸**í•˜ê³  LM ë³µì¡ë„ ì¸¡ë©´ì—ì„œ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” í‰ê·  RNN ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª‡ ê°€ì§€ **ê·¼ì‚¬ ì¶”ë¡  ì²´ê³„ë¥¼ ì œì•ˆ**í•œë‹¤.       
* **Hiraoka ë“±(2020)**      
    * Unigram LM í† í°í™” ì œì•ˆ ë¶„í¬(Â§ 6.4.3 ì°¸ì¡°)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ ëª¨ë¸ì„ êµ¬í˜„í•˜ëŠ”ë°,      
    ì´ ë¶„í¬ëŠ” ë¬¸ì¥ì˜ **ğ‘›-best í† í°í™”**ê°€ ëª¨ë“  **ë¬¸ì¥ ì¸ì½”ë” ëª¨ë¸ì— ë…ë¦½ì ìœ¼ë¡œ ê³µê¸‰**ë˜ë©°,     
    **ê²°ê³¼ ë¬¸ì¥ ì„ë² ë”©ì€ ìš°ì„ ìˆœìœ„ í† í°í™” ê°€ëŠ¥ì„±ì— ë”°ë¼ í‰ê· í™”**ëœë‹¤.      
* **Hiraoka ë“±(2021)**      
    * ë³„ë„ì˜ ì†ì‹¤ë¡œ í† í°í™” ë° ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê³ ,      
    ì „ìëŠ” ë‚®ì€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì†ì‹¤ì„ ìƒì„±í•˜ëŠ” í† í°í™”ë¥¼ ë³´ìƒí•˜ë©°,     
    í›„ìëŠ” ì¡°ê±´ë¶€(ë° ê°•í™”) LMì—ì„œ ìƒ˜í”Œë§ëœ í•˜ë‚˜ì˜ í† í°í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ëª¨ë¸ì„ sequence-to-sequence ì„¤ì •ìœ¼ë¡œ í™•ì¥í•œë‹¤. 
 
</div>
</details>  







<details>
<summary>ğŸ“œ 5.2.2 Exact marginalization using additional independence assumptions: segmental neural language models</summary>
<div markdown="1">


     

**[Kong et al.(2016)]**    
* segmental neural language modelsì˜ ë” ì¸ê¸° ìˆëŠ” ì†”ë£¨ì…˜ ê°œì²™      
* ê·¸ëŠ” ë¶„í•  ë¬¸ì œë¥¼ **ë‹¨ì¡°ë¡œìš´ 13ê°œì˜ seq2seq ì‘ì—…ìœ¼ë¡œ ìºìŠ¤íŒ…**í•˜ì—¬ charactersì—ì„œ í•˜ìœ„ ë¬¸ìì—´ì˜ ì»¤ë²„ ìˆœì„œ(sequence of substrings), ì¦‰ ë¶„í• ë¡œ ì „í™˜í–ˆë‹¤.      
* ```BiRNN```ì„ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬ë˜ê³  í¬í•¨ëœ **ì „ì²´ raw stringì— ëŒ€í•œ ì„¸ê·¸ë¨¼íŠ¸ ì˜ˆì¸¡ì„ ì¡°ê±´í™”**      
â¡ ì„¸ê·¸ë¨¼íŠ¸í™” ê²°ì •/ì ìˆ˜ëŠ” ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš© ê°€ëŠ¥             
* ì´ëŸ¬í•œ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì„¸ê·¸ë¨¼íŠ¸ ê°€ëŠ¥í•œ ëª¨ë“  ê°œë³„ í•˜ìœ„ ë¬¸ìì—´ì„ ë…ë¦½ì ìœ¼ë¡œ ì±„ì í•œ ë‹¤ìŒ,     
ê°œë³„ ì ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬ ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì±„ì           
â¡ ë™ì  í”„ë¡œê·¸ë˜ë°(DP)ì„ ì‚¬ìš©í•˜ì—¬ covering of the entire input stringë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆë‹¤    
* **ë™ì‘ ì›ë¦¬**     
   * ì¤‘ì•™ ë…ë¦½ì„± ê°€ì •(central independence assumption) ë•Œë¬¸ì´ë‹¤.       
   * ì´ ëª¨ë¸ì€ ì„¸ê·¸ë¨¼íŠ¸ì— ì ìˆ˜ë¥¼ ë§¤ê¸¸ ë•Œ ë‹¤ë¥¸ ì„¸ê·¸ë¨¼íŠ¸ì— ì˜ì¡´í•˜ì§€ ì•Šê³  **ë‹¨ì§€ ì£¼ë³€ ë¬¸ìì— ì˜ì¡´**í•  ë¿ì´ë‹¤.     


**[Wang et al. (2017)]**
* ìœ„ì˜ Kong et al.(2016)ì— ëŒ€í•œ í™•ì¥      
* ì´ ëª¨ë¸ì€ ì„¸ê·¸ë¨¼íŠ¸í™”ë¥¼ ëª°ë¼ë„ ì‹¤í–‰ ê°€ëŠ¥     
â¡ ê°œë³„ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± í”„ë¡œì„¸ìŠ¤ì—ì„œ, ê³¼ê±° í‘œí˜„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¶œë ¥ì— ëŒ€í•œ ë¬¸ìì— ëŒ€í•´ per-segment RNNì„ ë³´ìœ       
â¡ ë™ì  í”„ë¡œê·¸ë˜ë°ì„ ì¤‘ë‹¨í•˜ì§€ ì•Šê³  ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê³µìœ  ê°€ëŠ¥      


**[LMing]**         
* ì´ì œ **ì…ë ¥ì— ëŒ€í•œ ì¡°ê±´ì„ ìƒëµ**í•˜ì—¬ ì„¸ê·¸ë¨¼íŠ¸ ì–¸ì–´ ëª¨ë¸ì´ë¼ëŠ” ìš©ì–´ë¥¼ ë§Œë“  Sunê³¼ Deng(2018)ì˜ ëª¨ë¸ì„ ì‚°ì¶œ     
* í•œìì— ëŒ€í•´ í›ˆë ¨     
* ë¹„ì§€ë„ í•™ìŠµëœ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘êµ­ì–´ ë‹¨ì–´ ë¶„í• ì—ì„œ ê²½ìŸí•˜ëŠ” ê²ƒìœ¼ë¡œ ë§Œë“¤ì–´ì§      
* 2ì°¨ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ì˜ **ê³„ì‚°ì„ ì‹¤í˜„ ê°€ëŠ¥í•˜ê²Œ ìœ ì§€**í•˜ê¸° ìœ„í•´ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìµœëŒ€ **4ìë¡œ ì œí•œ**í•œë‹¤.      


**[Grave et al. (2019)]**        
* Transformersë¥¼ ë…ë¦½ì ì¸ character-level global backboneìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬, **ë…ë¦½**ì ìœ¼ë¡œ **ë™ì¼í•œ ì í”„**ë¥¼ í•œë‹¤.     
* ì˜ì–´ open-vocabulary language modelingì—ì„œ í‰ê°€í•  ë•Œ perplexityê°€ ê°œì„ ë˜ì—ˆë‹¤.    
* í•˜ì§€ë§Œ, obtained segmentationì„ ì‚¬ìš©í•˜ê±°ë‚˜ í‰ê°€í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ë°œê²¬      
â¡ ì›ì¸: ì´ëŠ” ê·¸ë“¤ ì—­ì‹œ ìµœì†Œ 400ë²ˆ ë‚˜íƒ€ë‚˜ëŠ” 4-gramsë§Œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì¼ ê°€ëŠ¥ì„±ì´ í¬ë‹¤.       

**[Kawakami et al. (2019)]**      
* ìœ„ì™€ ë™ì¼í•œ **ë…ë¦½ ì•„ì´ë””ì–´ë¥¼ ì‚¬ìš©**     
* **ë‹¤ë¥¸ì :** ë¬¸ìì—´ ì„¸ê·¸ë¨¼íŠ¸ì˜ ë°©ì¶œ(missions of string segments)ì€  Kawakami et al. (2017)ì™€ ê°™ì€ ë¬¸ì ìˆ˜ì¤€ ëª¨ë¸(Â§ 4.2 ì°¸ì¡°)ê³¼ í•™ìŠµëœ ì„ë² ë”©ì´ ìˆëŠ” ëŒ€ê·œëª¨ í•˜ìœ„ ë¬¸ìì—´ ì„¸íŠ¸(ìµœëŒ€ 10-gramsê¹Œì§€ í›ˆë ¨: ë°ì´í„°ì— ì¶©ë¶„íˆ ìì£¼ ë‚˜íƒ€ë‚¨)ì˜ ì»¨í…ìŠ¤íŠ¸ ì˜ì¡´ì  í˜¼í•©ì—ì„œ ë¹„ë¡¯ë¨        
* perplexityë¿ë§Œ ì•„ë‹ˆë¼ ì¼ë¶€ baselines(ì œ5.3ì¡° ì°¸ì¡°)ì„ ëŠ¥ê°€í•˜ëŠ” ë‹¨ì–´ ë¶„í•  ì„±ëŠ¥ì„ ê°–ìŒ     
* í•˜ì§€ë§Œ, ì—¬ì „íˆ ì¼ë¶€ ì´ì „ ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ë” ë‚˜ìœ ì„±ëŠ¥ì„ ë³´ì„      
    * marginal likelihood ëŒ€ì‹  ë¶„í•  ì„±ëŠ¥ì— hyperparametersë¥¼ ì¡°ì •í•˜ì—¬ **ë¶ˆê³µí‰í•œ ì´ì **ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ì£¼ì¥     
* ì´ë¯¸ì§€ ìº¡ì…˜ì— ëŒ€í•œ í›ˆë ¨ì„ í•  ë•Œ ëª¨ë¸ì´ **ì„¤ëª…ë˜ëŠ” ì´ë¯¸ì§€ì— ì•¡ì„¸ìŠ¤**í•  ìˆ˜ ìˆì„ ë•Œ,    
 perplexityê³¼ ë¶„í•  ì„±ëŠ¥ì´ ëª¨ë‘ **í–¥ìƒ**ëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬       
 â¡ **ë‹¨ì¼ ì–¸ì–´ ë° ë‹¨ì¼ ì–¸ì–´ í…ìŠ¤íŠ¸**(monolingual and unimodal text)ê°€ í•™ìŠµ ì„¸ë¶„í™”ê°€ ë‹¤ë¥¸ ì–‘ì‹, ë‹¤ë¥¸ ì–¸ì–´ê°€ ì¡´ì¬í•  ë•Œë³´ë‹¤ **ë” ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ**ì„ ë³´ì—¬ì¤€ë‹¤.    
 
 
**[He et al. (2020)]**    
* NMT ì‹œìŠ¤í…œì˜ ëŒ€ìƒ ì¸¡ ìƒì„±ê¸°ë¡œ ìœ ì‚¬í•œ ì„¸ê·¸ë¨¼íŠ¸ ëª¨ë¸ì„ êµ¬ì¶•     
â¡ ìœ„ì™€ ê°™ì´ character backbone ì•„ì´ë””ì–´ë¥¼ ë”°ë¥´ëŠ” Transformer-based versionëª¨ë¸ì„ ë™ì  í”„ë¡œê·¸ë˜ë°ì´ ê°€ëŠ¥í™”ë¥¼ ìœ„í•´ ì‚¬ìš©í•¨         
* NMT ì‹œìŠ¤í…œì„ ìµœì¢… ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³  **í•™ìŠµëœ tokenizerë¡œ ì‚¬ìš©**í•œë‹¤.      
â¡ ì´ëŠ” ë™ì  í”„ë¡œê·¸ë¨ì„ marginalizationì—ì„œ **maximization**ë¡œ ë³€ê²½í•˜ì—¬ BPE ë˜ëŠ” ìœ ë‹ˆê·¸ë¨ LM ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” **DPEë¼ëŠ” ìƒˆë¡œìš´ ë¶„í• ì„ ì–»ìŒ**ìœ¼ë¡œì¨ ì‰½ê²Œ ë‹¬ì„± ê°€ëŠ¥(Â§ 6.4 ì°¸ì¡°).       
* ```small Transformer-based NMT```ëª¨ë¸ë¡œ í† í°í™”í•˜ëŠ” í•™ìŠµì´ **ë” í° ëª¨ë¸ì—ì„œ ì‚¬ìš©**í•˜ê¸° ìœ„í•´ BPEë³´ë‹¤ ë” ë‚˜ì€ ë¶„í• ì„ ìƒì„±í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.      
* íŠ¹íˆ ë²ˆì—­ ì‘ì—…ì— í† í°í™” ëª¨ë¸ì„ í›ˆë ¨í•˜ë©´ **ì†ŒìŠ¤ ì–¸ì–´ì— ë”°ë¼ ë‹¤ë¥¸ ë¶„í• ì´ ìƒì„±**ë˜ë©°, ë” ì¤‘ìš”í•œ ê²ƒì€ **ë” ë‚˜ì€ ë¶„í• (segmentation)ì´ ìƒì„±**ëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.        


**[Downey et al. (2021)]**     
* ë¬¸ìë¥¼ ì¡°ê±´í™”í•˜ê³  ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì•„ì´ë””ì–´ëŠ” ê²°ê³¼ê°€ ```RNN-based SNLMs```ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•˜ì§€ëŠ” ì•Šì§€ë§Œ,     
Downey et al. (2021)ì˜ Transformers ë° ```left-to-right autoregressive Transformers```ì—ì„œ ë°œê²¬ëœ **ë°©í–¥ ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë§**(adirectional masked language modeling ) ì„¤ì •ìœ¼ë¡œ í™•ì¥ëœë‹¤.       
* ì´ ëª¨ë¸ë“¤ ì¤‘ ë§ì€ ê²ƒë“¤ì´ ```UnigramLM```ì— ê¸°ë°˜ì„ ë‘” ëª¨ë¸ë“¤ì˜ ë³€í˜•ì´ë¼ëŠ” ì ì´ ì¤‘ìš”í•˜ë‹¤ Â§6.4.3.       

 </div>
</details>  


-----

## 5.3 Finding words through Bayesian non-parametrics
ğ‘›-gramê³¼ word-based ì–¸ì–´ ëª¨ë¸ë“¤ì˜ ì‹œëŒ€ì—,   

**[MacKay and Peto (1995)]**       
* ìë™ íšŒê·€ ì–¸ì–´ ëª¨ë¸ë“¤ì˜ Bayesian ê´€ì  ë„ì›€ì´ ëœë‹¤ëŠ” ê²ƒì„ ì•Œì•„ëƒ„      
* í‰ê· ì´ ì €ì°¨ ë¶„í¬(lower-order distributions)ì¸ ë””ë¦¬í´ë ˆ ë¶„í¬ì—ì„œ ê³ ì°¨ ë¶„í¬(higher-order distributions)ë¥¼ ë„ì¶œí•˜ëŠ” ê³„ì¸µì  ëª¨ë¸ì—ì„œ ì¶”ë¡ ìœ¼ë¡œ ğ‘›-gram ëª¨ë¸ì—ì„œ smoothing ë° backoffë¥¼ ì¬í•´ì„í•œë‹¤.


**[Teh(2006)]**      
* ìœ„ì˜ ìƒê°ì„ í™•ì¥     
* ìš°ë¦¬ê°€ ë‹¤ì‹œ ì„ì˜ì ìœ¼ë¡œ í° ì°¨ìˆ˜ì˜ ğ‘›-gram ë¶„í¬ë¥¼ ê°–ëŠ” ê³„ì¸µì  PYP ì–¸ì–´ ëª¨ë¸ì„ ì œì•ˆ        
* PYP ì–¸ì–´ ëª¨ë¸      
    * ğ‘›-gram ì–¸ì–´ ëª¨ë¸ smoothingê³¼ ìœ ì‚¬      
    * ê·¸ëŸ¬ë‚˜ ğ‘›ì˜ ì„ íƒì„ í¬ê¸°í•˜ëŠ” ì›ì¹™ì ì¸ ë°©ë²•ì„ ì œê³µ      

**[Wood et al. (2011)]**     
* ìœ„ ëª¨ë¸ë§ ì•„ì´ë””ì–´ì˜ ì •ì ì˜ ë‹¬ì„±        
* **ì„±ëŠ¥**    
    * ì„ì˜ì˜ ì´ì§„ ë°ì´í„°ì— ëŒ€í•´ ë›°ì–´ë‚œ ì••ì¶• ì„±ëŠ¥ì„ ìë‘í•¨      
    * ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—… ì—¬ì „íˆ ë§¤ìš° ì˜ ìˆ˜í–‰        
    * í•˜ì§€ë§Œ ì´ë•Œ ì‹ ê²½ ëª¨ë¸ì´ ì´ë¯¸ ê°•ë ¥í•œ ê²½ìŸìë¡œ ë‚˜íƒ€ë‚¨     


**[Goldwater et al. (2006b)]**        
* ì´ Bayesian ê´€ì ì„ í™•ì¥       
* ìƒˆë¡œìš´ ë‹¨ì–´ë“¤ì´ ì–´ë–»ê²Œ ì²˜ìŒ ë§Œë“¤ì–´ì§€ê³  ê·¸ë¦¬ê³  ê·¸ê²ƒë“¤ì´ ì‹¤í–‰ í…ìŠ¤íŠ¸ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ë¥¼ ì„¤ëª…í•¨       
* **ì–¸ê¸‰í•œ ê¸°ìˆ :** 2ë‹¨ê³„ ì–¸ì–´ ëª¨ë¸ë§ (ì œ4.2ì ˆ ì°¸ì¡°)      
    * 1ë‹¨ê³„: ìƒì„±ê¸° (ìƒˆë¡œìš´ ì–´íœ˜ì†Œë¥¼ ìƒì„±í•˜ëŠ”)      
    * 2ë‹¨ê³„: ì–´ëŒ‘í„° (ì—¬ê¸°ì„œ ì¬ì‚¬ìš©ì„ ì§€ë°°í•˜ëŠ”, PyP)        
    * ìœ„ì—ì„œ ë§í•œ ê²ƒê³¼ ê°™ì´ ì´ ë‘ê°œì˜ ë‹¨ê³„ëŠ” ì„œë¡œ ìƒí˜¸ì‘ìš©í•œë‹¤.      
* **ê¸°ëŠ¥**   
   * ë‹¨ì–´ ê²½ê³„ë¥¼ ìœ ì¶”í•˜ëŠ” ê²ƒ ê°€ëŠ¥í•´ì§    
   * = ë¹„ì§€ë„ ë‹¨ì–´ ë¶„í• ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§      
   * ì´ìœ : í…ìŠ¤íŠ¸ë¥¼ ì„¤ëª…í•˜ëŠ” 2ë‹¨ê³„ ëª¨ë¸ê³¼ ë¬´í•œí•œ ìˆ˜ì˜ ê°€ëŠ¥í•œ ì–´íœ˜ì†Œì— ì–‘ì˜ í™•ë¥ ì„ í• ë‹¹í•  ìˆ˜ ìˆëŠ” Bayesian ë¹„ëª¨ìˆ˜ ë©”íŠ¸ë¦­(nonparametrics)ì˜ ì‚¬ìš© ë•ë¶„       
* ì¸ì§€ ê³¼ì •, íŠ¹íˆ ì•„ë™ ì–¸ì–´ ìŠµë“ì„ ì„¤ëª…í•˜ê³  ëª¨ë¸ë§í•¨ìœ¼ë¡œì¨ ë” ë§ì€ ë™ê¸°ë¥¼ ë¶€ì—¬ë°›ìŒ.       

**[Goldwater et al. (2009)]**      
* transcribeëœ ìœ ì•„ ì§€í–¥ ìŒì„±ì„ ë¶„í• í•˜ê¸° ìœ„í•œ Unigramê³¼ Bigram Dirichlet Processs(DP)ë¥¼ ìš”ì•½í•˜ì—¬ ì˜¤ë˜ëœ ë¹„ë² ì´ì§€ì•ˆ ì ‘ê·¼ ë°©ì‹ë³´ë‹¤ ìš°ì›”í•¨ì„ ë³´ì—¬ì¤€ë‹¤.      


**[Mochihashi et al. (2009)]**     
* ì•„ì´ë””ì–´ë¥¼ bigram DPsì—ì„œ âˆ-gram nested/hierarchical PYPsë¡œ í™•ì¥í•˜ì—¬ **ì˜ì–´ í•„ê¸° í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë‹¨ì–´ ë¶„í• **ì„ ê°œì„ í•œë‹¤. 



**[Elsner et al. (2013)]**     
* ì¼ë ¨ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê´€ì°°ëœ ì‹¤í˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” **ìŒì„± í”„ë¡œì„¸ìŠ¤**ë¥¼ ì¶”ê°€ë¡œ ëª¨ë¸ë§í•œë‹¤.      


-----



## 5.4 Related task: Unsupervised Chinese Word Segmentation
ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ë° ë² íŠ¸ë‚¨ì–´ì™€ ê°™ì€ **ê³µë°± êµ¬ë¶„ìê°€ ì—†ëŠ” ì–¸ì–´**ì— ëŒ€í•œ ë‹¨ì–´ ì„¸ë¶„í™”ëŠ” ì¤‘ìš”í•œ ì—°êµ¬ ì˜ì—­ì´ë©° ê¹Œë‹¤ë¡­ê¸°ë¡œ ì•…ëª… ë†’ì„ ìˆ˜ ìˆë‹¤.     

ì¤‘êµ­ì–´ ë‹¨ì–´ ë¶„í• (CWS)ì—ì„œëŠ” ì‹ ê²½ ì–¸ì–´ ëª¨ë¸ì„ í¬í•¨í•˜ëŠ” ë¹„ì§€ë„ ë‹¨ì–´ ë¶„í• ì„ íƒìƒ‰í•˜ëŠ” ë° ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆë‹¤.     

ì „í†µì ìœ¼ë¡œ, ì¸ê¸° ìˆëŠ” ë¹„ì§€ë„ ì ‘ê·¼ ë°©ì‹ì€ ë‘ ê°€ì§€ ì£¼ìš” ê²½ë¡œë¥¼ ì·¨í•œë‹¤.        
**1)** ì°¨ë³„ì  ëª¨ë¸(discriminative models)      
**2)** ìƒì„±ì  ëª¨ë¸(generative models)      


**[1) ì°¨ë³„ì  ëª¨ë¸]**     
* í›„ë³´ ë¶„í• ì— ëŒ€í•œ ì„ ëŸ‰í•œ ì²™ë„(goodness measures)ì— ì˜ì¡´í•œë‹¤.      
* í†µê³„ì  ì¸¡ì • ë°©ë²•     
    * ìƒí˜¸ ì •ë³´(MI: Mutual Information)     
    * ì •ê·œí™”ëœ ë¶„ê¸° ì—”íŠ¸ë¡œí”¼ ë³€í™”(nVBE: normalized Variation of Branching Entropy)    
    * ìµœì†Œ ì„¤ëª… ê¸¸ì´(MDL: Minimum Description Length) ë“±ì´ í¬í•¨ëœë‹¤(Â§6.3)     


**[2) ìƒì„±ì  ëª¨ë¸]** 
* ê°€ì¥ ë†’ì€ ìƒì„± í™•ë¥ ì˜ ìµœì ì˜ ë¶„í• ì„ ì°¾ê¸° ìœ„í•œ í†µê³„ ëª¨ë¸ ì„¤ê³„ì— ì´ˆì ì„ ë§ì¶¤.     
* ëª¨ë¸ì˜ ì¢…ë¥˜     
    * ìˆ¨ê²¨ì§„ ë§ˆë¥´ì½”í”„ ëª¨ë¸(HMM: Hidden Markov Model)    
    * ê³„ì¸µì  ë””ë¦¬í´ë ˆ í”„ë¡œì„¸ìŠ¤(HDP: Hierarchical Dirichlet Process)     
    * ì¤‘ì²© í”¼íŠ¸ë§Œ-ìš”ë¥´ í”„ë¡œì„¸ìŠ¤(NPY: Nested Pitman-Yor Process) ë“±ì´ í¬í•¨ëœë‹¤(Â§5.3) 
* ğ‘›-gram ì–¸ì–´ ëª¨ë¸ì„ **neural language ëª¨ë¸**ë¡œ ëŒ€ì²´í•˜ì—¬ ì°¨ë³„ì  ì ‘ê·¼ ë°©ì‹ì„ í™•ì¥í•˜ëŠ” ê²ƒì€ ì‚¬ì†Œí•œ ì¼ì´ë‹¤.     
* ìƒì„± ì ‘ê·¼ ë°©ì‹ì˜ ê²½ìš°, ì´ì „ ì—°êµ¬ëŠ” **context encoderì™€ segment decoder**ë¥¼ ì‚¬ìš©í•˜ì—¬ **neural language ëª¨ë¸**ì„ êµ¬ì„±í•˜ë©´ í†µê³„ì  ëŒ€ì‘ë¬¼ì— ëŒ€í•œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆë‹¤(un and Deng, 2018, Â§5.2.2).






---
----


# 6 Learning subword vocabularies and segmentations
     
**[subword ë‹¨ìœ„ì˜ íŠ¹ì§•]**     
* word-level ëª¨ë¸ê³¼ character-level ëª¨ë¸ ì‚¬ì´ì˜ ì›í™œí•œ ì „í™˜ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤(Â§3)     
* pre-tokenizationë¥¼ í†µí•´ ì–»ì€ ë‹¨ì–´ ìœ ì‚¬ í† í°(word-like tokens)ì„ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•œë‹¤      
* ìœ„ì™€ ê°™ì€ ë¶„í• ì€ ê°€ëŠ¥í•œ ëª¨ë“  subword ë‹¨ìœ„ ì§‘í•©ì€ **ìœ í•œ**í•˜ë©° **í›ˆë ¨ ë°ì´í„°ì—ì„œ ê²°ì •ë  ìˆ˜ ìˆ**ì§€ë§Œ, **ëª¨ë“  ë¬¸ì**(ë˜ëŠ” ë°”ì´íŠ¸, Â§8.3 ì°¸ì¡°)**ë¥¼ í¬í•¨**í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •í•œë‹¤.     
* test timeì— ë‚˜íƒ€ë‚˜ì„œ, held-out ë°ì´í„°ì•ˆì˜ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.      
* subword ì •ë³´ì— ëŒ€í•´ ìƒê°í•˜ëŠ” ê²ƒì€ í˜•íƒœí•™ì ìœ¼ë¡œ í’ë¶€í•œ ì–¸ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ë” ë§ì€ ì „í†µì„ ê°€ì§ˆ ìˆ˜ ìˆìŒ     

**[Mikolov et al. (2012)]**
* OOV(Out of Vorder) ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì–¸ì–´ ëª¨ë¸ë§ ì˜ì–´ë¥¼ ìœ„í•œ ë‹¨ì–´ ëŒ€ì‹  **subword units18ì„ ì‚¬ìš©**í•  ê²ƒì„ ì œì•ˆí–ˆë‹¤.     

**[Sennrich et al. (2016)]**
* ìœ„ì™€ ê°™ì€ í† í¬ë‚˜ì´ì§•ì— ëŒ€í•œ ë…¸ë ¥ì—ë„ ë¶ˆêµ¬í•˜ê³ , Sennrich et al. (2016) ì´í›„ ëŒ€ë¶€ë¶„ì˜ í˜„ëŒ€ NLP ëª¨ë¸ì€ ì•„ë‹ˆë”ë¼ë„ ë§ì€ ëª¨ë¸ì—ì„œ **í¬ê³  ë¬´í•œí•œ ì–´íœ˜ í¬ê¸°ì™€ ì‹¸ìš°ëŠ” ê²ƒ**ì´ ê´€ë¡€ê°€ ë˜ì—ˆë‹¤.     


ìœ„ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ì„œ **"ë¬´ì—‡ì„ subword ë‹¨ìœ„ë¡œ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ê°€?"**ê°€ ë§¤ìš° ì¤‘ìš”í•´ì¡Œë‹¤    

**[subword ë‹¨ìœ„ì˜ ì¢…ë¥˜]**      
* ìˆ˜ë™ìœ¼ë¡œ êµ¬ì„±ë˜ê³  ì–¸ì–´ì ìœ¼ë¡œ ì •ë³´ê°€ ìˆëŠ” ê·œì¹™ ê¸°ë°˜(rule-based) ì‹œìŠ¤í…œ(Â§ 6.1)       
* ë‹¤ë¥¸ ì˜µì…˜ì€ ì „í†µì , ì–¸ì–´ì ìœ¼ë¡œ ë™ê¸° ë¶€ì—¬, í‰ê°€ëœ data-driven segmentation learners(Â§ 6.3)     
* ë¹ ë¥´ê³  ì‰¬ìš°ë©° downstream ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ë‹¨ìˆœí•œ íœ´ë¦¬ìŠ¤í‹±(Â§ 6.4)      


í•©ë¦¬ì ì¸ ë™ê¸°ì—ë„ ë¶ˆêµ¬í•˜ê³ ,     
ì•„ë˜ì˜ ì–¸ì–´ë“¤ì—ê²Œ **ë¶„í• ì€ ì¢‹ì§€ ì•Šì€ ì•„ì´ë””ì–´**ì¼ ìˆ˜ ìˆë‹¤     
â¡ ì•„ëì–´ì™€ íˆë¸Œë¦¬ì–´ì™€ ê°™ì€ Semitic ì–¸ì–´(non-concatenative morphological phenomenaê°€ ì¡´ì¬)      
â¡ ìœ„ì™€ ê°™ì€ ì–¸ì–´ëŠ” character-level ëª¨ë¸ì´ë‚˜ very small subword inventoriesí•œ ëª¨ë¸ì— ì˜í•´ ë” ì˜ ì œê³µëœë‹¤(Amrhein and Sennrich (2021))         


---

## 6.1 Manually constructed linguistic analyzers
í˜•íƒœí•™ì  ë¶„ì„ì€ í˜•íƒœí•™ì (morphologically)ìœ¼ë¡œ í’ë¶€í•œ ì–¸ì–´ì— ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.    
â¡ ì´ë¥¼ ìœ„í•´ ë‹¨ì–´ í˜•íƒœë¥¼ lemmata(ë‹¨ì–´ì˜ ê¸°ë³¸í˜•)ì™€ inflections(ë‹¨ì–´ì˜ êµ´ì ˆ)ë“¤ë¡œ ë¶„ì„í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë„êµ¬ë“¤ì´ ê°œë°œë˜ì—ˆë‹¤.  



**[Porter, 1980]**     
* ìœ„ì˜ í˜•íƒœí•™ì  ë°©ë²•ë¡ ì„ ìœ„í•œ ë„êµ¬ ì¤‘ ê°€ì¥ ì´ˆê¸°ì´ì ê°€ì¥ ìœ ëª…í•œ ê²ƒ       
* ì–¸ì–´í•™ìë“¤ì´ ìœ í•œ ìƒíƒœ(finite-state tools) ë„êµ¬(FST; Beesley and Karttunen, 2003)ë¥¼ ì‚¬ìš©í•˜ì—¬ **ìˆ˜ë™ìœ¼ë¡œ êµ¬ì„±**í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.      
â¡ ì´ëŠ” ì¢…ì¢… í˜•íƒœí•™ì  í”„ë¡œì„¸ìŠ¤ê°€ ì²´ê³„ì ì¸ ì„¤ëª…ì„ ì œê³µí•¨     
â¡ ë³µì¡í•œ ë°ì´í„° ê¸°ë°˜ ëª¨ë¸ì„ í•™ìŠµí•˜ë ¤ê³  ì‹œë„í•˜ëŠ” ê²ƒë³´ë‹¤ ìœ í•œ ìƒíƒœ ë¶„ì„ê¸°ë¥¼ **ìˆ˜ë™ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” ê²ƒì´ ë” ë¹ ë¥´ê³  ì €ë ´**í•˜ê¸° ë•Œë¬¸(Beemer et al., 2020).        
* ìœ í•œ ìƒíƒœ ë„êµ¬ì˜ ì‚¬ìš©     
   * lemmata(ë‹¨ì–´ì˜ ê¸°ë³¸í˜•)ì™€ ë‹¤ë¥¸ subword ë‹¨ìœ„ì˜ ê³µê°œ(overt) ë¶„í•  ë˜ëŠ” ë°œê²¬ì— ì‚¬ìš© ê°€ëŠ¥    
   * ìœ í•œ ìƒíƒœ ê¸°ê³„ê°€ ì…ë ¥ ë¬¸ìì—´ì˜ ì–´ë–¤ ë¶€ë¶„ì´ ì¶œë ¥ ì„ íƒìœ¼ë¡œ ê·€ê²°ë˜ì—ˆëŠ”ì§€ë¥¼ ì‰½ê²Œ ì¶”ì ê°€ëŠ¥    
   * í˜•íƒœë¡ ì ì¸ logical taggerë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„í• ì„ ìœ ë„ ê°€ëŠ¥.     
* ìœ„ì˜ ê¸°ëŠ¥ë“¤ì€ ë¬¼ë¡  ìˆ˜ë™ ì£¼ì„ì— ì˜ì¡´í•˜ì§€ë§Œ, ê·¸ ì´ìƒì€ ì¢…ì¢… ëŠë¦¬ê³  ë¶ˆí•„ìš”í•˜ê²Œ ë³µì¡í•˜ë‹¤ê³  ê°„ì£¼ëœë‹¤ëŠ” ì ì„ ì§€ì í•  ê°€ì¹˜ê°€ ìˆë‹¤.    


**[Tan et al. (2020)â€™s BITE]**
* ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  taggingì˜ lemmatization(ë‹¨ì–´ì˜ ê¸°ë³¸í˜•) ì¡°í•©ì€ í¬ê³  ì ì¬ì ìœ¼ë¡œ ë…¸ì´ì¦ˆê°€ ë§ì€ ì–´íœ˜ë¥¼ ë‹¤ë£¨ëŠ” ë° ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©ë˜ì–´ ì™”ë‹¤.       
* BITEëŠ” ë…¸ì´ì¦ˆë¥¼ ë°©ì§€í•˜ê³  ë³€ì¦ë²• ë°ì´í„°ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ êµ´ì ˆëœ í˜•íƒœë¥¼ **lemmaì™€ tagë¡œ ë³€í™˜**í•œë‹¤.      
* ì´ surveyì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì— ëŒ€í•œ ì¤‘ìš”í•œ ì°¨ì´ì ì€ ê·¸ëŸ¬í•œ ì ‘ê·¼ë²•ì´ **ë” ê°•í•´ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆë‹¤**ëŠ” ê²ƒì´ë‹¤.      
* ì•ì„œ ë§í•œ ìˆœìˆ˜ ì—°ê²° ì„¸ë¶„í™”(purely concatenative segmentation)ëŠ”    
     * ìˆœìˆ˜ ì—°ê²° ì„¸ë¶„í™”(purely concatenative segmentation)ì˜ ì˜ˆ      
     * "hopingâ€  â¡ â€œhope V.PTCP;PRSâ€        
     *  â€œateâ€ â¡ â€œeat PST,â€        
     *  ê° íŒ¨ëŸ¬ë‹¤ì„ì˜ ë‹¤ë¥¸ **í˜•ì‹ê³¼ ì •ë³´ë¥¼ ê³µìœ **í•  ìˆ˜ ìˆê²Œ í•¨       


**[Hofmann et al. (2021)]**
* ìœ„ì™€ ê°™ì€ ì ‘ê·¼ ë°©ì‹ì˜ ì´ì ì„ ë³´ì—¬     
* í† í°í™” ì „ì— ë‹¨ì–´ë¥¼ í˜•íƒœì†Œë¡œ ë¶„í• í•˜ì—¬ íŒŒìƒ ê³¼ì •ì„ ì·¨ì†Œí•˜ë©´ ì •ì„œ ë° ì£¼ì œ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ê°œì„ í•  ìˆ˜ ìˆë‹¤ê³  ê´€ì°°í•œë‹¤.      


-----


## 6.2 Other Language-Specific Methods
German, where compounds are never separated
with spaces, has prompted research into compound
splitting (Koehn and Knight, 2003; Braschler and
Ripplinger, 2004; Macherey et al., 2011). Another
tricky example is Sanskrit, where segmentation is
complicated by the fact that there are processes that
occur at / cross word boundaries (Krishna et al.,
2017; Huet, 2003). More recently, in the era of
neural and subword-based models, questions of
tokenization have most recently been researched
for Arabic, where Alyafeai et al. (2021) examined
various language-agnostic and language-specific
tokenizers and find that the performance varies depending on factors like the size and morphological
complexity of the datasets. For Chinese, Si et al.
(2021) converted characters into stroke orders or
romanization sequences before applying BPE in order to capture potential sub-character information
based on glyph or pronunciation. Park et al. (2020)
shows that a hybrid approach of morphological segmentation followed by BPE (Â§6.4) works best for
most Korean datasets.


-----


## 6.3 Unsupervised morphological segmentation
While subword representations are now commonly
evaluated based on their use for a downstream application, initial work in this space often directly
assessed the linguistic validity of subword19 segmentations by means of databases such as CELEX
(Baayen et al., 1995) or annotations from linguistic
grammars and analyzers.
In both computational models of corpora and
speakers, it has been found that â€œdistributional regularity and phonotactic constraints are useful for segmentationâ€ (Brent and Cartwright, 1996). de Marcken (1996) proposed to deconstruct text recursively from sentences over words and morphs into
characters through â€œcomposition and perturbation,â€
presenting results towards recovering linguistically
plausible words. Brent et al. (1995) proposed an
essentially minimum description length (MDL; Rissanen, 1989) based approach to morphological segmentation, in which the sum of potential vocabulary unitsâ€™ length and the length of text encoded
with this vocabulary is minimized. MDL-based
approaches were prolifically adapted and expanded
for unsupervised morphological segmentation, as
in Linguistica (Goldsmith, 2001), and found to generate segmentations with high correlations to morpheme boundaries on English and Romance languages (Baroni, 2000; Goldsmith, 2001). Initially,
these approaches were only lightly guided by additional information about possible morphological
structure or paradigmsâ€”partitioning word types
into sets of stems with either suffixes20 or prefixes,
they could not recursively split morphs into additional subwords or account for conditioned character changesâ€”and so with only one morpheme
boundary they were most appropriate only for the
languages on which they were initially tested.
The use of morphological â€˜categoriesâ€™ and additional structure within segmentation models expanded their recall and applicability. The Morfessor family21 comprises several unsupervised and
semi-supervised segmentation models which aimed
to incorporate linguistic biases to improve initial
naÃ¯ve MDL models. Morfessor 1.0 (Creutz and
Lagus, 2002), later called the Morfessor Baseline,
is a recursive MDL model based on unigram morph
frequencies and lengths; without additional structure it has a clear tendency to over-segment and
create spurious splits such as â€˜s + plit.â€™ Morfessor CatMAP (Creutz and Lagus, 2005), or categories maximum-a-posteriori, added a hierarchical HMM-based model of the sequential nature of
loose morphological categories (prefixes, stems,
and suffixes), where priors could be learned in a
semi-supervised way from wordlists; this model
remained ideal for the concatenative morphology
found in such languages as evaluated in the Morpho
Challenge22â€”English, Finnish, Turkish, German
(Kurimo et al., 2010). The FlatCat model (GrÃ¶nroos et al., 2014) flattened this structure, which
reduced accuracy under unsupervised conditions
but simplified and improved semi-supervised learning. The most recent Morfessor model, EM+Prune
(GrÃ¶nroos et al., 2020) merges the above tradition
with recent techniques, leading to a model that is a
strict generalization of Unigram LM (Kudo, 2018,
see Â§6.4.3).
Many of the approaches to morphological segmentation implicitly treated the task as paradigm
learning by incorporating the notion of morphological paradigms and inflection classes. Within
this perspective, one research arch focused on expanding the limited paradigmatic structure in early
MDL models either through explicit rules, clustering, or â€˜chainsâ€™ (Snover and Brent, 2002; Creutz
and Lagus, 2005; Monson et al., 2007, 2009; Lignos, 2010; Narasimhan et al., 2015). Another focused on improving segmentation by discovering
forms with shared paradigms, by inducing morphological relatedness across surface forms and
further allowing for e.g., spelling changes to improve discovery of shared structure across irregular forms (Schone and Jurafsky, 2001; Snover and
Brent, 2001; Yarowsky and Wicentowski, 2000;
Bergmanis and Goldwater, 2017). While segmentation from this perspective can result in more compact corpus representations, with higher segmentation recall and greater corpus-level consistency,
their precision is often lower than with e.g., Morfessor or frequency-driven techniques. As briefly
discussed in Â§6.5, use of morphological segmentation as tokenization for downstream tasks provides
only inconsistent improvements compared to the
lighter-weight techniques of Â§6.4, with recent work
predominantly electing for the simplicity of these
approaches.




-----

## 6.4 Modern fast subword segmentation
algorithms
As explained earlier, the breakthrough for subword
tokenization nowadays considered central was the
use of Byte-Pair-Encoding (BPE; Gage, 1994) by
Sennrich et al. (2016) for machine translation.23


6.4.1 BPE (Gage, 1994; Sennrich et al., 2016)
BPE is a compression algorithm from a family termed â€œmacro-schemasâ€ (Storer and Szymanski, 1982) in which substrings are replaced with
references to them. The name was coined in
Gage (1994), although equivalent algorithms have
been applied for pattern discovery in natural language (Wolff, 1975) and complexity of genetic sequences (Ãngel JimÃ©nez-MontaÃ±o, 1984) earlier.24
When learning a tokenization, BPE replaces pairs
of adjacent symbols with a new symbol representing that pair, iteratively merging all occurrences of
the pair that occurs most often at any given time.
At test time, the same procedure of merging can be
performed by executing all recorded merges in the
order in which they were conducted during training
of the tokenization model.
Byte-Level BPE (Wang et al., 2019) applies BPE
not to characters, but raw bytes (see Â§8.3); it is
used in GPT-2 (Radford et al., 2019) and other
models. BPE-dropout (Provilkov et al., 2020) is an
extension allowing for subword regularization (see
Â§6.4.3.

6.4.2 WordPiece (Schuster and Nakajima,
2012)
A very similar technique had been proposed under
the name â€œWordPieceâ€ by Schuster and Nakajima
(2012) for Japanese and Korean text (where reliance on space-separated tokens is impossible as
text is written without spaces), though it is also
used in BERT (Devlin et al., 2018) and other models. Unlike BPE, WordPiece doesnâ€™t merge the
most often co-occuring pair but pairs that increase
the likelihood that an ğ‘›-gram based language model
trained with this updated vocabulary reaches on
data (the fact that only some counts need to be
updated in such a model and the use of frequencybased heuristics for selecting and batching of multiple merge candidates keep the process computationally feasible). To segment text, WordPiece follows
a per-word left-to-right longest-match-first strategy,
allowing for very fast linear-time processing (Song
et al., 2021).


6.4.3 UnigramLM (Kudo, 2018)
Kudo (2018) picks up on the idea of judging subword candidates by evaluating their use in a language model, but it uses a simple unigram language model (hence calling the algorithm unigram
LM) and iteratively removes subword units from a
starting vocabulary that contains far more subword
units than are desired: on every iteration, the unigram LM is trained using EM and then the lowestprobability items are pruned from the vocabularyâ€”
the process is repeated a few times until a desired
vocabulary size has been reached.
Interestingly, this probabilistic setup also cleanly
models the fact that there are many possible segmentations that are consistent with a given string
(in the extreme case, one could always fall back to
characters). They report that training with sampled
segmentation (termed â€œsubword regularizationâ€) instead of just using one deterministic segmentation
indeed improves machine translation performance.
The same motivation led Provilkov et al. (2020)
to propose BPE-dropout where the skipping of individual merges in the BPE construction leads to
variety in segmentations. Subword regularization
not only has been shown to help in monolingual
in-domain tasks, but also for improving transfer in
multilingual models, see Â§7.
The observation that sampling segmentation
helps is confirmed by Hiraoka et al. (2019), who
employ a Bayesian nonparametric model (see Â§5.3)
as the LM that defines the tokenization.
Wang et al. (2021a) build a similar model where
the unigram LM is based on character-level BiLSTM encodings of the input and apply it to unsupervised Chinese Word Segmentation (see Â§5.4).25


6.4.4 SentencePiece (Kudo and Richardson,
2018)
Not itself an algorithm as often assumed, but actually a software package, SentencePiece (Kudo
and Richardson, 2018) offers both BPE and Unigram LM algorithms (so specifying â€œSentencePieceâ€ is certainly not informative enough). Importantly, unlike their other implementations it does
not treat spaces as special guaranteed word boundaries, allowing learned units to cross these boundaries and obviating the need for pre-tokenization
in languages without whitespace-tokenized words
like Chinese and Japanese.


-----


## 6.5 Comparing morphological segmentation to BPE and friends
Several studies have compared linguistically motivated segmentation with data-driven ones, without
conclusive results (to say the least).
Bostrom and Durrett (2020) claim that UnigramLM obtains better segmentation than BPE,
both qualitatively (they tend to better correspond
to morphemes and Morfessor (Creutz and Lagus,
2007) morphs) and quantitatively (they improve
BERT-style modelsâ€™ performance on modern NLP
tasks a little in English and a lot in Japanese).
When using (manually analyzed or gold) morphological analysis, Matthews et al. (2018) show
that language modeling can be improved for agglutinative languages like Turkish. In Schwartz
et al. (2020)â€™s low-resource study shows Morfessorbased language models (and character-based ones,
see Â§8) outperform BPE-based ones. Pan et al.
(2020) likewise improve NMT on Turkish and
Uyghur by using morphological analyzers before
applying BPE.
Using unsupervisedly obtained â€œmorphologicalâ€
subwords on the other hand, only Ataman and
Federico (2018b) find that a model based on Morfessor FlatCat can outperform BPE; Zhou (2018),
Domingo et al. (2018), MachÃ¡cek et al. Ë‡ (2018),
and Saleva and Lignos (2021) find no reliable improvement over BPE for translation. Banerjee and
Bhattacharyya (2018) analyze translations obtained
segmenting with Morfessor and BPE, and conclude
that a possible improvement depends on the similarity of the languages. Huck et al. (2017) propose
thus to combine both approaches.
As a possible explanation for the good performance of BPE, GallÃ© (2019) claims that the performance of BPE is related to its compression capac
ity: with respect to members from the same class
of compression algorithm, BPE performs close to
the top in data compression benchmarks.


-----


## 6.6 How many units do we need?
Another open question is the question of how many
merges (or what other prior) one should select for
optimal performance. The best-performing number
may depend on the task and the domain and differ
by language (Mielke et al., 2019; Domingo et al.,
2018).26 More and thus larger subword units allow
for and lead to more memorization (Kharitonov
et al., 2021), which may or may not be desired
depending on the application.
Predicting the number of merges that works best
without having to try different sizes would be desirable and Gowda and May (2020) claim to have
found one such heuristic: merge as much as possible to shorten the overall sequence lengths while
making sure that 95% of subword units appear at
least 100 times (presumably in training data). Their
motivation is that neural machine translation models are frequency-biased in their outputs and thus
maintaining a more uniform frequency distribution
is better.27 A similar study undertaken by Ding
et al. (2019) reiterate how contradictory suggestions for number of merges in past work are and
add that in low-resource scenarios far fewer merges
seem to be better, a trend with Transformers which
differs from that with LSTMs, leading to an interesting question: should smaller corpora mean you
canâ€™t afford characters or is it rather that you canâ€™t
afford words?
A simple online answer to the question of how to
select merges is presented by Salesky et al. (2018):
while training an NMT model using BPE segments,
gradually increase the vocabulary size by merging BPE vocabulary items, adding new, bigger
BPE segments until they obtain diminishing returns. Embeddings for the newly introduced subwords are initialized by merging the embeddings
of the two merged BPE segments with an autoencoder. Formulating the vocabulary selection prob
lem as a search for the set of tokens with the highest entropy, Xu et al. (2021) proposes an optimal
transport driven selection from BPE units that obtains vocabulary merges that often outperform a
language-independent standard setting for translation. Another recent method that comes with a
stopping criteria (and therefore dispenses with an
additional hyperparameter) is Vilar and Federico
(2021) which defines the likelihood of a vocabulary with respect to a sequence, and improves that
likelihood greedily


---
----


# 7 Shared vocabularies in multilingual models
Many NLP applications process text in more than
one language at a time, the most obvious example
perhaps being a machine translation system. In
such cases, one could either use (and potentially
learn) a tokenizer per language or a single tokenizer for both languages (also allowing sharing of
embeddings if desired). Building a highly multilingual system that translates between more than
two languages, Johnson et al. (2017) perform the
former and first encounter questions like whether
to oversample low-resource languages for learning
a data-driven tokenizer and if so to which degree.
These questions are addressed differently in the
now more common highly multilingual pre-trained
Transformers like mBERT (Devlin et al., 2018)
and XLM (CONNEAU and Lample, 2019), and
XLM-R (Conneau et al., 2020). In these models
the sharing of learned representations is hypothesized to help transfer between languages by Pires
et al. (2019), though Wu and Dredze (2019) provide inclonclusive results for this claim. It is worth
pointing out that K et al. (2020) disagree and claim
that subword overlap is not as important for transfer.
Even though all these models settle make sure to
oversample low-resource languages at least some
amount, Ãcs (2019) and Rust et al. (2021) show
that tokenization in BERT-based Transformers is
still biased towards high-resource languages. This
bias is visible in a wordâ€™s â€œfertility,â€ i.e., the number of subwords a word is split into on average
(which for example is much lower for English than
it is for, say, Finnish), but they also find it affecting
results in controlled (comparing monolingual to
multilingual tokenization) downstream tasks. Maronikolakis et al. (2021) find that these granularity
differences in tokenization between languages also
greatly affect sharing of semantic representations.
For selecting appropriate tokenization in a multilingual setting, Chung et al. (2020) offer an approach for retraining models from scratch, selecting subword vocabularies for language clusters to
explicitly control for allocation and sharing. If
on the other hand retraining from scratch is not
feasible, one option is to add new subword units
for the underresourced/oversegmented languages.
Wang et al. (2020b) and Chau et al. (2020) both
propose such additions with randomly initialized
embeddings, but these approaches did not perform
well when studied by Ebrahimi and Kann (2021);
extending the idea, Liu et al. (2021) propose to
use information about existing subword units to estimate embeddings instead of initializing newly
added units randomly (similar to Salesky et al.
(2018)). A different option is proposed by Wang
et al. (2021b), who instead force the model to use
(already existing) smaller subword units in highresource languages like English to make the segmentations across languages more similar and thus
aid transferâ€”thus avoiding the complete retraining
that comes with changing the segmentation method
or allocation. In particular, they fine-tune the model
to perform the target task 1) well with the deterministic segmentation (that undersegments highresource languages relative to low-resource ones),
2) well with sampled segmentations (so even highresource languagesâ€™ words are segmented more),
and 3) equally (in terms of a low divergence between the respective output distributions) between
the two.





















