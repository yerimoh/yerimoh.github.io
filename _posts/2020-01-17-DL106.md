---
title: "deep learning: Transformer êµ¬í˜„, ì½”ë“œ ì„¤ëª…"
date:   2020-01-17
excerpt: ""
category: [Deep Learning]
layout: post
tag:
- Deep Learning
order: 0

comments: true
---

# ëª©ì°¨
- [INRTO](#inrto)
- [Transformer ëª¨ë¸ ê°œìš”](#transformer-ëª¨ë¸-ê°œìš”)
- [Encoder](#encoder)
  * [ê°œìš”](#ê°œìš”)
  * [ì¸ì½”ë” ì½”ë“œ](#ì¸ì½”ë”-ì½”ë“œ)
  * [Embedding](#embedding)
  * [positional encoding](#positional-encoding)
  * [Self-Attention](#self-attention)
  * [Multi-Head Attention](#multi-head-attention)
- [Decoder](#decoder)
  * [ê°œìš”](#ê°œìš”-1)
  * [Masked Multi Head Attention](#masked-multi-head-attention)


---


# INRTO



**[NMT(Neural Machine Translation)]**     
Transformerë¥¼ NMTì— ì¨ë³´ê² ë‹¤.      
Transformer ì•„í‚¤í…ì²˜ì˜ ê¸°ì´ˆë¥¼ ë‹¨ê³„ë³„ë¡œ ì‚´í´ë³´ê³  ì´ë¥¼ ì‚¬ìš©í•´ ì˜ì–´ ë¬¸ì¥ì„ ë…ì¼ì–´ ë¬¸ì¥ìœ¼ë¡œ ë²ˆì—­í•´ ë³´ê² ìŠµë‹ˆë‹¤.     

<details>
<summary>ğŸ“œ NMT(Neural Machine Translation) ë” ì•Œì•„ ë³´ê¸°</summary>
<div markdown="1">
  
**[NMT(Neural Machine Translation)ì˜ ê°œë…]**    
NLP ì‘ì—… ì¤‘ í•˜ë‚˜ë¡œ,      
ì¸ê³µì‹ ê²½ë§ì„ í™œìš©í•œ ê¸°ê³„í•™ìŠµì„ í†µí•´ ì–¸ì–´ ë²ˆì—­ ëª¨ë¸ ìƒì„± ë° ë²ˆì—­ ì„œë¹„ìŠ¤ ì œê³µ ê¸°ìˆ       
NMTëŠ” ìë™í™”ëœ ì–¸ì–´ ë²ˆì—­ì„ ìœ„í•œ ì—”ë“œ íˆ¬ ì—”ë“œ ëŸ¬ë‹ ì ‘ê·¼ ë°©ì‹    

**[NMTì˜ ì›ë¦¬]**      
ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì´ ì ìš©ëœ ì—”ì§„ì„ í†µí•´ ì „ì²´ ë¬¸ë§¥ íŒŒì•… í›„ ë¬¸ì¥ ë‚´ ë‹¨ì–´, ìˆœì„œ, ë¬¸ë§¥ ì˜ë¯¸ì°¨ì´ ë“± ë°˜ì˜í•˜ì—¬ ë¬¸ì¥ë‹¨ìœ„ ê²°ê³¼ ì¶œë ¥

![image](https://user-images.githubusercontent.com/76824611/132584398-fbe03cbd-d3c6-4d7a-8c78-25c5ce536064.png)
ì¶œì²˜: Nvidia
  
</div>
</details> 




**[ëª©í‘œ]**   
ì˜ì–´ ë¬¸ì¥ì„ ë…ì¼ì–´ ë¬¸ì¥ìœ¼ë¡œ ë²ˆì—­
![image](https://user-images.githubusercontent.com/76824611/132583677-934db47f-a28f-4fb4-b4be-f1cd6f5e0043.png)


**[ì½ê¸° ìœ„í•´ í•„ìš”í•œ ì§€ì‹]**    
* [attention](https://yerimoh.github.io/DL19/)             
* [transformer](https://yerimoh.github.io/Lan/): ì •ë§ ë¬´ì¡°ê±´ë¬´ì¡°ê±´ ì •ë…í•˜ì ì´ê±° ì½ê³ ì˜¤ë©´ ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” ê»Œì´ë‹¤.     

**[ì› ë…¼ë¬¸]**    
[Attention is All You Need](https://arxiv.org/abs/1706.03762)
  
**[ì „ì²´ì ì¸ transefomer êµ¬ì¡°]**   
![transform20fps](https://user-images.githubusercontent.com/76824611/132598734-4f8602f3-b50c-4fdd-bc0d-cbf48d645c92.gif)
ì¶œì²˜: [êµ¬ê¸€](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)


----
---

ğŸ‘€, ğŸ¤·â€â™€ï¸ , ğŸ“œ    
ì´ ì•„ì´ì½˜ë“¤ì„ ëˆ„ë¥´ì‹œë©´ ì½”ë“œ, ê°œë… ë¶€ê°€ ì„¤ëª…ì„ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:)

---
----

# Transformer ëª¨ë¸ ê°œìš”
PyTorch ëª¨ë“ˆ ê¸°ë³¸ í´ë˜ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ Transformer ëª¨ë¸ ì½”ë“œ     
ê¸°ë³¸ ëª¨ë¸ì€ ì•„ë˜ì˜ forward ë©”ì„œë“œì— ë‚˜ì™€ ìˆìŒ     
â¡ ë°ì´í„°ê°€ ì¸ì½”ë”ë¥¼ í†µê³¼í•œ ë‹¤ìŒ ë””ì½”ë”ë¥¼ í†µê³¼í•¨     
![image](https://user-images.githubusercontent.com/76824611/132586029-fc33a5fd-2af6-46c3-b3ec-58fbe08a9332.png)

<details>
<summary>ğŸ‘€ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
```python
import torch.nn as nn
class TransformerModel(nn.Module):

    def __init__(self, encoder, decoder):
        super().__init__()
        self._is_generation_fast = False
        self.encoder = encoder
        self.decoder = decoder
        
    def forward(self, src_tokens, src_lengths, prev_output_tokens):
        encoder_out, padding_mask = self.encoder(src_tokens, src_lengths)
        decoder_out = self.decoder(prev_output_tokens, encoder_out, padding_mask)
        return decoder_out
```
  
</div>
</details>

----
----

# Encoder

## ê°œìš”
Transformer ë…¼ë¬¸ì—ì„œ ì¸ì½”ë”ì™€ ë””ì½”ë” ëª¨ë‘ ê°ê°  ğ=6 ê°œì˜ ë™ì¼í•œ ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ë¯€ë¡œ ì´ ë ˆì´ì–´ëŠ” 12ê°œì…ë‹ˆë‹¤.        
* 6ê°œì˜ ì¸ì½”ë” ë ˆì´ì–´ì—ëŠ” ê°ê° 2ê°œì˜ í•˜ìœ„ ë ˆì´ì–´ê°€ ìˆìŠµë‹ˆë‹¤.   
   * ì²« ë²ˆì§¸ í•˜ìœ„ ë ˆì´ì–´: [Multi-Head Self Attention ë©”ì»¤ë‹ˆì¦˜](https://yerimoh.github.io/Lan/#self-attention-%EC%9B%90%EB%A6%AC)         
   * ë‘ ë²ˆì§¸ í•˜ìœ„ ë ˆì´ì–´: [ë‹¨ìˆœí•œ Fully connected feed forward ë„¤íŠ¸ì›Œí¬](https://yerimoh.github.io/Lan/#sub-layer2-feed-forward)      

**ì¸ì½”ë”**: ì†ŒìŠ¤ ë¬¸ì¥ì„ ìˆ¨ê²¨ì§„ ìƒíƒœ ë²¡í„°ë¡œ ì¸ì½”ë”©í•˜ëŠ” ë° ì‚¬ìš© ë¨     
**ë””ì½”ë”**: ìƒíƒœ ë²¡í„°ì˜ ë§ˆì§€ë§‰ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ìƒ ì–¸ì–´ì˜ ë¬¸ìë¥¼ ì˜ˆì¸¡ í•¨        

ì¸ì½”ë“œ ë¸”ë¡ ì•ˆ     
![image](https://user-images.githubusercontent.com/76824611/132587176-6e94d0eb-8331-4ebf-a758-be277a1c874c.png)

## ì¸ì½”ë” ì½”ë“œ
ì—¬ëŸ¬ ê°œì˜ ```TransformerEncoderLayers```(ê¸°ë³¸ 6ê°œ)ê°€ ```self.layers```ì— ì¶”ê°€ë˜ì–´ ëª¨ë¸ì„ ```forward```ë¡œ í†µê³¼í•  ë•Œ í˜¸ì¶œë¨.

```TransformerEncoder.forward()```ê°€ í˜¸ì¶œë˜ë©´,     
* 1ï¸âƒ£ ì…ë ¥ ì†ŒìŠ¤ í† í°(```src_tokens```)ì´ ì„ë² ë”©ëœ(```embed_tokens``` ì‚¬ìš©) ë‹¤ìŒ í¬ì§€ì…”ë„ ì¸ì½”ë”©ì— ì¶”ê°€ë¨(ì¶”í›„ ì„¤ëª…)    
* 2ï¸âƒ£ ë“œë¡­ì•„ì›ƒ ë° ì¼ë¶€ í…ì„œ ì¡°ì‘ í›„ ì„ë² ë”©ëœ í† í° ```x```ê°€ ```for layer in self.layers```ë¥¼ ì‹œì‘í•˜ëŠ” ë£¨í”„ì—ì„œ 6ê°œì˜ ```TranformerEncoderLayers``` ê°ê°ì„ í†µê³¼í•¨    


<details>
<summary>ğŸ‘€ smplify ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
  
```python
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import math, copy, time
import seaborn as sns

from encoder_demos.demo_fairseq.models.fairseq_model import BaseFairseqModel, FairseqDecoder, FairseqEncoder
from encoder_demos.demo_fairseq.models.fairseq_incremental_decoder import FairseqIncrementalDecoder
```  
```python
import torch.nn as nn
class TransformerEncoder(nn.Module):
    """Transformer encoder."""

    def __init__(self, args, embed_tokens, left_pad=True):
        super().__init__()
        self.dropout = args.dropout
        self.fuse_dropout_add = args.fuse_dropout_add
        self.fuse_relu_dropout = args.fuse_relu_dropout

        embed_dim = embed_tokens.embedding_dim
        self.padding_idx = embed_tokens.padding_idx
        self.max_source_positions = args.max_source_positions

        self.embed_tokens = embed_tokens
        self.embed_scale = math.sqrt(embed_dim)
        self.embed_positions = PositionalEmbedding(
            args.max_source_positions, embed_dim, self.padding_idx,
            left_pad=left_pad,
            learned=args.encoder_learned_pos,
        ) if not args.no_token_positional_embeddings else None

        # 2ï¸âƒ£.2 forë¬¸ì— ë¶™ì–´ìˆëŠ” í•¨ìˆ˜ ì‹¤í–‰
        self.layers = nn.ModuleList([])
        self.layers.extend([
            TransformerEncoderLayer(args)
            for i in range(args.encoder_layers)
        ])

        self.normalize = args.encoder_normalize_before
        if self.normalize:
            self.layer_norm = FusedLayerNorm(embed_dim) if args.fuse_layer_norm else nn.LayerNorm(embed_dim)

    # ```TransformerEncoder.forward()```ê°€ í˜¸ì¶œë¨
    def forward(self, src_tokens, src_lengths):
        # 1ï¸âƒ£ embed tokens and positions
        x = self.embed_scale * self.embed_tokens(src_tokens)
        if self.embed_positions is not None:
            x += self.embed_positions(src_tokens)
        x = F.dropout(x, p=self.dropout, training=self.training)

        # B x T x C -> T x B x C
        # The tensor needs to copy transposed because
        # fused dropout is not capable of handing strided data
        if self.fuse_dropout_add :
            x = x.transpose(0, 1).contiguous()
        else :
            x = x.transpose(0, 1)

        # compute padding mask
        encoder_padding_mask = src_tokens.eq(self.padding_idx)
        if not encoder_padding_mask.any():
            _encoder_padding_mask = None
        else:
            _encoder_padding_mask = encoder_padding_mask

        # 2ï¸âƒ£.1 encoder layers : self.layersë¥¼ ì‹œì‘í•˜ëŠ” ë£¨í”„ì—ì„œ 6ê°œì˜ TranformerEncoderLayers ê°ê°ì„ í†µê³¼
        for layer in self.layers:
            x = layer(x, _encoder_padding_mask)

        if self.normalize:
            x = self.layer_norm(x)

        return x, encoder_padding_mask # x.shape == T x B x C, encoder_padding_mask.shape == B x T
```
  
</div>
</details>


<details>
<summary>ğŸ‘€ ì „ì²´ encoder ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
ìœ„ì˜ ê²½ìš°ëŠ” ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ transformer ì¤‘ ì„¤ëª…í•  ì¼ë¶€ë¥¼ ì¶”ì¶œí•´ì˜¨ ê²ƒì´ë‹¤.    
ì•„ë˜ì˜ ê²½ìš°ëŠ” ì „ì²´ ì½”ë“œì´ë¯€ë¡œ ì°¸ì¡°ë§Œ í•´ë‘ì     

forward ë©”ì„œë“œë¥¼ ì‚´í´ë³´ë©´,     
* 1ï¸âƒ£ ì„ë² ë”©ëœ í† í° xê°€ [Self-Attention](https://yerimoh.github.io/Lan/#sub-layer1-self-attention) ë©”ì»¤ë‹ˆì¦˜ì„ í†µê³¼í•¨(ì•„ë˜ ì„¤ëª… ì°¸ì¡°)          
* ```self.fuse_dropout```    
   * 2ï¸âƒ£ ê¸°ë³¸ì ìœ¼ë¡œ ```self.fuse_dropout_add```ëŠ” True: Self Attentionì˜ ê²°ê³¼ê°€ ì„ í˜• ë ˆì´ì–´ fc1ì„ í†µê³¼í•˜ê³  ë‚˜ì„œ ë“œë¡­ì•„ì›ƒì´ ì ìš©ë¨           
   * 3ï¸âƒ£ ```self.fuse_relu_dropoutì€ False```: ë‘ ë²ˆì§¸ ì„ í˜• ë ˆì´ì–´ fc2ë¥¼ í†µê³¼(ë“œë¡­ì•„ì›ƒ X) â¡ [ì¶œë ¥ì¸µì— ê°€ê¹Œìš°ë©´ ë“œë¡­ì•„ì›ƒ ì ìš© ì•ˆí•¨](https://yerimoh.github.io/DL8/#%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D)     

```python
class TransformerEncoderLayer(nn.Module):
    """Encoder layer block.
    In the original paper each operation (multi-head attention or FFN) is
    postprocessed with: dropout -> add residual -> layernorm.
    In the tensor2tensor code they suggest that learning is more robust when
    preprocessing each layer with layernorm and postprocessing with:
    dropout -> add residual.
    We default to the approach in the paper, but the tensor2tensor approach can
    be enabled by setting `normalize_before=True`.
    """

    def __init__(self, args):
        super().__init__()
        self.embed_dim = args.encoder_embed_dim
        self.self_attn = MultiheadAttention(
            self.embed_dim, args.encoder_attention_heads,
            dropout=args.attention_dropout,
        )
        self.dropout = args.dropout
        self.relu_dropout = args.relu_dropout
        self.fuse_dropout_add = args.fuse_dropout_add
        self.fuse_relu_dropout = args.fuse_relu_dropout
        self.normalize_before = args.encoder_normalize_before
        self.fc1 = Linear(self.embed_dim, args.encoder_ffn_embed_dim)
        self.fc2 = Linear(args.encoder_ffn_embed_dim, self.embed_dim)
        self.layer_norms = nn.ModuleList([FusedLayerNorm(self.embed_dim) for i in range(2)])


    def forward(self, x, encoder_padding_mask):
        residual = x

        x = self.maybe_layer_norm(0, x, before=True)
        # 1ï¸âƒ£ Self-Attention í†µê³¼
        x, _ = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask)
        # 2ï¸âƒ£ ë“œë¡­ì•„ì›ƒì´ ì ìš©
        if self.fuse_dropout_add and self.training :
            x = jit_dropout_add(x, residual, self.dropout, self.training)
        else :
            x = F.dropout(x, p=self.dropout, training=self.training)
            x = residual + x
        x = self.maybe_layer_norm(0, x, after=True)

        residual = x
        x = self.maybe_layer_norm(1, x, before=True)

        # 3ï¸âƒ£ ë“œë¡­ì•„ì›ƒ X
        if self.fuse_relu_dropout :
            x = jit_relu_dropout(self.fc1(x), self.relu_dropout, self.training)
        else :
            x = F.threshold(self.fc1(x),0,0)
            x = F.dropout(x, p=self.relu_dropout, training=self.training)
        x = self.fc2(x)

        if self.fuse_dropout_add and self.training :
            x = jit_dropout_add(x, residual, self.dropout, self.training)
        else :
            x = F.dropout(x, p=self.dropout, training=self.training)
            x = residual + x
        x = self.maybe_layer_norm(1, x, after=True)
        return x

    def maybe_layer_norm(self, i, x, before=False, after=False):
        assert before ^ after
        if after ^ self.normalize_before:
            return self.layer_norms[i](x)
        else:
            return x
```
  
</div>
</details>


## Embedding
ë‹¨ì–´ ì„ë² ë”©ì€ ë„¤íŠ¸ì›Œí¬ê°€ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°œì„ í•˜ëŠ” ë° ìˆì–´ í•µì‹¬ì„    
ê°„ë‹¨íˆ ë§í•´ì„œ, ë‹¨ì–´ ì„ë² ë”©ì€ íŠ¹ì • ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ì„     
[ë” ì•Œì•„ë³´ëŸ¬ ê°€ê¸°](https://yerimoh.github.io/DL15/#1%EF%B8%8F%E2%83%A3-embedding-%EA%B3%84%EC%B8%B5-%EB%8F%84%EC%9E%85)    


ì„ë² ë”©ì€ **ë§¨ ì•„ë˜ ì¸ì½”ë”ì—ì„œë§Œ** ë°œìƒí•œë‹¤       

ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”   
* í† í°í™”í•˜ë©´ ë¬¸ì¥ì´ "ë¬¸ì¥ì˜ ë(EOS)"ì„ ë‚˜íƒ€ë‚´ëŠ” 2ë¡œ ëë‚˜ëŠ” ë²ˆí˜¸ ëª©ë¡ìœ¼ë¡œ ë³€í™˜ë¨.       
* **í† í°í™”ëœ í‘œí˜„**ì€ **[í¬ì§€ì…”ë„ ì¸ì½”ë”ì™€ ê²°í•©](https://yerimoh.github.io/Lan/#positional-encoding)** ë˜ì–´ ì„ë² ë”©ëœ ë²¡í„°ë¥¼ ë§Œë“¤ë©° ì´ ë²¡í„°ëŠ” ê·¸ë¦¼ 4ì— í‘œì‹œëœ Self-Attention ë ˆì´ì–´ì— ëŒ€í•œ **ì…ë ¥**ì…ë‹ˆë‹¤.       
* [í† í°í™” ë” ì•Œì•„ë³´ê¸°](https://yerimoh.github.io/DL21/#3%EF%B8%8F%E2%83%A3-%ED%86%A0%ED%81%B0%ED%99%94)

<details>
<summary>ğŸ‘€ í† í°í™” ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
```python
import encoder_demos.tokenize as tok

input_text = "I am looking for a place to eat."
tokens = tok.demo(input_text)
print("\nInput text:        %s\nTokenized output: " % input_text, tokens[0].numpy())
```

ì‹¤í–‰ ê²°ê³¼
```python
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types

Input text:        I am looking for a place to eat.
Tokenized output:  [  40  105 1804   19   14  358   12 9637    5    2]
```
ì¦‰ ë¬¸ì¥ì´ ì»´í“¨í„°ê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ IDë¡œ ë³€í™©ì´ ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.    
  
</div>
</details>

ì´ì œ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë³€ê²½í•˜ê³  í† í°í™”ëœ ì¶œë ¥ ë²¡í„°ê°€ ì–´ë–»ê²Œ ë³€ê²½ë˜ëŠ”ì§€ í™•ì¸í•´ë³´ì    
â¡ ë²ˆì—­ì„ í–ˆì„ë•Œ ì˜ ì¶œë ¥ë˜ëŠ”ì§€ ì†ì‹¤ê°’ì„ ë³´ì    

<details>
<summary>ğŸ‘€ í† í°í™” í™•ì¸ ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
```python
# Translate an input sentence in English to German

import encoder_demos.functional_translation as ft
input_sentence = "I am looking for a place to eat."

e, g, h = ft.demo(input_sentence)
print("En:", e)
print("German:", g)
print('')
print("H:", h)
print('H is the hypothesis along with an average log-likelihood')
```

ì‹¤í–‰ ê²°ê³¼
```python
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| loading model(s) from /data/checkpoints/JoC_Transformer_FP32_PyT_20190304.pt
| Sentences are being padded to multiples of: 1
generated batches in  0.00028586387634277344 s
En: I am looking for a place to eat.
German: Ich bin auf der Suche nach einem Ort zu ÃŸe.

H: -0.3850874900817871
H is the hypothesis along with an average log-likelihood
```
  
  
</div>
</details>

ê°€ì„¤ ì ìˆ˜ëŠ” ë¡œê·¸ í™•ë¥ ë¡œ ì¶œë ¥ë˜ë¯€ë¡œ ìŒìˆ˜ì´ë‹¤.     
ì´ ê°€ì„¤ì˜ í™•ë¥ ì„ exp(H)ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.    

<details>
<summary>ğŸ‘€ exp(H) ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
```python
print('Probability of H = {}'.format(round(np.exp(h),4)))
```

ì‹¤í–‰ ê²°ê³¼
```python
Probability of H = 0.6804
```
ì¦‰ ì˜ ë²ˆì—­í•  í™•ë¥ ì´ $$1- (ì•„ê¹Œì˜ ì†ì‹¤ê°’)$$ ì¸ ì•½ 0.68ì´ë‹¤  
  
</div>
</details>

---

## positional encoding
[positional encoding ìì„¸íˆ ì•Œì•„ë³´ê¸°](https://yerimoh.github.io/Lan/#positional-encoding)      
í¬ì§€ì…”ë„ ì¸ì½”ë”©ì€ ì„ë² ë”©ê³¼ ì°¨ì›ì´ ë™ì¼í•˜ë¯€ë¡œ(dmodel) ë‘ ê°œë¥¼ í•©ì¹  ìˆ˜ ìˆë‹¤.     
â¡ ëª¨ë¸ì´ ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ë¥¼ ì´í•´ê°€ëŠ¥     

ì´ ë…¼ë¬¸ì—ì„  positional encodingì— ì£¼íŒŒìˆ˜ê°€ ì„œë¡œ ë‹¤ë¥¸ ì‚¬ì¸ ë° ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.    
![image](https://user-images.githubusercontent.com/76824611/132593271-968fbbeb-fe63-474b-a507-78bb2766fc42.png)
* **pos**: ìœ„ì¹˜    
* **i**: ì°¨ì›    

<details>
<summary>ğŸ“œ  ì˜ˆì œë¥¼ í†µí•œ ìœ„ì˜ ê³µì‹ ì„¤ëª… </summary>
<div markdown="1">
  
dmodel = 4ë¼ê³  ê°€ì •í•´ ë³´ì  
â¡ W ì…ë ¥ ì‹œí€€ìŠ¤ ìœ„ì¹˜ë¡œ Pos âˆˆ [0, L-1]ì€ **4ì°¨ì›** ë°°ê´€ìœ¼ë¡œ í‘œí˜„ë¨     
  
ì „ìW ë²¡í„°. i âˆˆ [0, 255]ë¥¼ ì„¤ì •í•˜ë©´,    
* **4ì°¨ì› ì„ë² ë”© ë²¡í„°ì˜ ì§ìˆ˜ ì§€ìˆ˜**: sin(pos/100002i/dmodel) í•¨ìˆ˜ë¥¼ ì‚¬ìš©     
* **4ì°¨ì› ì„ë² ë”© ë²¡í„°ì˜ í™€ìˆ˜ ì§€ìˆ˜**: cos(pos/100002i/dmodel)ë¥¼ ì‚¬ìš©      
  
ì…ë ¥ ë¬¸ì¥ì˜ ì²« ë²ˆì§¸ ìœ„ì¹˜: pos = 0     
ì„ë² ë”© ë²¡í„°ì˜ ì²« ë²ˆì§¸ ì§€ìˆ˜: k=0     
â¡ ì„ë² ë”© ë²¡í„°ì˜ ì²« ë²ˆì§¸ ì°¨ì› k = 0ì˜ ì²« ë²ˆì§¸ PE(í¬ì§€ì…”ë„ ì¸ì½”ë”©)ëŠ” sin(0/100000/4) = **ì§ìˆ˜** ì´ë‹ˆê¹Œ       
â¡ ë‘ ë²ˆì§¸ ì°¨ì› k = 1ì˜ ë‘ ë²ˆì§¸ PEëŠ” cos(0/100000/4)  = **í™€ìˆ˜** ì´ë‹ˆê¹Œ        
â¡ ì¦‰,  ë²ˆì§¸ ì°¨ì› k = 2ì™€ ë„¤ ë²ˆì§¸ ì°¨ì› k =3ì˜ ê²½ìš° PEëŠ” ê°ê° sin(0/100002/4) ë° cos(0/100002/4)ê°€ ë©ë‹ˆë‹¤.    

1ï¸âƒ£     
ì´ì œ **ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì²« ë²ˆì§¸ ë‹¨ì–´**ì— ëŒ€í•œ **í¬ì§€ì…”ë„ ì¸ì½”ë”©**ì„ ì‘ì„±í•  ìˆ˜ ìˆìŒ     
$$PE (pos =0) = [sin(0/100000/4), cos(0/100000/4), sin(0/100002/4), cos(0/100002/4)].$$$

ì´ë¥¼ ê°„ë‹¨í•œ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´,     
$$PE (pos =0) = [sin(0/100000), cos(0/100000), sin(0/100), cos(0/100)] = [0, 1, 0, 1].$$

2ï¸âƒ£   
ë‹¤ìŒ ë‹¨ê³„ëŠ” ì´ ë²¡í„°ë¥¼ ì„ë² ë”© ë²¡í„° ewì— ì¶”ê°€í•˜ê³  ìƒˆ ë²¡í„° e'wë¥¼ ì–»ëŠ” ê²ƒ      
$$e^w = PE (pos =0) + e_w$$

ì˜ˆì‹œì— ë„£ì–´ë³´ë©´(ìœ„ì˜ ì¶”ì²œ í¬ìŠ¤íŠ¸ë¥¼ ì½ì—ˆë‹¤ëŠ” ì „ì œí•˜ì—)     
* e^w = embedding+ time info    
* PE (pos =0) = positional encoding    
* ew.= enbedding    
 
![image](https://user-images.githubusercontent.com/76824611/132594177-07c90068-8390-4ac3-99ef-c75c59b586da.png)

3ï¸âƒ£   
ê·¸ëŸ° ë‹¤ìŒ pos = 1, 2, â€¦ L - 1ì—ì„œ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê° ë‹¨ì–´ì— ëŒ€í•œ $$e_w$$ë¥¼ ê³„ì‚°  
  
</div>
</details>  

**[ì½”ë“œ]**    

ì•„ë˜ì˜ [PositionalEncoding ëª¨ë“ˆ](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)ì„ ì‚¬ìš©í•´ í¬ì§€ì…”ë„ ì¸ì½”ë”©ì„ ì„ë² ë”© ë²¡í„°ì— ì¶”ê°€ ê°€ëŠ¥     

ì¸ì½”ë” ë° ë””ì½”ë” ìŠ¤íƒ ëª¨ë‘ì—ì„œ ì„ë² ë”©ê³¼ í¬ì§€ì…”ë„ ì¸ì½”ë”©ì˜ í•©ê³„ì— ë“œë¡­ì•„ì›ƒì´ ì ìš©ë¨     
* ì›ë³¸ ë¬¸ì„œì—ì„œ ê¸°ë³¸ ëª¨ë¸ì˜  ğ‘ƒğ‘‘ğ‘Ÿğ‘œğ‘  ë¹„ìœ¨ì€ 0.1      
* ì•„ë˜ëŠ” PE í•¨ìˆ˜ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì„ë² ë”© dim =20ì¸ 0ìœ¼ë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬ì„ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œì¼ ë¿ë‹ˆë‹¤!   

<details>
<summary>ğŸ‘€ ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
```python
from torch.autograd import Variable
class PositionalEncoding(nn.Module):
    "Implement the PE function."
    def __init__(self, d_model, dropout, max_len=500):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        print("dropout:", dropout)
        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model)
        
        print("d_model:", d_model)
        position = torch.arange(0.0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0.0, d_model, 2) *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        print("pe:", pe[:,0:2])
    
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        
        x = x + Variable(self.pe[:, :x.size(1)], 
                         requires_grad=False)
        return self.dropout(x)

```

ì‹œê°í™”
```python
# visualization from harvardnlp/annotated-transformer GitHub (MIT license)
plt.figure(figsize=(15, 5))
pe = PositionalEncoding(20, 0)
y = pe.forward(Variable(torch.zeros(1, 100, 20)))
plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())
_ = plt.legend(["dim %d"%p for p in [4,5,6,7]])
```
 
ê²°ê³¼
```python  
dropout: 0
d_model: 20
pe: tensor([[[0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00],
         [8.4147e-01, 5.4030e-01, 3.8767e-01, 9.2180e-01, 1.5783e-01, 9.8747e-01, 6.3054e-02, 9.9801e-01, 2.5116e-02, 9.9968e-01, 9.9998e-03, 9.9995e-01, 3.9811e-03, 9.9999e-01, 1.5849e-03, 1.0000e+00, 6.3096e-04, 1.0000e+00, 2.5119e-04, 1.0000e+00]]])
```
![image](https://user-images.githubusercontent.com/76824611/132595207-535b2b9a-ed17-4bcc-9128-0d277a09f3f9.png)
  

  
</div>
</details>

ì²« ë²ˆì§¸ ìœ„ì¹˜ì˜ í¬ì§€ì…”ë„ ì¸ì½”ë”© ê°’ì€ ë‹¤ìŒê³¼ ê°™ìŒ    
```
[ 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00]
```
ì´ ê°’ì€ ìœ„ì˜ PE ê³„ì‚°ê³¼ ì¼ì¹˜í•¨   


---

## Self-Attention  
[Self-Attention ìì„¸íˆ ì•Œì•„ë³´ê¸°](https://yerimoh.github.io/Lan/#sub-layer1-self-attention)    

 ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ë…ì¼ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ë° ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‚´í´ë³´ëŠ”ê²Œ ìš°ë¦¬ ëª©í‘œë‹¤     
 ì•„ë˜ ì…€ì€ í•˜ë‚˜ì˜ ë¬¸ì¥ì—ì„œ ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤     

ì´ ì…€ì„ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ íŒŒì¼ì´ ìˆì–´ì•¼ í•œë‹¤

* ì‚¬ì „ íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸: /data/JoC_Transformer_FP32_PyT_20190304.pt
* En-De ë°ì´í„°ì„¸íŠ¸

1ï¸âƒ£ Self-Attentionì„ ì‹œê°í™”í•´ì•¼ í•˜ëŠ” ëª¨ë“ˆì„ ì„¤ì •    
<details>
<summary>ğŸ‘€ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
```python
import encoder_demos.self_attention as sa
def normalize(arr):
    out = np.zeros_like(arr)
    for rowno in range(arr.shape[0]):
        vals = arr[rowno, :]
        out[rowno, :] = (vals - np.min(vals)) / (np.max(vals) - np.min(vals))
    return out
```
  
</div>
</details>

2ï¸âƒ£ Self-Attention í—¤ë“œ ì¤‘ í•˜ë‚˜ì˜ ê°€ì¤‘ì¹˜ ì„¤ì •

ì´ì œ Self-Attention í—¤ë“œ ì¤‘ í•˜ë‚˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤    

ì´ ë‹¨ê³„ì—ì„œ Transformerê°€ ê° ë‹¨ì–´(ì˜ˆ: "place")ë¥¼ ë¬¸ì¥ì˜ ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ì™€ ë¹„êµí•œë‹¤.     
â¡ ì´ëŸ¬í•œ ë¹„êµ ê²°ê³¼ê°€ ë¬¸ì¥ì˜ ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ì˜ Attention ì ìˆ˜/ê°€ì¤‘ì¹˜ì…ë‹ˆë‹¤.     
* Attention ì ìˆ˜: ë‹¤ë¥¸ ë‹¨ì–´ê°€ ê°ê° ì£¼ì–´ì§„ ë‹¨ì–´(ì˜ˆ: "place")ì˜ ë‹¤ìŒ í‘œí˜„ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€ë¥¼ ê²°ì •       


ì…ë ¥ ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ ëª‡ëª‡ ë‹¨ì–´ì™€ EOS(ë¬¸ì¥ì˜ ë) ë¬¸ì ê°„ì˜ ì–´í…ì…˜ ì—°ê´€ì„±ì´ ê°€ì¥ ê°•í•¨

ê° ë‹¨ì–´ì˜ **ê°€ì¥ ë†’ì€ Attention ê°’**ì„ **ì‹œê°í™”**í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ       
â¡ ì´ë ‡ê²Œ í•˜ë ¤ë©´ ì•„ë˜ì˜ ì…€ì„ ì‹¤í–‰í•˜ë˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”í•˜ëŠ” ë¼ì¸ì˜ ì£¼ì„ì„ ì œê±°(```a_w = normalize(a_w)```)

<details>
<summary>ğŸ‘€ ì œê±° ì „ ì „ì²´ë¥¼ ë³´ê¸° ìœ„í•œ ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
```python
sentence = "I am looking for a place to eat."
#sentence = "This is a much more complex sentence and, as a result, is much longer."
attn, attn_weights = sa.demo(sentence, return_early='self_attn')
a_w =attn_weights[0,:, :].cpu().numpy()  #you can print the attention weights.

sentence += " EOS"
sentence = sentence.replace(".", " .").replace(",", " ,").split(" ")

#a_w = normalize(a_w)

sns.set()
plt.figure(figsize=(15,15))
ax = sns.heatmap(a_w, vmin=0, vmax=1, cmap=sns.diverging_palette(200,10, n=200), xticklabels=sentence, square=True, yticklabels=sentence, cbar=True,cbar_kws={"shrink": .82})
ax.xaxis.set_ticks_position('top')
plt.yticks(rotation=0)
plt.show()
```
  
```python
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| loading model(s) from /data/checkpoints/JoC_Transformer_FP32_PyT_20190304.pt
| Sentences are being padded to multiples of: 1
generated batches in  0.00019598007202148438 s  
```  
![image](https://user-images.githubusercontent.com/76824611/132596521-c1b4d866-0583-428f-97e1-1159bb4f9f00.png)
  
  
  
</div>
</details>


ì´ì œ ì£¼ì„ì„ ì œê±°í•˜ê³  ì§‘ì¤‘í•´ì•¼ í•  ë¶€ë¶„ì„ ë” ì§‘ì¤‘í•´ì„œ ë³´ì    
* ì²« ë²ˆì§¸ ë‹¨ì–´ "I"ì— ëŒ€í•´ ê°€ì¥ ë†’ì€ Attention ì ìˆ˜ê°€ ë‹¨ì–´ "looking"ì— ì£¼ì–´ì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.    
* "eat"ì˜ ê²½ìš° EOS ë¬¸ìì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ê°€ ê°€ì¥ ë†’ë‹¤    
* ë‘ ë²ˆì§¸ë¡œ ë†’ì€ ì–´í…ì…˜ ì ìˆ˜ëŠ” "place"ì— ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤.

<details>
<summary>ğŸ‘€ ì§‘ì¤‘ ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
```python
sentence = "I am looking for a place to eat."
#sentence = "This is a much more complex sentence and, as a result, is much longer."
attn, attn_weights = sa.demo(sentence, return_early='self_attn')
a_w =attn_weights[0,:, :].cpu().numpy()  #you can print the attention weights.

sentence += " EOS"
sentence = sentence.replace(".", " .").replace(",", " ,").split(" ")

a_w = normalize(a_w)

sns.set()
plt.figure(figsize=(15,15))
ax = sns.heatmap(a_w, vmin=0, vmax=1, cmap=sns.diverging_palette(200,10, n=200), xticklabels=sentence, square=True, yticklabels=sentence, cbar=True,cbar_kws={"shrink": .82})
ax.xaxis.set_ticks_position('top')
plt.yticks(rotation=0)
plt.show()
```
ê²°ê³¼
```python
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| loading model(s) from /data/checkpoints/JoC_Transformer_FP32_PyT_20190304.pt
| Sentences are being padded to multiples of: 1
generated batches in  0.00020933151245117188 s
```  

![image](https://user-images.githubusercontent.com/76824611/132597034-d2f28852-41df-46e8-8e30-474afb80c077.png
 
  
</div>
</details>

----

## Multi-Head Attention
[Multi-Head Attention ë” ì•Œì•„ë³´ê¸°](https://yerimoh.github.io/Lan/#self-attention-%EC%9B%90%EB%A6%AC)     
ì…€í”„ ì–´í…ì…˜ì—ì„œ ê°œì„ ëœ ì–´í…ì…˜ì„ "Multi-Head Attention"ì´ë¼ê³  í•¨     
Multi-Head Attentionì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ ë‹¤ë¥¸ ìœ„ì¹˜ ë˜ëŠ” í•˜ìœ„ ê³µê°„ì— ì§‘ì¤‘í•  ìˆ˜ ìˆìŒ   
![image](https://user-images.githubusercontent.com/76824611/132597203-c3a0e932-ef5a-47d5-9fe3-acc8e9af3ed9.png)


----
----

# Decoder

## ê°œìš”   
![tf](https://user-images.githubusercontent.com/76824611/132572962-94a60e8b-2182-466a-8d1d-47a86ee83a14.gif)   
[ë” ì•Œì•„ë³´ê¸°](https://yerimoh.github.io/Lan/#decoder)    
ì¸ì½”ë”© ë‹¨ê³„ë¥¼ ì™„ë£Œí•œ í›„ ë””ì½”ë”© ë‹¨ê³„ê°€ ì‹œì‘ë¨   

ë””ì½”ë”ëŠ” ì¸ì½”ë”ì™€ ë§¤ìš° ìœ ì‚¬í•œ êµ¬ì¡°ë¥¼ ê°–ê³  ìˆìŒ     
* ê° ì¸ì½”ë” ë ˆì´ì–´ì˜ 2ê°œì˜ í•˜ìœ„ ë ˆì´ì–´     
* ì´ ì™¸) ë””ì½”ë”ëŠ” ì¸ì½”ë” ìŠ¤íƒì˜ ì¶œë ¥ì— ëŒ€í•´ [Multi-Head Attention](https://yerimoh.github.io/Lan/#sub-layer2-encoder-decoder-attention-layer)ì„ ìˆ˜í–‰í•˜ëŠ” ì„¸ ë²ˆì§¸ í•˜ìœ„ ë ˆì´ì–´ë¥¼ ì‚½ì…í•©ë‹ˆë‹¤.         
* ì¸ì½”ë”ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ê° í•˜ìœ„ ë ˆì´ì–´ ì£¼ìœ„ì— [Residual Connection](https://yerimoh.github.io/Lan/#the-residuals)ì´ ìˆìœ¼ë©° ë’¤ì´ì–´ ë ˆì´ì–´ ì •ê·œí™”ê°€ ì§„í–‰ë©ë‹ˆë‹¤.     




**[TransformerDecoder í´ë˜ìŠ¤]**         
ê°„ë‹¨í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì œê±°í•œ ë¼ì¸,    
* ì¼ë¶€ ì •ê·œí™”    
* Data transposition      
* argsì—ì„œ ì½ì–´ì˜¨ ê°’      
 
ì œê±°ëœ ë¼ì¸ì„ ë³´ë ¤ë©´ [ì›ë˜ êµ¬í˜„](https://github.com/NVIDIA/DeepLearningExamples/blob/8c3514071275b2805b29372f6dabe515d431416f/PyTorch/Translation/Transformer/fairseq/models/transformer.py#L298)ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

```self.layers```ì½”ë“œ
* ```TransformerDecorderLayer```ì˜ ```args.decoder_layers```(ê¸°ë³¸ 6ê°œ) ë³µì‚¬ë³¸ìœ¼ë¡œ êµ¬ì„±ëœ ```nn.ModuleList```ì„    

```forward```ë©”ì„œë“œ    
* ```TransformerEncoderLayer```ì™€ ë§¤ìš° ìœ ì‚¬í•œ ë°©ì‹: ì„ë² ë”©ëœ í† í° xê°€ Self Attention ë©”ì»¤ë‹ˆì¦˜ì„ í†µê³¼í•œ í›„ì— fc1ê³¼ fc2ë¥¼ í†µê³¼      
* **ì¸ì½”ë”ì™€ì˜ ì°¨ì´ì **:  Self-Attentionê³¼ Fully Connected ë ˆì´ì–´ ì‚¬ì´ì— **[Attention](https://yerimoh.github.io/Lan/#sub-layer1-self-attention-layer)**í”„ë¡œì„¸ìŠ¤ ì¡´ì¬     
   * xë¥¼ ë””ì½”ë”©í•˜ê³  ê²°êµ­ì—ëŠ” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” Multi-Head Attentionì´ ì‚¬ìš©ë¨     

ë””ì½”ë”© ê³¼ì •ì˜ ê° ë‹¨ê³„ëŠ” **ì¶œë ¥ ì‹œí€€ìŠ¤ì—ì„œ ì—˜ë¦¬ë¨¼íŠ¸ë¥¼ ì¶œë ¥**í•˜ë©°,      
ì´ ê²½ìš°ì—ëŠ” ì˜ì–´ ë²ˆì—­ ë¬¸ì¥ì„ ì¶œë ¥í•¨     

**ì²˜ë¦¬ ê³¼ì •**       
* 1) ì¸ì½”ë”ê°€ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ë©´ì„œ ì‹œì‘       
* 2) ê·¸ëŸ° ë‹¤ìŒ ìƒë‹¨ ì¸ì½”ë”ì˜ ì¶œë ¥ì´ ì–´í…ì…˜ ë²¡í„° K ë° V ì„¸íŠ¸ë¡œ ë³€í™˜ë¨         
* 3) ì´ëŸ¬í•œ ë²¡í„°ëŠ” ê° ë””ì½”ë”ì—ì„œ ```"ì¸ì½”ë”-ë””ì½”ë” Attention"``` ë ˆì´ì–´ì— ì‚¬ìš©ë˜ì–´ **ë””ì½”ë”ê°€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ í•´ë‹¹ ìœ„ì¹˜ì— ì§‘ì¤‘**í•  ìˆ˜ ìˆê²Œ í•´ ì¤Œ.      

ì•„ë˜ ê·¸ë¦¼ì˜ ë””ì½”ë” ì¸¡ì€ ë‹¤ìŒ ì—°ì‚°ì„ ì—°ì†í•´ì„œ ìˆ˜í–‰í•¨
![image](https://user-images.githubusercontent.com/76824611/132598107-8f1dcb47-f05c-40f3-9fca-5e8d97b23544.png)

```Step1_out = OutputEmbedding512 + PositionEncoding512```

```Step2_Mask = masked_multihead_attention(Step1_out)```

```Step2_Norm1 = layer_normalization(Step2_Mask) + Step1_out```

```Step2_Multi = multihead_attention(Step2_Norm1 + out_enc) + Step2_Norm1```

```Step2_Norm2 = layer_normalization(Step2_Multi) + Step2_Multi```

```Step3_FNN = FNN(Step2_Norm2)```

```Step3_Norm = layer_normalization(Step3_FNN) + Step2_Norm2```

```out_dec = Step3_Norm```



<details>
<summary>ğŸ‘€ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
```python
# Import libraries
import os
import io
import sys
import PIL

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from typing import Optional, Dict
import math, copy, time

from encoder_demos.demo_fairseq.models.fairseq_model import BaseFairseqModel, FairseqDecoder, FairseqEncoder
from encoder_demos.demo_fairseq.models.fairseq_incremental_decoder import FairseqIncrementalDecoder
```
  
```python
from torch import Tensor
from typing import Optional, Dict
class TransformerDecoder(FairseqIncrementalDecoder):
    """Transformer decoder."""

    def __init__(self, args, embed_tokens, no_encoder_attn=False, left_pad=False):
        super().__init__()
        self.dropout = args.dropout
        self.share_input_output_embed = args.share_decoder_input_output_embed
        self.fuse_dropout_add = args.fuse_dropout_add
        self.fuse_relu_dropout = args.fuse_relu_dropout

        embed_dim = embed_tokens.embedding_dim
        padding_idx = embed_tokens.padding_idx
        self.max_target_positions = args.max_target_positions

        self.embed_tokens = embed_tokens
        self.embed_scale = math.sqrt(embed_dim)
        self.embed_positions = PositionalEmbedding(
            args.max_target_positions, embed_dim, padding_idx,
            left_pad=left_pad,
            learned=args.decoder_learned_pos,
        ) if not args.no_token_positional_embeddings else None

        self.layers = nn.ModuleList([])
        self.layers.extend([
            TransformerDecoderLayer(args, no_encoder_attn)
            for _ in range(args.decoder_layers)
        ])


    def forward(self,
                prev_output_tokens: Tensor,
                encoder_out: Tensor,
                encoder_padding_mask: Tensor,
                incremental_state: Optional[Dict[str, Dict[str, Tensor]]]=None):
        # embed positions
        positions = self.embed_positions(
            prev_output_tokens,
            incremental_state=incremental_state,
        ) if self.embed_positions is not None else None

        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
            if positions is not None:
                positions = positions[:, -1:]

        # embed tokens and positions
        x = self.embed_scale * self.embed_tokens(prev_output_tokens)
        if positions is not None:
            x += positions
        x = F.dropout(x, p=self.dropout, training=self.training)

        # B x T x C -> T x B x C
        # The tensor needs to copy transposed because
        # fused dropout is not capable of handing strided data
        if self.fuse_dropout_add :
            x = x.transpose(0, 1).contiguous()
        else :
            x = x.transpose(0, 1)
        attn = None

        # decoder layers
        for layer in self.layers:
            x, attn = layer(
                x,
                encoder_out,
                encoder_padding_mask if encoder_padding_mask.any() else None,
                incremental_state,
            )


        return x, attn
```
  
</div>
</details>



---

## Masked Multi Head Attention
ë””ì½”ë”ì˜ Self Attention ë ˆì´ì–´: ë””ì½”ë”ì˜ ê° ìœ„ì¹˜ê°€ **í•´ë‹¹ ìœ„ì¹˜ë¥¼ í¬í•¨í•´ ë””ì½”ë”ì˜ ëª¨ë“  ìœ„ì¹˜ë¥¼ ì²˜ë¦¬**í•  ìˆ˜ ìˆê²Œ í•´ ì¤Œ.      
* ë””ì½”ë”ì—ì„œ **ìë™ íšŒê·€ ì†ì„±ì„ ìœ ì§€**í•˜ê¸° ìœ„í•´ **ì™¼ìª½ ë°©í–¥ ì •ë³´ íë¦„ì„ ë°©ì§€**í•´ì•¼í•¨         
* ì¶œë ¥ ì„ë² ë”©ì´ í•˜ë‚˜ì˜ ìœ„ì¹˜ë¡œ ì˜¤í”„ì…‹ëœë‹¤ëŠ” ì‚¬ì‹¤ì„ ê³ ë ¤í•  ë•Œ ì´ ë§ˆìŠ¤í‚¹ì€ i ë¯¸ë§Œì˜ ìœ„ì¹˜ì—ì„œ ì•Œë ¤ì§„ ì¶œë ¥ì— ë”°ë¼ì„œë§Œ ìœ„ì¹˜ iì— ëŒ€í•œ ì˜ˆì¸¡ì´ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆë„ë¡ í•¨       
* ë‹¤ì‹œ ë§í•´ì„œ, **ë¯¸ë˜ì˜ ë‹¨ì–´ê°€ ì–´í…ì…˜ì˜ ì¼ë¶€ê°€ ë˜ì§€ ì•Šë„ë¡** Masked-Multi-Head Attentionì´ ì ìš©ë¨     

[ë” ì•Œì•„ë³´ê¸°](https://yerimoh.github.io/Lan/#sub-layer1-self-attention-layer)

<details>
<summary>ğŸ‘€ì½”ë“œ ë³´ê¸°</summary>
<div markdown="1">
  
```python
def subsequent_mask(size):
    "Mask out subsequent positions."
    attn_shape = (1, size, size)
    subsequent_mask = np.tril(np.ones((1,10,10))).astype('uint8')    
    return torch.as_tensor(subsequent_mask)
```
  
```python
subsequent_mask(10)[0]
```
  
ê²°ê³¼
```python
tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)
```
ë§ˆìŠ¤í‚¹ì€ ë‹¨ì–´ì™€ ì†ŒìŠ¤ ë‹¨ì–´ ë’¤ì—("ë¯¸ë˜ì—") ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì‚¬ì´ì˜ ìœ ì‚¬ì„±ì„ ì—†ì• ëŠ” ì—­í• ì„ í•¨.    
â¡ ë‹¨ìˆœíˆ ê·¸ëŸ¬í•œ ì •ë³´ë¥¼ ì œê±°í•˜ë¯€ë¡œ ëª¨ë¸ì— ì‚¬ìš©ë  ìˆ˜ ì—†ìœ¼ë©° ì„ í–‰ ë‹¨ì–´ì™€ì˜ ìœ ì‚¬ì„±ë§Œ ê³ ë ¤ë¨      
  
</div>
</details>


<details>
<summary>ğŸ‘€ ìœ„ì˜ ë§ˆìŠ¤í‚¹ ì‹œê°í™” ë³´ê¸°</summary>
<div markdown="1">
  
```python
plt.figure(figsize=(5,5))
cmap = plt.cm.GnBu_r
plt.imshow(subsequent_mask(10)[0], cmap=cmap)
None
```
![image](https://user-images.githubusercontent.com/76824611/132598582-67783456-9b5e-4d5c-b3fb-89c27306488d.png)

  
</div>
</details>
