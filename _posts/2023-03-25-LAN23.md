---
title: "Named Entity Recognition and Normalization Applied to LargeScale Information Extraction from the Materials Science Literature ì •ë¦¬"
date:   2023-03-25
excerpt: "Named Entity Recognition and Normalization Applied to LargeScale Information Extraction from the Materials Science Literature paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---



**[ì› ë…¼ë¬¸]**     
[Named Entity Recognition and Normalization Applied to LargeScale Information Extraction from the Materials Science Literature](https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.9b00470)



-----


# ABSTRACT
**[ë¬¸ì œ]**  
materials science articlesì˜ ìˆ˜ëŠ” ì§€ë‚œ ìˆ˜ì‹­ ë…„ ë™ì•ˆ ì—¬ëŸ¬ ë°°ë¡œ ì¦ê°€í–ˆìŒ.    
â¡ <span style="background-color:#FFE6E6">**new resultë¥¼** ì´ì „ì— í™•ë¦½ëœ ë¬¸í—Œê³¼ **ì—°ê²°**í•˜ëŠ” ë° ìˆì–´ materials discovery pipelineì˜ ì£¼ìš” **ë³‘ëª© í˜„ìƒ**ì´ ë°œìƒ</span>í•œë‹¤.    

**[ë³¸ ë…¼ë¬¸ì˜ í•´ê²°]**    
* ì´ ë¬¸ì œì— ëŒ€í•œ ì ì¬ì ì¸ í•´ê²°ì±…ì€,  
ê²Œì‹œëœ ê¸°ì‚¬ì˜ <span style="background-color:#fff5b1">**êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì›ì‹œ í…ìŠ¤íŠ¸**ë¥¼ **í”„ë¡œê·¸ë˜ë° ì¿¼ë¦¬ë¥¼ í—ˆìš©í•˜ëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ í•­ëª©ì— ë§¤í•‘**í•˜ëŠ” ê²ƒ</span>ì„.      
* ì´ë¥¼ ìœ„í•´ materials science literatureì—ì„œ <span style="background-color:#fff5b1">ëŒ€ê·œëª¨ ì •ë³´ ì¶”ì¶œì„ ìœ„í•´ **named entity recognition(NER)ì´ ìˆëŠ” text mining**ì„ ì ìš©</span>í•¨    
* NER ëª¨ë¸ì€ ì•„ë˜ì˜ materials science documentì—ì„œ ì•„ë˜ì˜ **summary-level informationì„ ì¶”ì¶œ**í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆë‹¤.     
    * inorganic material mentions    
    * sample descriptors     
    * phase labels       
    * material properties and applications        
    * any synthesis and characterization methods 
* ë³¸ ë…¼ë¬¸ì˜ classifierëŠ” 87%ì˜ ì •í™•ë„(f1)ë¥¼ ë‹¬ì„±í•˜ê³  327ë§Œ ê°œì˜ materials science abstractsì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì ìš©ëœë‹¤.      
* ë³¸ ë…¼ë¬¸ì€ 8ì²œë§Œ ê°œ ì´ìƒì˜ <span style="background-color:#fff5b1">**materials-science-related named entitiesë¥¼ ì¶”ì¶œ**í•˜ê³ , ê° **abstractì˜ ë‚´ìš©ì€ êµ¬ì¡°í™”ëœ í˜•ì‹ì˜ ë°ì´í„°ë² ì´ìŠ¤ í•­ëª©ìœ¼ë¡œ í‘œí˜„**</span>ëœë‹¤.       
* ë³¸ ë…¼ë¬¸ì€ ë‹¨ìˆœí•œ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬, "meta-questions"(ê¹Œë‹¤ë¡œìš´ ì§ˆë¬¸)ì— ë‹µë³€í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.     


**[êµ¬í˜„ ë§í¬]**     
* ëª¨ë“  ë°ì´í„°ì™€ ê¸°ëŠ¥ì€ [Github](http://matscholar.com) ë° [ì›¹ì‚¬ì´íŠ¸](https://github.com/materialsintelligence/matscholar)ì—ì„œ ë¬´ë£Œë¡œ í’€ì–´ë‘     


-----

# 1. INTRODUCTION

í˜„ì¬, historical materials science ì§€ì‹ì˜ ëŒ€ë¶€ë¶„ì€ ë¹„ì •í˜• í…ìŠ¤íŠ¸ì„    
ë˜í•œ dataë„ ë„ˆë¬´ ë§ì•„ materials scientistsë“¤ì€ í‰ìƒ ë™ì•ˆ ì´ ì •ë³´ì˜ ì¼ë¶€ë§Œ ì•¡ì„¸ìŠ¤ ê°€ëŠ¥í•¨.     


**[NER(Named Entity Recognition)]**     
* [NER](https://wikidocs.net/30682)ì€ ì‹ ë¬¸ ê¸°ì‚¬ì™€ ê°™ì€ ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ëŒì˜ ì´ë¦„ê³¼ ì§€ë¦¬ì  ìœ„ì¹˜ì™€ ê°™ì€ **ì •ë³´ë¥¼ ì¶”ì¶œ**í•˜ê¸° ìœ„í•œ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ê¸°ìˆ ë¡œ ê°œë°œë¨    
* ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ë¬¸ì„œëŠ” **ë¬¸ì„œì— í¬í•¨ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ í‘œí˜„**ê°€ëŠ¥      



**[entity normalization]**         
* ì˜ë¯¸: processì—ì„œ ê° entityë¥¼ **ê³ ìœ í•œ ë°ì´í„°ë² ì´ìŠ¤ ì‹ë³„ìì— ë§¤í•‘**í•˜ëŠ” ê²ƒ         
ì˜ˆë¥¼ ë“¤ì–´,         
<center> â€œage hardeningâ€ </center>     
<center> â€œprecipitation hardeningâ€ </center>      
ìœ„ì˜ ë‘ ë¬¸ì¥ì€ ê°™ì€ ì˜ë¯¸ì„. ì´ë ‡ê²Œ ë‹¤ë¥¸ í‘œí˜„ë“¤ì„ ê°™ì€ ì‹ë³„ìì— ë§¤í•‘í•˜ëŠ” ê²ƒ                 
* âš ï¸ ì´ëŸ¬í•œ ë™ë“±ì„±ì„ ì¸ì‹í•˜ë„ë¡ ê¸°ê³„ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒì€ ë§¤ìš° ì–´ë ¤ì›€        
* âš ï¸ materials science domainì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” resourcesê°€ ì—†ìœ¼ë©° entity normalizationëŠ” ì•„ì§ ë³´ê³ ë˜ì§€ ì•ŠìŒ      




![image](https://user-images.githubusercontent.com/76824611/227841594-3b3396fb-d7a2-470c-9d5d-5b8e1b54abc3.png)



ì¬ë£Œ ê³¼í•™ í…ìŠ¤íŠ¸ì—ì„œ ìš”ì•½ ìˆ˜ì¤€ì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ë ¤ëŠ” ëŒ€ê·œëª¨ ë…¸ë ¥ì€ ì—†ì—ˆë‹¤.    
ê·¸ëŸ¬ë‚˜ ê²Œì‹œëœ ë¬¸ì„œë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ í•­ëª©ìœ¼ë¡œ í‘œì‹œí•œë‹¤ë©´, Materials informatics researchersì˜ ì•„ë˜ì˜ ë…¸ë ¥ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.      
* ìˆ˜ë°± ë˜ëŠ” ìˆ˜ì²œ ê°œì˜ materialsì— ëŒ€í•œ ì˜ˆì¸¡(ê´€ë ¨ë…¼ë¬¸ [1](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.89.094104), [2](https://arxiv.org/pdf/1706.00192.pdf))               
* ì¶œíŒëœ ë¬¸í—Œì— ëŒ€í•œ large-scale questions       


**[ë³¸ ë…¼ë¬¸]**      
* materials science ë¬¸í—Œì—ì„œ ëŒ€ê·œëª¨ ì •ë³´ ì¶”ì¶œì„ ìœ„í•´ **entity normalization**ì™€ í•¨ê»˜ **NERì„ ì ìš©**.      
* 327ë§Œ ê°œ ì´ìƒì˜  science journal articlesì— ì •ë³´ ì¶”ì¶œ      
articleì˜ abstractë¶€ë¶„ë§Œ ì‚¬ìš©(ê°€ì¥ ìš”ì•½ ë˜ì–´ìˆê¸° ë•Œë¬¸)      
* <span style="background-color:#fff5b1">**NER ëª¨ë¸**</span>     
    * 800ê°œì˜ hand-annotated abstractsë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ëœ ì‹ ê²½ë§ì„(overall f1 score of 87%)          
* <span style="background-color:#fff5b1">**Entity normalization**</span>     
    * **ë‘ entitiesê°€ ë™ì˜ì–´ì¸ì§€ ì—¬ë¶€ë¥¼ ì¸ì‹í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµ**í•˜ëŠ” supervised ML modelì‚¬ìš© (f1 ì ìˆ˜ê°€ 95%)          
    * Entity normalizationëŠ” **ë¬¸ì„œ ì¿¼ë¦¬ì—ì„œ ì‹ë³„ëœ ê´€ë ¨ í•­ëª©ì˜ ìˆ˜ë¥¼ í¬ê²Œ ì¦ê°€**ì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤.            
    * ê° **abstract**ì˜ ë¹„ì •í˜• í…ìŠ¤íŠ¸ëŠ” ë¬¸ì„œì— ëŒ€í•œ **summary-level information**í•˜ëŠ” **êµ¬ì¡°í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ í•­ëª©ìœ¼ë¡œ ë³€í™˜**ëœë‹¤.            
* ìœ„ ë‘ ë°©ë²•ìœ¼ë¡œ **large-scale information extraction**í•œ ê²°ê³¼,       
researchersì´ **ì´ì „ì—ëŠ” ë¶ˆê°€ëŠ¥í–ˆë˜ ê·œëª¨**ë¡œ ì¶œíŒëœ ë¬¸í—Œì— **ì•¡ì„¸ìŠ¤**í•˜ê³  **í™œìš©**í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤€ë‹¤.      
* ë˜í•œ ë‹¤ìŒ <span style="background-color:#fff5b1">**ë°ì´í„° ì„¸íŠ¸**ë¥¼ ê³µê°œ</span>í•¨    
    * **(i)** NERì— ëŒ€í•œ êµìœ¡ ë°ì´í„°ë¡œ ì‚¬ìš©í•  [800 hand-annotated materials science abstracts](https://doi.org/10.6084/m9.figshare.8184428)          
    * **(ii)** named entitiesë¥¼ normalized í˜•íƒœë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•œ ì„¸ë¶€ ì •ë³´ê°€ í¬í•¨ëœ [JSON íŒŒì¼](https://doi.org/10.6084/m9.figshare.8184365)             
    * **(iii)** extracted named entities, 3.2700ë§Œ ê°œì˜ science articlesì— ëŒ€í•œ [corresponding digital object identifier (DOI)](https://doi.org/10.6084/m9.figshare.8184413)    
 * section 5ì— ìš”ì•½ëœ ê²ƒì²˜ëŸ¼ ë°ì´í„°ì™€ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ì¸í„°í˜ì´ìŠ¤í•˜ê¸° ìœ„í•œ ê³µê°œ ì›¹ì‚¬ì´íŠ¸ì™€ APIë¥¼ ê³µê°œí•¨     




---
---


# 2. METHODOLOGY

ì•„ë˜ ê·¸ë¦¼ì€ named entity recognitionì— ëŒ€í•œ ì „ì²´ Workflowì„    
![image](https://user-images.githubusercontent.com/76824611/227841719-66adeaec-e0ea-4b61-ac09-ba777ba3cfa7.png)
**[STEP]**    
* **(i)** documentsê°€ ìˆ˜ì§‘ë˜ì–´ corpusì— ì¶”ê°€ë¨      
* **(ii)** í…ìŠ¤íŠ¸ preprocessed (tokenized and cleaned)       
* **(iii)** training ë°ì´í„°ì˜ ê²½ìš°, documentsì˜ small subsetì— ë ˆì´ë¸”ì´ ì§€ì •ë¨     
(SPL = symmetry/phase ë ˆì´ë¸”, MAT = ì¬ë£Œ, APL = ì‘ìš© í”„ë¡œê·¸ë¨)         
* **(iv)** labeled documentsëŠ” ë ˆì´ë¸”ë§ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ì—ì„œ ìƒì„±ëœ word embeddings(Word2vec)ì™€ ê²°í•©ëœ í›„,    
named entity recognitionì„ ìœ„í•œ neural networkë¥¼ trainì‹œí‚´     
* **(v)** neural networkì—ì„œ entities ì¶”ì¶œ     



â€» Scikit-learn, Tensorflow, Keras python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ë¨


## 2.1. Data Collection and Preprocessing. 

**[2.2.1 Document Collection]**     
* **materials science articles**(1900ë…„ì—ì„œ 2018ë…„ ì‚¬ì´ì— ì¶œíŒ)ì˜ **abstractsë¶€ë¶„**ì„ text miningí•¨     
* Elsevierâ€™s Scopusì— ì˜í•´ indexeëœ 1100ê°œ ì´ìƒì˜ ê´€ë ¨ ì €ë„ ëª©ë¡ì„ ë§Œë“¦, ì €ë„ì€ ìˆ˜ì§‘ ì‚¬ì´íŠ¸ëŠ” ì•„ë˜ì™€ ê°™ìŒ      
   * Scopus and ScienceDirect APIs 27    
   * the SpringerNature API,28     
   * web scraping for journals published by the Royal Society of Chemistry29       
   * Electrochemical Society.30     
* these articleì˜ abstracts(associated metadata including title, authors, publication year,
journal, keywords, DOI, and url)ì€ ê° ê³ ìœ í•œ IDë¥¼ í• ë‹¹ë°›ê³  ì´ì¤‘ MongoDB/ElasticSearch ë°ì´í„°ë² ì´ìŠ¤ì— ê°œë³„ ë¬¸ì„œë¡œ ì €ì¥ë¨    
* ì „ì²´ì ìœ¼ë¡œ our corpusëŠ” 3.27 million abstractsë¥¼ í¬í•¨í•œë‹¤.      







**[2.2.2. Text Preprocessing]**     
ë¬¸ì„œ ì „ì²˜ë¦¬ì˜ ë‹¨ê³„ ì•„ë˜ì™€ ê°™ë‹¤.     
* **1)** ChemDataExtractorë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°í™” 12     
   * **1-1)** raw textë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„í•     
   * **1-2)** ê° ë¬¸ì¥ì„ ê°œë³„ í† í°ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì„ í¬í•¨í•¨.      
* **2)** rule-based preprocessing steps       
   * ìì„¸í•œ ê±´ [Supporting information for](https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b00470/suppl_file/ci9b00470_si_001.pdf)ì— ìˆìŒ       
   * valid chemical formulaë¡œ ì‹ë³„ëœ í† í°ì€ ì›ì†Œì™€ order of elementsê°€ ì¤‘ìš”í•˜ì§€ ì•Šë„ë¡ ì •ê·œí™”ë¨     
   (ì˜ˆ: NiFeëŠ” Fe50Ni50ê³¼ ë™ì¼)    
   * Valence states of elementsëŠ” ë³„ë„ì˜ í† í°ìœ¼ë¡œ ë¶„í• ëœë‹¤.(ì˜ˆ: Fe(III)ëŠ” ë‘ ê°œì˜ ë³„ë„ í† í°, Feì™€ (III)ê°€ ëœë‹¤).       
   * í† í°ì´ í™”í•™ì‹ì´ë‚˜ ì›ì†Œ ê¸°í˜¸ê°€ ì•„ë‹ˆë©°, ì²« ë²ˆì§¸ ë¬¸ìë§Œ ëŒ€ë¬¸ìì¸ ê²½ìš°ì—ëŠ” ë‹¨ì–´ë¥¼ ì†Œë¬¸ìë¡œ ë§Œë“¦.      
   * ë‹¨ìœ„ê°€ ìˆëŠ” ìˆ«ìëŠ” ì¢…ì¢… ChemDataExtractorë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°í™”ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ ì¡´ì¬             
   â¡ ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ê³µí†µ ë‹¨ìœ„ë¥¼ ìˆ«ìì—ì„œ ë¶„í• í•˜ê³  ëª¨ë“  ìˆ«ìë¥¼ íŠ¹ìˆ˜ í† í° ```<nUm>```ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.     
   â¡ ì´ë ‡ê²Œ í•˜ë©´ ë‹¨ì–´ í¬ê¸°ê°€ ìˆ˜ë§Œ ê°œ ì¤„ì–´ë“¦     


   

**[2.1.3. Document Selection]**     
* inorganic materials science ë…¼ë¬¸ì— ì¤‘ì ì„ ë‘ .(ì—°êµ¬ë²”ìœ„ ë°– ë…¼ë¬¸ë„ í¬í•¨í•˜ê¸´í•¨_       
* abstractì— ê¼­ merterialì— ëŒ€í•œ ìš©ì–´ê°€ í•˜ë‚˜ ì´ìƒ ì–¸ê¸‰ë˜ì–´ì•¼ì§€ ê´€ë ¨ ë…¼ë¬¸ì´ë¼ê³  ê°„ì£¼í•¨    
â¡ ì´ëŸ¬í•œ ë¶„ë¥˜ë¥¼ ìœ„í•´ NER ëª¨ë¸ì„ êµìœ¡í•˜ê¸° ì „ì— ë¨¼ì € ë¬¸ì„œ ì„ íƒì„ ìœ„í•œ classifierë¥¼ êµìœ¡í•¨.    
* ë…¼ë¬¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ classifier     
    * ì´ ëª¨ë¸ì€ abstractì„ "ê´€ë ¨" ë˜ëŠ” "ê´€ë ¨ ì—†ìŒ"ìœ¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ì´ì§„ classifierë‹¤.     
    * train ë°ì´í„°ì˜ ê²½ìš° ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ 1094ê°œì˜ abstractì— "ê´€ë ¨" ë˜ëŠ” "ê´€ë ¨ ì—†ìŒ"ì´ë¼ëŠ” ë ˆì´ë¸”ì„ ë¶™dì¸ë‹¤.   
    * ë ˆì´ë¸”ì´ ì§€ì •ëœ abstractëŠ” classifierë¥¼ í›ˆë ¨í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤.      
    * ìš°ë¦¬ëŠ”  logistic regressionì— ê¸°ë°˜í•œ linear classifierë¥¼ ì‚¬ìš©í•œ   
    * ì—¬ê¸°ì„œ ê° ë¬¸ì„œëŠ”  term frequencyâˆ’inverse document frequency([tfâˆ’idf](https://wikidocs.net/31698)) ë²¡í„°ë¡œ ì„¤ëª…ëœë‹¤.       
    * classifierê°€ 5-fold cross-validationì—ì„œ 89%accuracy($$f_1$$) ì ìˆ˜ë¥¼ íšë“ë‹¤.      
* ì´ë ‡ê²Œ êµ¬ë¶„í•´ë‘” meterialê³¼ ê´€ë ¨ì—†ëŠ” paperì—ë„ í›ˆë ¨í•˜ì˜€ë‹¤.    
â¡ ì´ë¥¼ í†µí•´ í˜„ì¬ ë” ë„“ì€ ë²”ìœ„ì˜ ì£¼ì œ(ì˜ˆ: í´ë¦¬ë¨¸ ë¬¸í—Œ)ì— ìµœì í™”ëœ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ë„êµ¬ë¥¼ ê°œë°œí•˜ê³  ìˆë‹¤.   


-----

   
   

## 2.2. Named Entity Recognition. 

NERì„ ì‚¬ìš©í•˜ì—¬ **ë¬¸ì„œë¥¼ ìš”ì•½**í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”,    
specific entity typesì„ ì¶”ì¶œí•˜ëŠ” ë° ê´€ì‹¬ì´ ìˆë‹¤.       

í˜„ì¬ê¹Œì§€ ì¬ë£Œ ê³¼í•™ì—ì„œ ì •ë³´ í‘œí˜„ì„ ìœ„í•œ ontology ë˜ëŠ” schemaë¥¼ ì •ì˜í•˜ê¸° ìœ„í•œ [ì—¬ëŸ¬ ë…¸ë ¥](https://www.semanticscholar.org/paper/A-survey-on-knowledge-representation-in-materials-Zhang-Zhao/487255347e00dcfc252e966079d6a71fba87783e)ì´ ìˆì—ˆë‹¤.      

ë³¸ ì—°êµ¬ì—ì„œëŠ” ê° ë¬¸ì„œì— ëŒ€í•´ **ì—°êµ¬ëœ ë‚´ìš©**ê³¼ **ì—°êµ¬ ë°©ë²•**ì„ ì•Œê³ ì í•œë‹¤.     

ì´ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì•„ë˜ **7ê°€ì§€ entity labelsì„ ì„¤ê³„**í•œë‹¤.       
* ë¬´ê¸° ì¬ë£Œ(MAT)       
* ëŒ€ì¹­/ìœ„ìƒ ë ˆì´ë¸”(SPL)     
* ìƒ˜í”Œ ì„¤ëª…ì(DSC)     
* ì¬ë£Œ íŠ¹ì„±(PRO)     
* ì¬ë£Œ ì ìš©(APL)     
* í•©ì„± ë°©ë²•(SMT)      
* íŠ¹ì„±í™” ë°©ë²•(CMT)    



ì´ëŸ¬í•œ labelsì˜ ì„ íƒì€ ì˜ ì•Œë ¤ì§„ ì•„ë˜ì˜ **materials science ì‚¬ë©´ì²´**ì— ì˜í•´ ì–´ëŠ ì •ë„ ë™ê¸° ë¶€ì—¬ëœë‹¤.       
* "ê°€ê³µ(processing)"     
* "êµ¬ì¡°(structure)"    
* "íŠ¹ì„±(properties)"     
* "ì„±ëŠ¥(performance)" 


ê° **íƒœê·¸ì— ëŒ€í•œ ì˜ˆ**ëŠ” ê° **íƒœê·¸ì— ì£¼ì„ì„ ë‹¬ê¸° ìœ„í•œ ê·œì¹™**ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ [Supporting information(S.4)](https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.9b00470/suppl_file/ci9b00470_si_001.pdf)ì— ì œê³µëœë‹¤.    

<details>
<summary>ğŸ“œ Supporting information(S.4)</summary>
<div markdown="1">
   
The rules for annotating each entity type are shown below.

* **1. Material (MAT):**    
Any inorganic solid or alloy, any non-gaseous element (at RT),      
e.g., â€œBaTiO3â€, â€œtitaniaâ€, â€œFeâ€.       
* **2. Symmetry/phase label (SPL):**       
Names for crystal structures/phases(ì •ë°©êµ¬ì¡° ì´ëŸ°ê±°),       
e.g., â€œtetragonalâ€, â€˜fccâ€, â€œrutileâ€, â€œperovskiteâ€; or, any symmetry label such as â€œP bnmâ€, or â€œP nmaâ€.    
* **3. Sample descriptor (DSC):**       
Special descriptions of the type/shape of the sample.(ì„±ì§ˆì„ ê°–ëŠ” íŠ¹ì´í•œ êµ¬ì¡°)         
Examples inlcude â€œsingle crystalâ€, â€œnanotubeâ€, â€œquantum dotâ€.       
* **4. Property (PRO)**:     
Anything measurable that can have a unit and a value,(ë‹¨ìœ„ì™€ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ì¸¡ì • ê°€ëŠ¥í•œ ëª¨ë“  ê²ƒ)      
e.g., â€œconductivityâ€, â€œband gapâ€;(ì „ë„ì„±, ì—ë„ˆì§€ê°­ ì´ëŸ°ê±°)    
or, any qualitative property or phenomenon exhibited by a material,(ë¬¼ì§ˆì— ì˜í•´ ë‚˜íƒ€ë‚˜ëŠ” ëª¨ë“  ì§ˆì  íŠ¹ì„± ë˜ëŠ” í˜„ìƒ)    
e.g., â€œferroelectricâ€, â€œmetallicâ€.     
* **5. Application (APL):**        
Any high-level application such as â€œphotovoltaics(íƒœì–‘ì „ì§€)â€, or any specific device such as â€œfield-effect transistorâ€.    
* **6. Synthesis method (SMT):**     
Any technique for synthesising a material,      
e.g., â€œpulsed laser depositionâ€, â€œsolid state reactionâ€,      
or any other step in sample production such       
as â€œannealingâ€ or â€œetchingâ€.
* **7. Characterization method (CMT):**     
Any method used to characterize a material, experiment or theory:     
e.g., â€œphotoluminescenceâ€, â€œXRDâ€, â€˜tight bindingâ€, â€œDFTâ€.     
It can also be a name for an equation or model.     
such â€œBethe-Salpeter equationâ€.  
   
   
   
Overall, we annotated 800 materials science abstracts. This consisted of 22,306 annotated
entities (out of 111380 tokens total).  
   
   
  
</div>
</details>  




ìœ„ì—ì„œ ì„¤ëª…í•œ íƒœê·¸ ì§€ì • ì²´ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ 800ê°œì˜ materials science abstractsì— **ì†ìœ¼ë¡œ ì£¼ì„**ì„ ë‹¬ì•˜ë‹¤.     

ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ abstractsì— ì£¼ì„ì„ ë‹¬ ìˆ˜ ìˆëŠ” "ì •í™•í•œ" ë°©ë²•ì€ ì—†ë‹¤ê³  ê°•ì¡°í•œë‹¤.      
ê·¸ëŸ¬ë‚˜ ë ˆì´ë¸”ë§ ì²´ê³„ê°€ í•©ë¦¬ì ì´ë¼ëŠ” ê²ƒì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ì£¼ì„ì„ë‹¨ ì¬ë£Œê³µí•™ìë“¤ê³¼ì˜ ì¼ì¹˜ìœ¨ì„ íŒŒì•…í•œ ê²°ê³¼ ì´ëŠ” 87.4%ì˜€ë‹¤.      
ì´ ê°’ì€ ë‘ ì£¼ì„ìê°€ ë™ì¼í•œ ë ˆì´ë¸”ì„ í• ë‹¹í•œ í† í°ì˜ ë°±ë¶„ìœ¨ë¡œ ê³„ì‚°ë˜ì—ˆë‹¤.    



ì£¼ì„ì˜ ê²½ìš° **IOB(Inside-Outside-Beginning)** í˜•ì‹ì„ ì‚¬ìš©í•œë‹¤.   

ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì™€ ê°™ì€ ë¬¸ì¥ì€ IOBë¡œ ì•„ë˜ì™€ ê°™ì´ íƒœê¹…í•œë‹¤.   



```python
Thin films of SrTiO3 were deposited     

# IOB tagging
(Thin; B-DSC)     ## DSCë€ íƒœê·¸ì˜ ì‹œì‘     
(films; I-DSC)    ## DSCë€ íƒœê·¸ì˜ ì•ˆ      
(of; O)           ## meterial entityê³¼ ìƒê´€ì—†ì–´ì„œ outside     
(SrTiO3; B-MAT)   ## MATë€ íƒœê·¸ì˜ ì‹œì‘       
(were; O)         ## meterial entityê³¼ ìƒê´€ì—†ì–´ì„œ outside       
(deposited; O)    ## meterial entityê³¼ ìƒê´€ì—†ì–´ì„œ outside     
```   
   
-----
   
## 2.3. Neural Network model. The neural network
architecture for our model is based on that of Lample et al.;34 a schematic of this architecture is shown in Figure 2a. We explain the key features of the model below.

The aim is to train a model in such a way that materials science knowledge is encoded; for example, we wish to teach a computer that the words â€œaluminaâ€ and â€œSrTiO3â€ represent materials, whereas â€œsputteringâ€ and â€œMOCVDâ€ correspond to synthesis methods. There are three main types of information that can be used to teach a machine to recognize which words correspond to a specific entity type: (i) word representation, (ii) local (within sentence) context, and (iii) word shape.

For (i), word representation, we use word embeddings. Word embeddings map each word onto a dense vector of real numbers in a high-dimensional space. Words that have a similar meaning, or are frequently used in a similar context, will have a similar word embedding. For example, entities such as â€œsputteringâ€ and â€œMOCVDâ€ will have similar vector representations; during training, the model learns to associate these word vectors as synthesis methods. The word embeddings are generated using the Word2vec approach of Mikolov et al.;26 the embeddings are 200-dimensional and are based on the skip-gram approach. Word embeddings are generated by training on our corpus of 3.27 million materials science abstracts. More information about the training of word embeddings is included in the Supporting Information (S.2).

**[ë³¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ì˜ base model]**        
* [[Lample ë“±ì˜ ì•„í‚¤í…ì²˜](https://arxiv.org/abs/1603.01360)](https://arxiv.org/abs/1603.01360)     
ì´ ì•„í‚¤í…ì²˜ì˜ ê°œëµë„ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.    
<img width="198" alt="image" src="https://user-images.githubusercontent.com/76824611/228559040-1f69d0dc-55fb-49cb-a0a5-95fb22c4bb92.png">


ë³¸ ëª¨ë¸ì˜ **íŠ¹ì§•**ì€ ì•„ë˜ì™€ ê°™ë‹¤.       



**[base modelì˜ íŠ¹ì§•]**      
* **ëª©í‘œ**    
**materials science knowledgeì´ ì¸ì½”ë”©ë˜ëŠ” ë°©ì‹**ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒì´ë‹¤.        
ì˜ˆë¥¼ ë“¤ì–´, "ì•Œë£¨ë¯¸ë‚˜"ì™€ "SrTiO3"ë¼ëŠ” ë‹¨ì–´ê°€ ì¬ë£Œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°˜ë©´ "í¼í„°ë§"ê³¼ "MOCVD"ëŠ” í•©ì„± ë°©ë²•ì— í•´ë‹¹í•œë‹¤ëŠ” ê²ƒì„ ì»´í“¨í„°ì— ê°€ë¥´ì¹˜ê³  ì‹¶ìŠµë‹ˆë‹¤. (i) ë‹¨ì–´ í‘œí˜„, (ii) ë¡œì»¬(ë¬¸ì¥ ë‚´) ì»¨í…ìŠ¤íŠ¸, (iii) ë‹¨ì–´ ëª¨ì–‘ ë“± íŠ¹ì • ì—”í„°í‹° ìœ í˜•ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë¥¼ ì¸ì‹í•˜ë„ë¡ ê¸°ê³„ì— ê°€ë¥´ì¹˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ì •ë³´ ìœ í˜•ì´ ìˆìŠµë‹ˆë‹¤.

(i) ë‹¨ì–´ í‘œí˜„ì˜ ê²½ìš° ë‹¨ì–´ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‹¨ì–´ ì„ë² ë”©ì€ ê° ë‹¨ì–´ë¥¼ ê³ ì°¨ì› ê³µê°„ì—ì„œ ì‹¤ìˆ˜ì˜ ì¡°ë°€í•œ ë²¡í„°ì— ë§¤í•‘í•©ë‹ˆë‹¤. ìœ ì‚¬í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ê±°ë‚˜ ìœ ì‚¬í•œ ë§¥ë½ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ëŠ” ìœ ì‚¬í•œ ë‹¨ì–´ ì„ë² ë”©ì„ ê°€ì§‘ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "í¼í„°ë§" ë° "MOCVD"ì™€ ê°™ì€ ì—”í‹°í‹°ëŠ” ìœ ì‚¬í•œ ë²¡í„° í‘œí˜„ì„ ê°€ì§ˆ ê²ƒì…ë‹ˆë‹¤. í›ˆë ¨ ì¤‘ì— ëª¨ë¸ì€ ì´ëŸ¬í•œ ë‹¨ì–´ ë²¡í„°ë¥¼ í•©ì„± ë°©ë²•ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. ë‹¨ì–´ ì„ë² ë”©ì€ Mikolov ë“±ì˜ Word2vec ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë©ë‹ˆë‹¤.;26 ì„ë² ë”©ì€ 200ì°¨ì›ì´ë©° ìŠ¤í‚µ-ê·¸ë¨ ì ‘ê·¼ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ë‹¨ì–´ ì„ë² ë”©ì€ 327ë§Œ ê°œì˜ ì¬ë£Œ ê³¼í•™ ì¶”ìƒì²´ì— ëŒ€í•œ êµìœ¡ì„ í†µí•´ ìƒì„±ë©ë‹ˆë‹¤. ë‹¨ì–´ ì„ë² ë”© í›ˆë ¨ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì§€ì› ì •ë³´(S.2)ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
puts/outputs from
different components of the model. The model in (a) represents the
word-level bidirectional LSTM, which takes as input a sequence of
words and returns a sequence of entity tags in IOB format. The wordlevel features for this model are the word embeddings for each word,
which are concatenated with the output of the character-level LSTM
run over the same word

ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì•„ë˜ ëª¨ë¸ì˜ ì£¼ìš” íŠ¹ì§•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.


For (ii), context, the model considers a sentence as a sequence of words, and it takes into account the local context of each word in the sentence. For example, in the sentence â€œThe band gap of ___ is 4.5 eVâ€, it is quite clear that the missing word is a material, rather than a synthesis method or some other entity, and this is obvious from the local context alone. To include such contextual information, we use a recurrent neural network (RNN), a type of sequence model that is capable of sequence-to-sequence (many-to-many) classification. As traditional RNNs suffer from problems in dealing with long-range dependencies, we use a variant of the RNN called long short-term memory (LSTM).35 In order to capture both forward and backward context, we use a bidirectional LSTM; in this way, one LSTM reads the sentence forward and the other reads it backward, with the results being combined.

For (iii), word shape, we include character-level information about each word. For example, material formulas like â€œSrTiO3â€ have a distinct shape, containing uppercase, lowercase, and numerical characters, in a specific order; this word shape can be used to help in entity classification. Similarly, prefixes and suffixes provide useful information about entity type; for example, the suffix â€œiumâ€, for example in â€œstrontiumâ€, is commonly used for elemental metals, so a word that has this suffix has a good chance of being part of a material name. In order to encode this information into the model, we use a character-level bidirectional LSTM over each word [Figure 2b]. The final outputs from the character-level LSTM (100 dimensional vectors) are concatenated with the word embeddings for each word; these final vectors are used as the word representations for the word-level LSTM [Figure 2a].

For the word-level LSTM, we use pretrained word embeddings that have been trained on our corpus of over 3.27 million abstracts. For the character-level LSTM, the character embeddings are not pretrained, but are learned during the training of the model.

The output layer of the model is a conditional random fields (CRF) classifier, rather than a typical softmax layer. Being a sequence-level classifier, the CRF is better at capturing the strong interdependencies of the output labels.36

The model has a number of hyperparameters, including the word- and character-level LSTM size, the character embedding size, the learning rate, and drop out. The NER performance was optimized by repeatedly training the model with randomly selected hyperparameters; the final model chosen was the one with the highest accuracy when assessed on the development set
   
   
## 2.4. Entity Normalization.    
After entity recognition, the
final step is entity normalization. This is necessarily required as
each entity may be written in numerous forms. For example,
â€œTiO2â€, â€œtitaniaâ€, â€œAO2 (A = Ti)â€, and â€œtitanium dioxideâ€ all
refer to the same specific stoichiometry: TiO2. For document
querying, it is important to store these entities in a normalized
format, so that a query for documents that mention â€œtitaniaâ€
also returns documents that mention â€œtitanium dioxideâ€. In
order to normalize material mentions, we convert all material
names into a canonical normalized formula. The normalized
formula is alphabetized and divided by the highest common
factor of the stoichiometry. In this way, â€œTiO2â€, â€œtitaniaâ€, and
â€œtitanium dioxideâ€ are all normalized to â€œO2Tiâ€. In some cases,
multiple stoichiometries are extracted from a single material
mention; for example, â€œZrxTi1âˆ’xO3 (x = 0, 0.5, 1)â€ is converted
to â€œO2Tiâ€, â€œO4TiZrâ€, and â€œO2Zrâ€. When a continuous range is
given, e.g, 0 â‰¥ x â‰¤ 1, we increment over this range in steps of
0.1. Material mentions are normalized using regular
expressions and rule-based methods, as well as by making
use of the PubChem lookup tables;37 final validity checks on
the normalized formula are performed using the pymatgen
library.38



Normalization of other entity types is also crucial for
comprehensive document querying. For example, â€œchemical
vapor depositionâ€, â€œchemical-vapour depositionâ€, and â€œCVDâ€
all refer to the same synthesis technique; i.e., they are
synonyms for this entity type. In order to determine that two
entities have the same meaning, we trained a classifier that is
capable of determining whether or not two entities are
synonyms.



The model uses the word embeddings for each entity as
features; after performing NER, each multiword entity is
concatenated into a single word, and new word embeddings
are trained such that every multiword entity has a single vector
representation. Each synonym consists of an entity pair, so the
features for the model are created by concatenating the word
embeddings of the two entities in question. In addition to the
word embeddings, which mostly capture the context in which
an entity is used, several other handcrafted features are
included (Supporting Information, S.5). To train the model,
10 000 entity pairs are labeled as being either synonyms or not
(see the Supporting Information, S.6). Using this data, a binary
random forest classifier is trained to be able to predict whether
or not two entities are synonyms of one another.



Using the synonym classifier, each extracted entity can be
normalized to a canonical form. Each entity is stored as its
most frequently occurring synonym (we exclude acronyms as a
normalized form); for example, â€œchemical vapor depositionâ€,
â€œchemical-vapour depositionâ€, and â€œCVDâ€ are all stored as
â€œchemical vapor depositionâ€, as this is the most frequently
occurring synonym that is not an acronym.




