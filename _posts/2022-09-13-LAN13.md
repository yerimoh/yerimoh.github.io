---
title: "FastText: Bag of Tricks for Efficient Text Classification 정리"
date:   2022-11-10
excerpt: "Bag of Tricks for Efficient Text Classification"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# intro
 
      

## 핵심  

## 읽기 위해 필요한 지식
* [word2vec](https://yerimoh.github.io/DL14/): baseline 모델이기 때문에 꼭 알아야 한다.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec 속도 개선으로 이 포스팅도 꼭 알아야 한다.      

## 원 논문
[Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)


---

# 목차  


---

# Abstract
이 논문은 text classification를 위한 간단하고 효율적인 baseline을 탐구한다.      
우리의 실험은 [fastText](https://yerimoh.github.io/LAN7/)가 accuracy 측면에서 딥 러닝 classifiers와 동등하지만, 훈련과 평가에선 이 모델이 몇 배나 더 빠르다는 것을 보여준다.      

이 논문은 standard multicore CPU를 사용하여 10분 이내에 10억 개 이상의 단어에 대해서 **빠르게 텍스트를 train**할 수 있고, 312K 클래스 중 50만 개의 문장을 1분 이내에 분류할 수 있다.   

✨ **즉 빠르게 train하며 빠르게 분류가 가능한 모델을 만들어 냈다**는 것이다 ✨ 

---


# 1. Introduction

**[기존 모델들의 문제]**    
* 텍스트 분류는 웹 검색, 정보 검색, 순위 및 문서 분류와 같은 많은 응용 프로그램이 있는 자연어 처리에서 중요한 작업이다.  
또한 이는 14도부터 신경망을 기반으로 한 모델이 점점 더 인기를 끌고 있다.    
➡ 이러한 모델은 실제로 매우 우수한 성능을 달성하지만,     
**훈련과 테스트 시간 모두에서 상대적으로 느린 경향**이 있어 매우 큰 데이터 세트에서의 사용엔 한계가 존재한다.     


**[linear classifiers의 잠재력]**        
* [linear classifiers](https://en.wikipedia.org/wiki/Linear_classifier)는 텍스트 분류 문제의 강력한 baselines으로 간주된다.     
이것은 단순하지만 적절하게 사용하면 stateof-the-art 성능을 얻을 수 있다.              
또한 **매우 큰 말뭉치로 확장할 수 있는 잠재력**도 가지고 있다.       




**[linear classifiers를 모델에 적용]**    
* 본 논문은 텍스트 분류의 맥락에서 이러한 baselines을 **large output space을 가진 매우 큰 말뭉치로 확장**하는 방법을 탐구한다.      
* [Word2vec](https://arxiv.org/abs/1301.3781)의 연구에서 영감을 받아,   
rank 제약과 빠른 loss 근사를 가진 **linear models이 10분 이내에 10억 단어에 대해 훈련할 수 있는 동시에 최첨단 수준의 성능을 달성할 수 있음을 보여준다**.       
* evaluation: 우리는 태그 예측과 감정 분석이라는 두 가지 다른 작업에서 [fastText](https://github.com/facebookresearch/fastText) 접근 방식의 품질을 평가한다.           




----


# 2. Model architecture


**[linear classifiers의 사용]**     
* 문장 분류를 위한 간단하고 효율적인 기준은 문장을 [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) (BoW) 으로 표현하고,     
linear classifier(예: logistic regression 또는 [SVM](https://yerimoh.github.io/ML3/)를 훈련하는 것이다.      


**[linear classifiers의 개선]**     
* 그러나 linear classifiers는 **features 및 class 간에 매개 변수를 공유하지 않는다**.      
* 이는 **예제가 거의 없는 large output space에서 일반화를 제한**할 수 있다.      
➡ 이 문제에 대한 일반적인 해결책은 linear classifier를 **low rank matrices로 분해**하거나 **다층 신경망을 사용**하는 것이다.   




**[모델 구성]**   
본 논문의 모델은 아래와 같다.
이는 rank 제약 조건이 있는 simple linear model을 보여준다.     

<img width="323" alt="image" src="https://user-images.githubusercontent.com/76824611/210780968-f9457e60-d22b-4410-889b-36e5ab4f57fe.png">
Figure 1: Model architecture of fastText for a sentence with N ngram features x1, . . . , xN . The features are embedded and averaged to form the hidden variable.

그림 1: Ngram 특징이 있는 문장에 대한 fastText의 모델 아키텍처 x1, . ., xN. 특징들은 숨겨진 변수를 형성하기 위해 내장되고 평균화된다.


1️⃣ 첫 번째 가중치 행렬 A는 단어 위의 look-up table이다.       
2️⃣ 그런 다음 단어 표현은 text representation으로 평균화된다.             
3️⃣ text representation은 linear classifier에 입력된다.      
   text representation은 잠재적으로 재사용될 수 있는 hidden variable입니다.     


<details>
<summary>📜 look-up table이란? </summary>
<div markdown="1">
 
 
Embedding 레이어는 **입력으로 들어온 단어를 분산 표현으로 연결해 주는 역할**을 하는데,     
그것이 **Weight에서 특정 행을 읽어오는 것과 같아** **이 레이어를 룩업 테이블(Lookup Table)**이라고 부르기도 합니다.   
 
이게 무슨 소리냐면, 
임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 **각 단어들은 모두 정수 인코딩**이 되어있어야 한다.       

특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 **임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있다.**    
![image](https://user-images.githubusercontent.com/76824611/210789841-ddb0d5c9-212d-4459-a8b5-4b92a17a8c29.png)
![image](https://user-images.githubusercontent.com/76824611/210788684-7cfd8297-99ea-4ac3-8d2f-57077702f69f.png)

이해가 안되다면 이 논문의 기반 논문인 [Word2vec](https://yerimoh.github.io/DL14/)를 제대로 알고오자.   

</div>
</details>


이 아키텍처는 중간 단어가 레이블로 대체되는 [cbow 모델](https://yerimoh.github.io/DL14/#cbow-%EB%AA%A8%EB%8D%B8)과 유사하다.      
[소프트맥스 함수](https://yerimoh.github.io/DL2/#%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4-%ED%95%A8%EC%88%98-softmax-function) $$f$$를 사용하여 사전 정의된 클래스에 대한 확률 분포를 계산한다.       
N개의 문서 집합의 경우, 이것은 클래스에 대한 negative loglikelihood를 최소화한다.     



<img width="354" alt="image" src="https://user-images.githubusercontent.com/76824611/210780823-b4f9953e-cfed-4628-8848-9b67c3aca1fa.png">
* 여기서 $$x_n$$은 n번째 document의 정규화된 bag of features이다.     
* $$y_n the label$$ A와 B의  weight matrices이다.        
* 이 model 은 [stochastic gradient descent](https://yerimoh.github.io/DL5/#%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95-sgd)와 linearly decaying learning rate를 사용하여 여러 CPU에서 비동기식으로 훈련된다.












<details>
<summary>📜 Negative Log-Likelihood (NLL) 보기</summary>
<div markdown="1">

실제 softmax을 쓸 때는 negative log-likelihood(NLL)와 사용된다.     

<img width="224" alt="image" src="https://user-images.githubusercontent.com/76824611/210790935-eb6ad9f7-dc6a-4ab0-8aca-4fc8ec44b626.png">

즉, 비유를 하자면 소프트맥스함수는 행복지수를 찾는 것이라면,      
Negative Log-Likelihood (NLL)는 불행지수를 찾는것이다.     

Negative Log-Likelihood (NLL)은 아래와 같은데,   
input이 0일 때 무한으로 가고, input이 1일 때 0으로 간다.     
![image](https://user-images.githubusercontent.com/76824611/210791528-7e2f0770-0c33-4377-ab4a-3c505efe13e4.png)

즉 정리해보면 아래와 같은 관계를 갖는다.   
![image](https://user-images.githubusercontent.com/76824611/210791835-8d044c5e-e3f7-43c5-8ab8-27ce3200cb5d.png)

loss를 계산할 때 우리는 정답 class에 대한 높은 확률은 낮은 loss로 이어지는 것을 볼 수 있다.           


</div>
</details>  

