---
title: "FastText: Bag of Tricks for Efficient Text Classification ì •ë¦¬"
date:   2022-11-10
excerpt: "Bag of Tricks for Efficient Text Classification"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# intro
 
      

## í•µì‹¬  

## ì½ê¸° ìœ„í•´ í•„ìš”í•œ ì§€ì‹
* [word2vec](https://yerimoh.github.io/DL14/): baseline ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ê¼­ ì•Œì•„ì•¼ í•œë‹¤.        
* [Embedding](https://yerimoh.github.io/DL15/): word2vec ì†ë„ ê°œì„ ìœ¼ë¡œ ì´ í¬ìŠ¤íŒ…ë„ ê¼­ ì•Œì•„ì•¼ í•œë‹¤.      

## ì› ë…¼ë¬¸
[Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)


---

# ëª©ì°¨  


---

# Abstract
ì´ ë…¼ë¬¸ì€ text classificationë¥¼ ìœ„í•œ ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì¸ baselineì„ íƒêµ¬í•œë‹¤.      
ìš°ë¦¬ì˜ ì‹¤í—˜ì€ [fastText](https://yerimoh.github.io/LAN7/)ê°€ accuracy ì¸¡ë©´ì—ì„œ ë”¥ ëŸ¬ë‹ classifiersì™€ ë™ë“±í•˜ì§€ë§Œ, í›ˆë ¨ê³¼ í‰ê°€ì—ì„  ì´ ëª¨ë¸ì´ ëª‡ ë°°ë‚˜ ë” ë¹ ë¥´ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.      

ì´ ë…¼ë¬¸ì€ standard multicore CPUë¥¼ ì‚¬ìš©í•˜ì—¬ 10ë¶„ ì´ë‚´ì— 10ì–µ ê°œ ì´ìƒì˜ ë‹¨ì–´ì— ëŒ€í•´ì„œ **ë¹ ë¥´ê²Œ í…ìŠ¤íŠ¸ë¥¼ train**í•  ìˆ˜ ìˆê³ , 312K í´ë˜ìŠ¤ ì¤‘ 50ë§Œ ê°œì˜ ë¬¸ì¥ì„ 1ë¶„ ì´ë‚´ì— ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.   

âœ¨ **ì¦‰ ë¹ ë¥´ê²Œ trainí•˜ë©° ë¹ ë¥´ê²Œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë§Œë“¤ì–´ ëƒˆë‹¤**ëŠ” ê²ƒì´ë‹¤ âœ¨ 

---


# 1. Introduction

**[ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ ë¬¸ì œ]**    
* í…ìŠ¤íŠ¸ ë¶„ë¥˜ëŠ” ì›¹ ê²€ìƒ‰, ì •ë³´ ê²€ìƒ‰, ìˆœìœ„ ë° ë¬¸ì„œ ë¶„ë¥˜ì™€ ê°™ì€ ë§ì€ ì‘ìš© í”„ë¡œê·¸ë¨ì´ ìˆëŠ” ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì¤‘ìš”í•œ ì‘ì—…ì´ë‹¤.  
ë˜í•œ ì´ëŠ” 14ë„ë¶€í„° ì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ì´ ì ì  ë” ì¸ê¸°ë¥¼ ëŒê³  ìˆë‹¤.    
â¡ ì´ëŸ¬í•œ ëª¨ë¸ì€ ì‹¤ì œë¡œ ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì§€ë§Œ,     
**í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ì‹œê°„ ëª¨ë‘ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦° ê²½í–¥**ì´ ìˆì–´ ë§¤ìš° í° ë°ì´í„° ì„¸íŠ¸ì—ì„œì˜ ì‚¬ìš©ì—” í•œê³„ê°€ ì¡´ì¬í•œë‹¤.     


**[linear classifiersì˜ ì ì¬ë ¥]**        
* [linear classifiers](https://en.wikipedia.org/wiki/Linear_classifier)ëŠ” í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë¬¸ì œì˜ ê°•ë ¥í•œ baselinesìœ¼ë¡œ ê°„ì£¼ëœë‹¤.     
ì´ê²ƒì€ ë‹¨ìˆœí•˜ì§€ë§Œ ì ì ˆí•˜ê²Œ ì‚¬ìš©í•˜ë©´ stateof-the-art ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.              
ë˜í•œ **ë§¤ìš° í° ë§ë­‰ì¹˜ë¡œ í™•ì¥í•  ìˆ˜ ìˆëŠ” ì ì¬ë ¥**ë„ ê°€ì§€ê³  ìˆë‹¤.       




**[linear classifiersë¥¼ ëª¨ë¸ì— ì ìš©]**    
* ë³¸ ë…¼ë¬¸ì€ í…ìŠ¤íŠ¸ ë¶„ë¥˜ì˜ ë§¥ë½ì—ì„œ ì´ëŸ¬í•œ baselinesì„ **large output spaceì„ ê°€ì§„ ë§¤ìš° í° ë§ë­‰ì¹˜ë¡œ í™•ì¥**í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•œë‹¤.      
* [Word2vec](https://arxiv.org/abs/1301.3781)ì˜ ì—°êµ¬ì—ì„œ ì˜ê°ì„ ë°›ì•„,   
rank ì œì•½ê³¼ ë¹ ë¥¸ loss ê·¼ì‚¬ë¥¼ ê°€ì§„ **linear modelsì´ 10ë¶„ ì´ë‚´ì— 10ì–µ ë‹¨ì–´ì— ëŒ€í•´ í›ˆë ¨í•  ìˆ˜ ìˆëŠ” ë™ì‹œì— ìµœì²¨ë‹¨ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤**.       
* evaluation: ìš°ë¦¬ëŠ” íƒœê·¸ ì˜ˆì¸¡ê³¼ ê°ì • ë¶„ì„ì´ë¼ëŠ” ë‘ ê°€ì§€ ë‹¤ë¥¸ ì‘ì—…ì—ì„œ [fastText](https://github.com/facebookresearch/fastText) ì ‘ê·¼ ë°©ì‹ì˜ í’ˆì§ˆì„ í‰ê°€í•œë‹¤.           




----


# 2. Model architecture


**[linear classifiersì˜ ì‚¬ìš©]**     
* ë¬¸ì¥ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì¸ ê¸°ì¤€ì€ ë¬¸ì¥ì„ [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) (BoW) ìœ¼ë¡œ í‘œí˜„í•˜ê³ ,     
linear classifier(ì˜ˆ: logistic regression ë˜ëŠ” [SVM](https://yerimoh.github.io/ML3/)ë¥¼ í›ˆë ¨í•˜ëŠ” ê²ƒì´ë‹¤.      


**[linear classifiersì˜ ê°œì„ ]**     
* ê·¸ëŸ¬ë‚˜ linear classifiersëŠ” **features ë° class ê°„ì— ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê³µìœ í•˜ì§€ ì•ŠëŠ”ë‹¤**.      
* ì´ëŠ” **ì˜ˆì œê°€ ê±°ì˜ ì—†ëŠ” large output spaceì—ì„œ ì¼ë°˜í™”ë¥¼ ì œí•œ**í•  ìˆ˜ ìˆë‹¤.      
â¡ ì´ ë¬¸ì œì— ëŒ€í•œ ì¼ë°˜ì ì¸ í•´ê²°ì±…ì€ linear classifierë¥¼ **low rank matricesë¡œ ë¶„í•´**í•˜ê±°ë‚˜ **ë‹¤ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©**í•˜ëŠ” ê²ƒì´ë‹¤.   




**[ëª¨ë¸ êµ¬ì„±]**   
ë³¸ ë…¼ë¬¸ì˜ ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ë‹¤.
ì´ëŠ” rank ì œì•½ ì¡°ê±´ì´ ìˆëŠ” simple linear modelì„ ë³´ì—¬ì¤€ë‹¤.     

<img width="323" alt="image" src="https://user-images.githubusercontent.com/76824611/210780968-f9457e60-d22b-4410-889b-36e5ab4f57fe.png">
Figure 1: Model architecture of fastText for a sentence with N ngram features x1, . . . , xN . The features are embedded and averaged to form the hidden variable.

ê·¸ë¦¼ 1: Ngram íŠ¹ì§•ì´ ìˆëŠ” ë¬¸ì¥ì— ëŒ€í•œ fastTextì˜ ëª¨ë¸ ì•„í‚¤í…ì²˜ x1, . ., xN. íŠ¹ì§•ë“¤ì€ ìˆ¨ê²¨ì§„ ë³€ìˆ˜ë¥¼ í˜•ì„±í•˜ê¸° ìœ„í•´ ë‚´ì¥ë˜ê³  í‰ê· í™”ëœë‹¤.


1ï¸âƒ£ ì²« ë²ˆì§¸ ê°€ì¤‘ì¹˜ í–‰ë ¬ AëŠ” ë‹¨ì–´ ìœ„ì˜ look-up tableì´ë‹¤.       
2ï¸âƒ£ ê·¸ëŸ° ë‹¤ìŒ ë‹¨ì–´ í‘œí˜„ì€ text representationìœ¼ë¡œ í‰ê· í™”ëœë‹¤.             
3ï¸âƒ£ text representationì€ linear classifierì— ì…ë ¥ëœë‹¤.      
   text representationì€ ì ì¬ì ìœ¼ë¡œ ì¬ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” hidden variableì…ë‹ˆë‹¤.     


<details>
<summary>ğŸ“œ look-up tableì´ë€? </summary>
<div markdown="1">
 
 
Embedding ë ˆì´ì–´ëŠ” **ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¨ ë‹¨ì–´ë¥¼ ë¶„ì‚° í‘œí˜„ìœ¼ë¡œ ì—°ê²°í•´ ì£¼ëŠ” ì—­í• **ì„ í•˜ëŠ”ë°,     
ê·¸ê²ƒì´ **Weightì—ì„œ íŠ¹ì • í–‰ì„ ì½ì–´ì˜¤ëŠ” ê²ƒê³¼ ê°™ì•„** **ì´ ë ˆì´ì–´ë¥¼ ë£©ì—… í…Œì´ë¸”(Lookup Table)**ì´ë¼ê³  ë¶€ë¥´ê¸°ë„ í•©ë‹ˆë‹¤.   
 
ì´ê²Œ ë¬´ìŠ¨ ì†Œë¦¬ëƒë©´, 
ì„ë² ë”© ì¸µì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ **ê° ë‹¨ì–´ë“¤ì€ ëª¨ë‘ ì •ìˆ˜ ì¸ì½”ë”©**ì´ ë˜ì–´ìˆì–´ì•¼ í•œë‹¤.       

íŠ¹ì • ë‹¨ì–´ì™€ ë§µí•‘ë˜ëŠ” ì •ìˆ˜ë¥¼ ì¸ë±ìŠ¤ë¡œ ê°€ì§€ëŠ” í…Œì´ë¸”ë¡œë¶€í„° **ì„ë² ë”© ë²¡í„° ê°’ì„ ê°€ì ¸ì˜¤ëŠ” ë£©ì—… í…Œì´ë¸”ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.**    
![image](https://user-images.githubusercontent.com/76824611/210789841-ddb0d5c9-212d-4459-a8b5-4b92a17a8c29.png)
![image](https://user-images.githubusercontent.com/76824611/210788684-7cfd8297-99ea-4ac3-8d2f-57077702f69f.png)

ì´í•´ê°€ ì•ˆë˜ë‹¤ë©´ ì´ ë…¼ë¬¸ì˜ ê¸°ë°˜ ë…¼ë¬¸ì¸ [Word2vec](https://yerimoh.github.io/DL14/)ë¥¼ ì œëŒ€ë¡œ ì•Œê³ ì˜¤ì.   

</div>
</details>


ì´ ì•„í‚¤í…ì²˜ëŠ” ì¤‘ê°„ ë‹¨ì–´ê°€ ë ˆì´ë¸”ë¡œ ëŒ€ì²´ë˜ëŠ” [cbow ëª¨ë¸](https://yerimoh.github.io/DL14/#cbow-%EB%AA%A8%EB%8D%B8)ê³¼ ìœ ì‚¬í•˜ë‹¤.      
[ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜](https://yerimoh.github.io/DL2/#%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4-%ED%95%A8%EC%88%98-softmax-function) $$f$$ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ ì •ì˜ëœ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ê³„ì‚°í•œë‹¤.       
Nê°œì˜ ë¬¸ì„œ ì§‘í•©ì˜ ê²½ìš°, ì´ê²ƒì€ í´ë˜ìŠ¤ì— ëŒ€í•œ negative loglikelihoodë¥¼ ìµœì†Œí™”í•œë‹¤.     



<img width="354" alt="image" src="https://user-images.githubusercontent.com/76824611/210780823-b4f9953e-cfed-4628-8848-9b67c3aca1fa.png">
* ì—¬ê¸°ì„œ $$x_n$$ì€ në²ˆì§¸ documentì˜ ì •ê·œí™”ëœ bag of featuresì´ë‹¤.     
* $$y_n the label$$ Aì™€ Bì˜  weight matricesì´ë‹¤.        
* ì´ model ì€ [stochastic gradient descent](https://yerimoh.github.io/DL5/#%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95-sgd)ì™€ linearly decaying learning rateë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ CPUì—ì„œ ë¹„ë™ê¸°ì‹ìœ¼ë¡œ í›ˆë ¨ëœë‹¤.












<details>
<summary>ğŸ“œ Negative Log-Likelihood (NLL) ë³´ê¸°</summary>
<div markdown="1">

ì‹¤ì œ softmaxì„ ì“¸ ë•ŒëŠ” negative log-likelihood(NLL)ì™€ ì‚¬ìš©ëœë‹¤.     

<img width="224" alt="image" src="https://user-images.githubusercontent.com/76824611/210790935-eb6ad9f7-dc6a-4ab0-8aca-4fc8ec44b626.png">

ì¦‰, ë¹„ìœ ë¥¼ í•˜ìë©´ ì†Œí”„íŠ¸ë§¥ìŠ¤í•¨ìˆ˜ëŠ” í–‰ë³µì§€ìˆ˜ë¥¼ ì°¾ëŠ” ê²ƒì´ë¼ë©´,      
Negative Log-Likelihood (NLL)ëŠ” ë¶ˆí–‰ì§€ìˆ˜ë¥¼ ì°¾ëŠ”ê²ƒì´ë‹¤.     

Negative Log-Likelihood (NLL)ì€ ì•„ë˜ì™€ ê°™ì€ë°,   
inputì´ 0ì¼ ë•Œ ë¬´í•œìœ¼ë¡œ ê°€ê³ , inputì´ 1ì¼ ë•Œ 0ìœ¼ë¡œ ê°„ë‹¤.     
![image](https://user-images.githubusercontent.com/76824611/210791528-7e2f0770-0c33-4377-ab4a-3c505efe13e4.png)

ì¦‰ ì •ë¦¬í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ ê´€ê³„ë¥¼ ê°–ëŠ”ë‹¤.   
![image](https://user-images.githubusercontent.com/76824611/210791835-8d044c5e-e3f7-43c5-8ab8-27ce3200cb5d.png)

lossë¥¼ ê³„ì‚°í•  ë•Œ ìš°ë¦¬ëŠ” ì •ë‹µ classì— ëŒ€í•œ ë†’ì€ í™•ë¥ ì€ ë‚®ì€ lossë¡œ ì´ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.           


</div>
</details>  


---


## 2.1 Hierarchical softmax


Hierarchical Softmaxì€ [negative sampling](https://yerimoh.github.io/DL15/#2--%EB%84%A4%EA%B1%B0%ED%8B%B0%EB%B8%8C-%EC%83%98%ED%94%8C%EB%A7%81%EC%9D%B4%EB%9E%80-%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98-%EB%8F%84%EC%9E%85)ê³¼ ê°™ì´ ì—°ì‚°ì´ ë„ˆë¬´ ë¹„ëŒ€í•´ì§€ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ ê³ ì•ˆëœ ë°©ì‹ì´ë‹¤.     

ì¦‰ í•œë§ˆë””ë¡œ ìš”ì•½í•˜ìë©´ Hierarchical Softmax ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ë°±í„°ì˜ ë‚´ì ì„ ì´ì§„ ë¶„ë¥˜ë¡œ ë°”ê¿” ê³„ì‚°ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.    
â¡ 100ë§Œê°œì˜ ë‹¨ì–´ë¥¼ ë²¡í„°ì˜ ë‚´ì ìœ¼ë¡œ êµ¬í•˜ë©´ ë²¡í„°ì˜ ë‚´ì ì„ 100ë§Œë²ˆ í•´ì•¼í•˜ì§€ë§Œ,       
 Hierarchical Softmax ë°©ë²•ì„ í†µí•œ ì´ì§„ë¶„ë¥˜ë¡œ êµ¬í•˜ë©´ $$log_2(100ë§Œ)$$ ì•½ 19ë²ˆë§Œ ê³„ì‚°í•˜ë©´ ë˜ëŠ” ê²ƒì´ë‹¤.     
 
ê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ ì´ì™€ ê°™ì´ ì´ì§„ë¶„ë¥˜ë¡œ ê³„ì‚°í•˜ëŠ”ì§€ ì•Œì•„ë³´ê² ë‹¤.    


**[ì˜ˆì‹œ]**    
ì´ëŠ” ëª¨ë¸ êµ¬ì¡° ìì²´ë¥¼ [full binary tree](https://yerimoh.github.io/Algo022/#%EC%9D%B4%EC%A7%84-%ED%8A%B8%EB%A6%AC-%EC%9C%A0%ED%98%95-types-of-binary-trees) êµ¬ì¡°ë¡œ ë°”ê¾¼ í›„ì— ë‹¨ì–´ë“¤ì€ [leaf node](https://yerimoh.github.io/Algo022/#%ED%8A%B8%EB%A6%AC%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B0%9C%EB%85%90)ì— ë°°ì¹˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ë¶€í„° ì‹œì‘ëœë‹¤.


ì˜ˆë¥¼ ë“¤ì–´ì„œ ì„¤ëª…í•´ë³´ê² ë‹¤.
ì•„ë˜ì™€ ê°™ì€ ë¬¸ì¥(Context)ê°€ ìˆë‹¤ê³  í•´ë³´ì.   

ê·¸ë ‡ë‹¤ë©´ ì´ ë¬¸ì¥ì—ì„œ **'to'** ë¼ëŠ” ì¤‘ì‹¬ë‹¨ì–´ë¥¼, window size = 1ì— í•´ë‹¹í•˜ëŠ” ì£¼ë³€ë‹¨ì–´ **(want, eat)** ë“¤ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤ê³  ê°€ì •í•˜ì.    

ê·¸ë ‡ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ treeì—ì„œ í•™ìŠµì´ ë  ê²ƒì´ë‹¤.     
í¸ì˜ë¥¼ ìœ„í•´ wantì™€ eatì¤‘ **want**ë¡œ ë§Œ ë¥¼ ì˜ˆì‹œë¡œ ë“¤ì—ˆë‹¤.


 
ì—¬ê¸°ì„œ ìµœì¢… í™•ë¥ (ì¶œë ¥ì¸µì˜ ê°’)ì€ rootë¶€í„° leafê¹Œì§€ ê°€ëŠ” ê¸¸ì— ìˆëŠ” í™•ë¥ ì„ ëª¨ë‘ ê³±í•˜ì—¬ ê³„ì‚°ëœë‹¤.     
ë” ë†’ì€ í™•ë¥ ì˜ edgeë¥¼ ì„ íƒí•´ë‚˜ê°„ë‹¤.    
ê·¸ë¦¬ê³  ì•„ë˜ì—ì„œ ë³´ë‹¤ì‹œí”¼ ê° edgeì˜ í•©ì€ 1ì´ë‹¤.(í™•ë¥ )(ë˜‘ê°™ì€ ìƒ‰ì˜ í•©ì€ 1ì´ë‹¤)    
ì¦‰ Iê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ê³„ì‚°í•´ë³´ë©´, **$$0.7 x 0.8 x 0.6$$** ì´ë‹¤.    

![image](https://user-images.githubusercontent.com/76824611/210959022-df840eb3-0892-4637-84f8-bf47411e93c6.png)


ì¦‰ ìœ„ì˜ ë°©ë²•ë“¤ì„ í†µí•´ Hierarchical SoftmaxëŠ” ì¶œë ¥ì¸µì˜ ê°’ì„ softmax í•¨ìˆ˜ë¡œ ì–»ëŠ” ê²ƒ ëŒ€ì‹ ì— binatr treeë¥¼ ì´ìš©í•˜ì—¬ ì–»ëŠ” ê²ƒì´ë‹¤.        


**[ì¼ë°˜í™”]**   
ê·¸ë ‡ë‹¤ë©´ ìœ„ì˜ ì‹ë“¤ì„ ì¼ë°˜í™”í•´ë³´ì.    
ì¼ë°˜í™”í•œ ë…¸ë“œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.     
![image](https://user-images.githubusercontent.com/76824611/210958934-b07bbb8f-6b76-4a12-9c6e-12dd8a26c5a6.png)



When the number of classes is large, computing the
linear classifier is computationally expensive. More
precisely, the computational complexity is O(kh)
where k is the number of classes and h the dimension of the text representation. In order to improve our running time, we use a hierarchical softmax (Goodman, 2001) based on the Huffman coding tree (Mikolov et al., 2013). During training, the
computational complexity drops to O(h log2
(k)).
The hierarchical softmax is also advantageous at
test time when searching for the most likely class.
Each node is associated with a probability that is the
probability of the path from the root to that node. If
the node is at depth l + 1 with parents n1, . . . , nl
, its
probability is





This means that the probability of a node is always
lower than the one of its parent. Exploring the tree
with a depth first search and tracking the maximum
probability among the leaves allows us to discard
any branch associated with a small probability. In
practice, we observe a reduction of the complexity
to O(h log2
(k)) at test time. This approach is further extended to compute the T-top targets at the
cost of O(log(T)), using a binary heap.

----

# 3 Experiments
We evaluate fastText on two different tasks.
First, we compare it to existing text classifers on the
problem of sentiment analysis. Then, we evaluate
its capacity to scale to large output space on a tag
prediction dataset. Note that our model could be implemented with the Vowpal Wabbit library,2 but we
observe in practice, that our tailored implementation
is at least 2-5Ã— faster.





##  N-gram features
Bag of words is invariant to word order but taking
explicitly this order into account is often computationally very expensive. Instead, we use a bag of
n-grams as additional features to capture some partial information about the local word order. This
is very efficient in practice while achieving comparable results to methods that explicitly use the order (Wang and Manning, 2012).
We maintain a fast and memory efficient
mapping of the n-grams by using the hashing
trick (Weinberger et al., 2009) with the same hashing function as in Mikolov et al. (2011) and 10M
bins if we only used bigrams, and 100M otherwise.
