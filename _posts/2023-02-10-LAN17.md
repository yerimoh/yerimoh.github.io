---
title: "IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION ì •ë¦¬"
date:   2023-02-10
excerpt: "IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING?: BENCHMARKS, BASELINES, AND BUILDING BLOCKS FOR NATURAL LANGUAGE POLICY OPTIMIZATION paper review"
#category: [Paper]
#layout: post
#tag:
#- Paper
order: 0

comments: true
---

# ëª©ì°¨


# ABSTRACT
**[LM with human preferencesì— RL ì ìš©]**     
* ë³¸ ë…¼ë¬¸ì€ ì¸ê°„ ì„ í˜¸ë„ì— ë§ì¶”ëŠ” pre-trained large language models (LMs) ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.        
* í…ìŠ¤íŠ¸ ìƒì„±ì„ ìˆœì°¨ì  decision-making problemë¡œ ë³¸ë‹¤ë©´, **ê°•í™” í•™ìŠµ(RL)ì€ ì ì ˆí•œ conceptual í”„ë ˆì„ì›Œí¬**ê°€ ë  ìˆ˜ ìˆë‹¤.         

**[ë‹¨ì ]**    
* ê·¸ëŸ¬ë‚˜ LM ê¸°ë°˜ ìƒì„±ì„ ìœ„í•´ RLì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ combinatorial action spaceìœ¼ë¡œ ì¸í•œ **training ë¶ˆì•ˆì „ì„±** ì¡´ì¬     
* LM alignmentì„ ìœ„í•´ ë§ì¶¤í™”ëœ **ì˜¤í”ˆ ì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë²¤ì¹˜ë§ˆí¬ì˜ ë¶€ì¡±**ì„ í¬í•¨í•œ ê²½í—˜ì  challengesì— ì§ë©´í•œë‹¤.     
â¡ ë”°ë¼ì„œ, <span style="background-color:#fff5b1"> RLì´ NLPë¥¼ ìœ„í•œ ì‹¤ìš©ì ì¸ íŒ¨ëŸ¬ë‹¤ì„ì¸ê°€? </span>ë¼ëŠ” ì§ˆë¬¸ì´  ì œê¸°ëœë‹¤.


**[ë‹¨ì  í•´ê²°]**    
ì´ì— ëŒ€í•œ ë‹µë³€ì„ ë•ê¸° ìœ„í•´ ì•„ë˜ 3ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•œë‹¤,     
* **RL4LMsë¥¼ ì†Œê°œ**   
  * **RLë¡œ ì–¸ì–´ ìƒì„±ê¸°ë¥¼ optimizing**í•˜ê¸° ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë“ˆì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬     
  * ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” **ì„ì˜ì˜ reward í•¨ìˆ˜ë¥¼ ì‚¬ìš©**í•˜ì—¬ HuggingFace ë¼ì´ë¸ŒëŸ¬ë¦¬(Wolf et al., 2020)ì—ì„œ encoder ë˜ëŠ” encoder-decoder **LMì„ í›ˆë ¨í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” on-policy RL algorithmsìœ¼ë¡œ êµ¬ì„±**ëœë‹¤.     
* **GRUE (General Reinforced-language Understanding Evaluation) benchmark ì œì‹œ**     
  * target ë¬¸ìì—´ì´ ì•„ë‹ˆë¼ **ì¸ê°„ ì„ í˜¸ë„ì˜ ìë™í™”ëœ ì¸¡ì •**ì„ captureí•˜ëŠ” reward ê¸°ëŠ¥ì— ì˜í•´ supervisedë˜ëŠ” 6ê°œ ì–¸ì–´ ìƒì„± ì‘ì—… ì„¸íŠ¸      
  * GRUEëŠ” NLP ì‘ì—…ì— ëŒ€í•œ RL algorithmsì˜ ì²« ë²ˆì§¸ leaderboard-style evaluationì´ë‹¤.      
* **NLPO(Natural Language Policy Optimization) ì†Œê°œ**    
   * ì–¸ì–´ ìƒì„±ì—ì„œ **combinatorial action spaceì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ëŠ” ë°©ë²•**ì„ ë°°ìš°ëŠ” ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ì„±ëŠ¥ì˜ RL algorithm       

**[ê²°ê³¼]**    
**1)** **RL ê¸°ìˆ **ì´ ì¼ë°˜ì ìœ¼ë¡œ ì¸ê°„ ì„ í˜¸ë„ì— LMì„ ë§ì¶”ëŠ” ë° **supervised ë°©ë²•ë³´ë‹¤ ë‚«ë‹¤**ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.     
**2)** **NLPOê°€** ìë™ ë° ì¸ê°„ í‰ê°€ë¥¼ ëª¨ë‘ ê¸°ë°˜ìœ¼ë¡œ previous policy gradient methods **(PPO)ë³´ë‹¤ ë” í° ì•ˆì •ì„±ê³¼ ì„±ëŠ¥**ì„ ë‚˜íƒ€ë‚¸ë‹¤       




-----

# 1 INTRODUCTION
ì–¸ì–´ ê¸°ìˆ ì˜ ê¶ê·¹ì ì¸ ëª©í‘œëŠ” **ì¸ê°„ê³¼ ìƒí˜¸ì‘ìš©**í•˜ëŠ” ê²ƒì´ë‹¤.     
**[ë‹¨ì ]**    
* ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ ì–¸ì–´ ëª¨ë¸ì€ **ì¸ê°„ ì„ í˜¸ë„ì˜ ì§ì ‘ì ì¸ ì‹ í˜¸ ì—†ì´ í›ˆë ¨**ë˜ë©°, supervised target ë¬¸ìì—´ì€ (ë•Œë¡œëŠ” ì¡°ì¡í•œ) proxy ì—­í• ì„ í•œë‹¤.     
* ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•©í•˜ëŠ” í•œ ê°€ì§€ ì˜µì…˜ì€ human-in-the-loopë¥¼ í†µí•œ ê²ƒì´ë‹¤.   
* ì¦‰, ëª¨ë¸ì´ í›ˆë ¨í•  ë•Œ ì‚¬ìš©ìëŠ” ê° ìƒ˜í”Œì— ëŒ€í•œ í”¼ë“œë°±ì„ ì˜¨ë¼ì¸ìœ¼ë¡œ ì œê³µí•´ì•¼ í•˜ì§€ë§Œ, ì´ ì •ë„ì˜ **ë°€ë„ ë†’ì€ supervision**ì€ ì¢…ì¢… **ê¸ˆì§€**ë˜ê³  **ë¹„íš¨ìœ¨**ì ì´ë‹¤.     


**[í•´ê²°: Automated metrics]**    
* ìœ„ì˜ ë¬¸ì œì— ëŒ€í•´ <span style="background-color:#DCFFE4">Automated metrics</span>ì€ ê´œì°®ì€ ì ˆì¶©ì•ˆì„ ì œê³µí•œë‹¤.    
   * pairwise learned preference models(Ouyang et al., 2022), BERTScore(Zhang et al., 2019), BLEURT(Sellam et al., 2020)ëŠ” ì´ì „ ë©”íŠ¸ë¦­(BLEU, METERE et al.)ì— ë¹„í•´ **ì¸ê°„ íŒë‹¨ê³¼ì˜ ìƒê´€ê´€ê³„ê°€ í¬ê²Œ ê°œì„ ** ë˜ì—ˆìœ¼ë©° **í‰ê°€ ë¹„ìš©ì´ ì €ë ´**í•˜ë‹¤.     

**[Automated metricsì˜ ë¬¸ì œ]**     
* ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ê¸°ëŠ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ **per-tokenë³„ë¡œ ì°¨ë³„í™” ë¶ˆê³¼**.       
* ì‚¬ëŒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ë©”íŠ¸ë¦­ì€ **ì „ì²´ generations**ì— ëŒ€í•œ **í’ˆì§ˆ ì¶”ì •ì¹˜ë§Œ ì œê³µ** ê°€ëŠ¥      


**[í•´ê²°: RL]**      
* <span style="background-color:#DCFFE4">ê°•í™” í•™ìŠµ(RL)</span>ì€ **ìˆœì°¨ì  ì˜ì‚¬ ê²°ì • ë¬¸ì œë¡œ ìºìŠ¤íŒ…**ë  ë•Œ LM ê¸°ë°˜ ìƒì„±ì— ëŒ€í•œ **ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ ìŠ¤ì¹¼ë¼ ëª©í‘œë¥¼ ìµœì í™”**í•˜ê¸° ìœ„í•œ ìì—°ìŠ¤ëŸ¬ìš´ ê²½ë¡œë¥¼ ì œê³µí•œë‹¤.     
* ìµœê·¼ ì—°êµ¬ëŠ” **preference-based rewardsì„ ì œí•œ**í•˜ì—¬ **fluency ê°œë…ì„ í†µí•©**í•¨ìœ¼ë¡œì¨ RLì„ í†µí•´ LMì„ ì¸ê°„ ì„ í˜¸ë„ì— ë§ì¶”ëŠ” ìœ ë§í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ,     
âš ï¸ ì´ëŸ¬í•œ ë°©ì‹ì˜ ì§„ì „ì€ **ì˜¤í”ˆ ì†ŒìŠ¤ ë²¤ì¹˜ë§ˆí¬ì™€ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ì˜ ë¶€ì¡±**ìœ¼ë¡œ ì¸í•´ ë°©í•´ë°›ê³  ìˆë‹¤.     



**[ë³¸ ë…¼ë¬¸]**     
LMì„ ë” ì˜ ì •ë ¬í•˜ê¸° ìœ„í•´ RL ì•Œê³ ë¦¬ë“¬ì„ êµ¬ì¶•í•˜ëŠ” ì—°êµ¬ë¥¼ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´ ë¼ì´ë¸ŒëŸ¬ë¦¬, ë²¤ì¹˜ë§ˆí¬ ë° ì•Œê³ ë¦¬ë“¬ì„ ì¶œì‹œí•œë‹¤. ë¨¼ì € PPO/A2C ë“±ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ê¸°ì¡´ RL ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ì  HuggingFace ëª¨ë¸(ì˜ˆ: GPT-2 ë˜ëŠ” T5)ì„ í›ˆë ¨í•  ìˆ˜ ìˆëŠ” RL4LMs ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¦´ë¦¬ìŠ¤í•œë‹¤. ë‹¤ìŒìœ¼ë¡œ, ìš°ë¦¬ëŠ” RL4ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ëœ ëª¨ë¸ì„ ì ìš©í•œë‹¤ìƒˆë¡œìš´ GRUE(ì¼ë°˜ ê°•í™” ì–¸ì–´ ì´í•´ í‰ê°€) ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ LM: GRUEëŠ” 7ê°œì˜ í˜„ëŒ€ NLP ì‘ì—…ì˜ ëª¨ìŒì´ë‹¤(ìì„¸í•œ ë‚´ìš©ì€ í‘œ 1 ì°¸ì¡°). ë‹¤ë¥¸ ë²¤ì¹˜ë§ˆí¬ì™€ ë‹¬ë¦¬, ìš°ë¦¬ëŠ” ê°ë…ëœ í›ˆë ¨ ëŒ€ì‹  ê° ì‘ì—…ì„ ë³´ìƒ ê¸°ëŠ¥ê³¼ ìŒìœ¼ë¡œ êµ¬ì„±í•œë‹¤. GRUEëŠ” ìœ ì°½í•œ ì–¸ì–´ ìƒì„±ê¸°ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì´ëŸ¬í•œ ë³´ìƒ ê¸°ëŠ¥ì„ ìµœì í™”í•˜ê¸° ìœ„í•´ ëª¨ë¸ì— ë„ì „í•œë‹¤. ìš°ë¦¬ëŠ” ë³´ìƒì„ ìµœì í™”í•˜ê¸° ìœ„í•´ ì‘ì—…ë³„ ê°ë… ì‚¬ì „ í›ˆë ¨ ìœ ë¬´ì— ê´€ê³„ì—†ì´ RLì„ í†µí•´ ì–¸ì–´ ëª¨ë¸ì„ í›ˆë ¨í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ê¸°ì¡´ RL ë°©ë²•ì„ ë„˜ì–´ í† í° ìˆ˜ì¤€ì—ì„œ ì–¸ì–´ ë¶„í¬ì— ëŒ€í•œ ì‘ì—…ë³„ ì œì•½ ì¡°ê±´ì„ ë™ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” NLPO(Natural Language Policy Optimization)ë¼ëŠ” ìƒˆë¡œìš´ ì •ì±… RL ì•Œê³ ë¦¬ë“¬ì„ ì†Œê°œí•œë‹¤.
To facilitate research in building RL algorithms to better align LMs, we release a library, a benchmark, and an algorithm. First, we release the RL4LMs library, which enables generative HuggingFace models (e.g., GPT-2 or T5) to be trained using a variety of existing RL methods like PPO/A2C/etc. Next, we apply models trained using RL4LMs to the new GRUE (General Reinforced-language Understanding Evaluation) benchmark: GRUE is a collection of 7 contemporary NLP tasks (see Table1 for details); in contrast to other benchmarks, instead of supervised training, we pair each task with reward function(s). GRUE challenges models to optimize these reward functions while remaining fluent language generators. We train language models via RLâ€”both with and without task specific supervised pre-trainingâ€”to optimize rewards. Finally, beyond existing RL methods, we introduce a novel on-policy RL algorithm called NLPO (Natural Language Policy Optimization), that dynamically learns task-specific constraints over the distribution of language at a token level.


GRUEì™€ ì¸ê°„ í‰ê°€ì— ëŒ€í•œ ì‹¤í—˜ì€ NLPOê°€ PPOë¥¼ í¬í•¨í•œ ëŒ€ì•ˆì— ë¹„í•´ ì–¸ì–´ ìœ ì°½ì„±ì„ ìœ ì§€í•˜ë©´ì„œ í•™ìŠµ ì„ í˜¸ ë³´ìƒì˜ ê· í˜•ì„ ë” ì˜ ìœ ì§€í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤(ê·¸ë¦¼ 1). ìš°ë¦¬ëŠ” ìŠ¤ì¹¼ë¼ ë³´ìƒ í”¼ë“œë°±ì—ì„œ í•™ìŠµí•˜ê¸° ìœ„í•´ RLì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì„ ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆë‹¤. (1) ì§€ë„ í•™ìŠµì„ í†µí•œ ì¶”ê°€ ì „ë¬¸ê°€ ì‹œì—°ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ë°ì´í„° íš¨ìœ¨ì„±ì´ ë” ë†’ì„ ìˆ˜ ìˆë‹¤. í•™ìŠµëœ ë³´ìƒ í•¨ìˆ˜ëŠ” 5ë°° ë” ë§ì€ ë°ì´í„°ë¡œ í›ˆë ¨ëœ ì§€ë„ ë°©ë²•ë³´ë‹¤ RL ë°©ë²•ì— ëŒ€í•œ ì‹ í˜¸ë¡œ ì‚¬ìš©ë  ë•Œ ë” í° ì„±ëŠ¥ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤nd (2) ë§¤ê°œ ë³€ìˆ˜ íš¨ìœ¨ì„±â€”ê°ë…ê³¼ NLPOì˜ ì¡°í•©ìœ¼ë¡œ í›ˆë ¨ëœ 2ì–µ 2ì²œë§Œ ê°œì˜ ë§¤ê°œ ë³€ìˆ˜ ëª¨ë¸ì´ 30ì–µ ê°œì˜ ê°ë… ëª¨ë¸ì„ ëŠ¥ê°€í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. ìš°ë¦¬ëŠ” ìš°ë¦¬ê°€ ê³µê°œí•œ ë²¤ì¹˜ë§ˆí¬, ê¸°ì¤€ì„  ë° ë¹Œë”© ë¸”ë¡ì´ LMì„ ì¸ê°„ ì„ í˜¸ë„ì— ë§ì¶”ëŠ” ì—°êµ¬ë¥¼ ì¶”ì§„í•˜ëŠ” ì—­í• ì„ í•˜ê¸°ë¥¼ ë°”ë€ë‹¤.
Experiments on GRUE and human evaluations show that NLPO better balances learning preference rewards while maintaining language fluency compared to alternatives, including PPO (Figure 1). We find that using RL to learn from scalar reward feedback can be more: (1) data efficient than using additional expert demonstrations via supervised learning (though a combination of both is best)â€”a learned reward function enables greater performance when used as a signal for an RL method than a supervised method trained with 5 times more data, and (2) parameter efficientâ€”enabling a 220 million parameter model trained with a combination of supervision and NLPO to outperform a 3 billion supervised model. We hope that the benchmarks, baselines, and building blocks we release serve to drive forward research in aligning LMs to human preferences.


----
----

# 3 RL4LMS: RLë¡œ LMSë¥¼ êµìœ¡í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

**[RL4LM]**     
* LM-based generationì—ì„œ **RL algorithms**ì„ **fine-tuning**í•˜ê³  **í‰ê°€**í•˜ê¸° ìœ„í•œ ë¹Œë”© ë¸”ë¡ì´ ìˆëŠ” **ì˜¤í”ˆ ì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸**          
* ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” **HuggingFaceì™€ baselines-3(Raffin et al., 2021)ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•**ë˜ì–´ ì¸í„°í˜ì´ìŠ¤ì˜ ì¤‘ìš” êµ¬ì„± ìš”ì†Œë¥¼ ê²°í•©í•œë‹¤.    
* RL4LMì€ stable-baselines-3ì˜ **on-policy RL algorithm**ìœ¼ë¡œ HuggingFaceì˜ decoder ì „ìš© ë˜ëŠ” encoder-decoder **transformer ëª¨ë¸ì„ í›ˆë ¨**ì‹œí‚¤ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.     
* ë˜í•œ PPO(Schulman et al., 2017), TRPO(Schulman et al., 2015a), A2C(Mnih et al., 2016) ë° ìì²´ NLPO(Â§4)ì™€ ê°™ì€ LM ë¯¸ì„¸ ì¡°ì •ì— ë§ê²Œ ì¡°ì •ëœ ì¸ê¸° ìˆëŠ” on-polic RL algorithmì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” êµ¬í˜„ì„ ì œê³µí•œë‹¤.      
* ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” **ëª¨ë“ˆì‹**ìœ¼ë¡œ ë˜ì–´ ìˆì–´ ì‚¬ìš©ìê°€ ë§ì¶¤í˜• í™˜ê²½ì„ í”ŒëŸ¬ê·¸ì¸í•˜ê³ , ê¸°ëŠ¥, ë©”íŠ¸ë¦­ ë° ì•Œê³ ë¦¬ì¦˜ì„ ë³´ìƒí•  ìˆ˜ ìˆë‹¤.     
* ì´ˆê¸° ë¦´ë¦¬ìŠ¤ì—ì„œëŠ” 6ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ NLP ì‘ì—…, 16ê°œì˜ í‰ê°€ ë©”íŠ¸ë¦­ ë° ë³´ìƒ, 4ê°œì˜ RL ì•Œê³ ë¦¬ë“¬ì— ëŒ€í•œ ì§€ì›ì„ ì œê³µí•œë‹¤.     


<details>
<summary>ğŸ“œ on-policyë€? </summary>
<div markdown="1">
  
 ![image](https://user-images.githubusercontent.com/76824611/219153619-95917905-607e-401a-9e83-f2eb8ae4310d.png)

**on-policy ?**           
- ì •ì±… ì—…ë°ì´íŠ¸ì— ì‹¤ì œë¡œ í–‰ë™ì„ í•˜ê³ ìˆëŠ” ê°€ì¥ ìµœì‹  ë²„ì „ì˜ policyë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹    
- ì¦‰, actionì„ ì„ íƒí•˜ëŠ” policyë¥¼ ì§ì ‘ evaluateí•˜ê³  imporveí•˜ëŠ” ë°©ì‹           
- ì´ë¡ ìƒ 1ë²ˆì´ë¼ë„ í•™ìŠµì„ í•´ì„œ policy improvementë¥¼ ì‹œí‚¨ ìˆœê°„, ê·¸ policyê°€ í–ˆë˜ ê³¼ê±°ì˜ experienceë“¤ì€ ëª¨ë‘ ì‚¬ìš©ì´ ë¶ˆê°€(ë°ì´í„° íš¨ìœ¨ x)            
- Data Efficiencyê°€ ë–¨ì–´ì§€ì§€ë§Œ **êµ¬í˜„ì´ ì‰½ê³ ** **ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì •ì±…ì— ì ìš© ê°€ëŠ¥**í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.      
- **behavior policyì™€ target policyê°€ ê°™ìŒ**: í˜„ì¬ í–‰ë™í•˜ëŠ” policyë¥¼ ê·¸ëŒ€ë¡œ updateí•  ëª©ì ìœ¼ë¡œ í™˜ê²½ì„ íƒìƒ‰         
-> í˜„ì¬ policyì— ì˜ì¡´ì , data dependent,  local optimalì— ìˆ˜ë ´í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±      

**off-policy ?**    
- on-policyì™€ ë‹¬ë¦¬ ì •ì±… ì—…ë°ì´íŠ¸ì— ì–´ë–¤ ë°ì´í„°ë¥¼ ì¨ë„ ìƒê´€ì´ ì—†ë‹¤. 
- a**ctionì„ ì„ íƒí•˜ëŠ” policyì™€ëŠ” ë³„ê°œë¡œ ë³„ë„ì˜ policyë¥¼ í•™ìŠµ**ì‹œí‚¤ëŠ” ë°©ì‹   
- ì¦‰, ê°€ì¥ ì—…ë°ì´íŠ¸ëœ ì •ì±…ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì•„ë‹ˆë¼ë„ ì •ì±… ì—…ë°ì´íŠ¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.    
- policyê°€ **ê³¼ê±°ì— í–ˆë˜ experienceë„ í•™ìŠµì— ì‚¬ìš©**ì´ ê°€ëŠ¥         
- behavior policyì™€ target policyê°€ ë‹¤ë¦„: í˜„ì¬ í–‰ë™í•˜ëŠ” policyë¥¼ ê·¸ëŒ€ë¡œ updateí•  ëª©ì ìœ¼ë¡œ í™˜ê²½ì„ íƒìƒ‰      
- Q-learning: ì´ ìˆ˜ì‹ì—ì„œëŠ” St+1ì—ì„œ ì„ íƒí•˜ëŠ” actionì„ maxë¡œ ë§¤ë²ˆ ìƒˆë¡­ê²Œ ê°€ì ¸ì˜¨ë‹¤/ Qí•¨ìˆ˜ê°€ ì—…ë°ì´íŠ¸ ë˜ë”ë¼ë„, St+1ì—ì„œì˜ ìµœì ì˜ ì„ íƒì§€ë¥¼ max_aë¡œ ë‹¤ì‹œ ê³ ë¥¼ ìˆ˜ ìˆë‹¤    
- ì¥ì     
  * ì‚¬ëŒ or other policy ì‚¬ìš© ê°€ëŠ¥      
  * ì‹¤ì»· íƒí—˜í•˜ë©´ì„œ optimal policy(greedy)ë¡œ sampleì–»ëŠ”ë‹¤   
  * ì¬í‰ê°€ ê°€ëŠ¥ (ì „ì— ëª»í–ˆë˜ ê²ƒë„ ì§€ê³  ë‚˜ì„œ ì•„ ëª»í–ˆêµ¬ë‚˜ë¼ê³  í‰ê°€ ê°€ëŠ¥)    
 
</div>
</details>  

---


## 3.1 ENVIRONMENTS: í† í° ë ˆë²¨ MDPë¡œì„œì˜ ìƒì„±
ê° ENVIRONMENTSì€ NLP taskì„          
* **dataset:** ìš°ë¦¬ëŠ” Nê°œì˜ ì˜ˆì œ ì¤‘ supervised datase $$D = {(x_i, y_i)}^N_{i=1}$$ì´ ì£¼ì–´ì§.    
  * $$x âˆˆ X$$: ì–¸ì–´ ì…ë ¥(language input)     
  * $$y âˆˆ Y$$: task ë¬¸ìì—´ì…ë‹ˆë‹¤.       
* **Generation:  Markov Decision Process (MDP)**    
  * Generationì€ **ìœ í•œí•œ ì–´íœ˜ Vë¥¼ ì‚¬ìš©**í•˜ì—¬ MDPë¡œ ë³¼ ìˆ˜ ìˆë‹¤.   
  * $$MDP: (S,A,P,r, ğœŒ_0, ğ›¾, T)$$.   
     * **$$S$$**: stateë“¤ì˜ ìœ í•œí•œ set     
     * **$$A$$**: actionë“¤ì˜ ìœ í•œí•œ set      
     * **$$P$$**: $$S * A * S$$ â¡ $$â„$$ì€ ì „í†µì ì¸ í™•ë¥       
      ì–´ë–¤ stateì—ì„œ ì–´ë–¤ actë¥¼ í•˜ë©´ì€ ë‹¤ìŒ stateë¡œ ê°ˆ í™•ë¥ ì´ ì–¼ë§Œì§€     
     * **$$r$$**: $$S$$ â¡ $$â„$$, reward í•¨ìˆ˜    
     * **$$ğœŒ_0$$**: $$S$$ â¡ $$â„$$, ì²˜ìŒ state($$s_0$$)ì˜ ë¶„í¬       
     ì‹œì‘í•˜ëŠ” stateê°€ ì—¬ëŸ¬ê°œê°€ ìˆì„ ìˆ˜ ìˆëŠ”ë° ì´ ì‹œì‘ ìœ„ì¹˜ì˜ ë¶„í¬
     * **$$ğ›¾ âˆˆ (0,1)$$**: discount factor        
     0~1ì‚¬ì´ì˜ ê°’ì„ ê°–ëŠ” Discount Factor (ì¡°ê¸ˆ ë” íš¨ìœ¨ì ì¸ pathë¥¼ ì°¾ê²Œ í•´ì¤Œ)    
     í˜„ì¬ ì–»ëŠ” ë³´ìƒì´ ë¯¸ë˜ì— ì–»ëŠ” ë³´ìƒë³´ë‹¤ ì–¼ë§ˆë‚˜ ë” ì¤‘ìš”í•œì§€ ì˜ë¯¸í•˜ëŠ” ê°’ (ë¯¸ë˜ì™€ ë¹„êµí•œ í˜„ì¬ ë³´ìƒì˜ ê°€ì¹˜)         
     * **$$T$$**: horizon    
  * MDPì˜ ê° ì—í”¼ì†Œë“œëŠ” **datasetì—ì„œ datapoint(x, y)ë¥¼ ìƒ˜í”Œë§í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘**í•˜ì—¬ **í˜„ì¬ time step tê°€ horizon Të¥¼ ì´ˆê³¼í•˜ê±°ë‚˜ ë¬¸ì¥ ì¢…ë£Œ(EOS) í† í°ì´ ìƒì„±ë  ë•Œ ëë‚œë‹¤**.         




---

## 3.2 REWARD FUNCTIONS AND EVALUATION METRICS
**[REWARD]**     
* RL4LMì€ <span style="background-color:#FFFFF0">ê°ê°ì˜ token</span>ë³„ ë˜ëŠ” <span style="background-color:#FFFFF0">ê°ê°ì˜ sequence</span> **generation rewardsì„ ìœ„í•œ ì¼ë°˜ì ì¸ ì¸í„°í˜ì´ìŠ¤**ë¥¼ ì œê³µí•œë‹¤.       
* êµ¬ì²´ì ìœ¼ë¡œ, ROUGE(Lin, 2004), BLEU(Papineni et al., 2002), SacreBLEU(Post, 2018), METERE(Banerjee & Lavie, 2005), BLEURT(Sellam et al., 2020)ì™€ ê°™ì€ n-gram ì¤‘ì²© ë©”íŠ¸ë¦­ì— ëŒ€í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µ     
 â¡ ìœ ì‚¬í•˜ê²Œ ë‹¤ì–‘í•œ ë²”ìœ„ì˜ **task-specific metrics**ì— ê´‘ë²”ìœ„í•œ RL algorithmsì„ ì‹ ì†í•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤.              

**[EVALUATION METRICS]**     
ë³µì¡ì„±/ìœ íš¨ì„±/ìì—°ì„± ì§€í‘œ(diversity/fluency/naturalness metrics) ì¡´ì¬     
ì¦‰, ì•„ë˜ì˜ ì§€í‘œë“¤ ì¡´ì¬     
* perplexity   
* Mean Segmented Type Token Ratio (MSSTR)      
* Shannon entropy over unigrams and bigrams, he ratio of distinct n-grams over the total number of n-grams (Distinct-1, Distinct-2)        
* count of n-grams that appear only once in the entire generated text      
* **task-specific, model-based human preference metrics** such as classifiers trained on human preference data collected in the methodology         


  
  
---


## 3.3 ON-POLICY ACTOR-CRITIC ALGORITHMS
RL4LMsëŠ” ì–¸ì–´ í™˜ê²½ì— ëŒ€í•œ **on-policy actor-critic algorithms**ì„ í†µí•´ LMì„ **ì²˜ìŒë¶€í„° fine-tuning**í•˜ê³  **í›ˆë ¨**í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•œë‹¤. 


<details>
<summary>ğŸ“œ actor-criticë€? </summary>
<div markdown="1">
 
ì—ì´ì „íŠ¸ì˜ í–‰ë™ í™•ë¥ ì„ ì§ì ‘ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì€ ë¶ˆì•ˆì •í•˜ê¸° ë•Œë¬¸ì— ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê°™ì´ ì¨ì„œ ì•ˆì •ì„±ì„ ë†’ì´ëŠ” ê²ƒ     
- **actor:** pì„¸íƒ€(a_t|s_t)ë¥¼ update -> ì—…ë°ì´íŠ¸í•œ Qff ê¸°ë°˜ìœ¼ë¡œ action    
- **critic:** Qwë¥¼ update -> actorì˜ actionì— ëŒ€í•œ í‰ê°€ë¥¼ í•˜ëŠ” ê²ƒ -> ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Që¥¼ ì—…ë°ì´íŠ¸     

 
ì„¸íƒ€ì™€ w e ì—…ë°ì´íŠ¸ 
 
</div>
</details>  


ê³µì‹ì ìœ¼ë¡œ, ì´ algorithms í´ë˜ìŠ¤ëŠ” ì•„ë˜ ê¶¤ì ì— ëŒ€í•´ **long term discounted rewardsì„ maximize**í•˜ê¸° ìœ„í•´ ì£¼ì–´ì§„ ìƒíƒœì—ì„œ actionì„ ì„ íƒí•˜ë ¤ê³  ì‹œë„í•˜ëŠ” í•¨ìˆ˜ì¸ $$Ï€_Î¸:S â†’ A$$ë¡œ ì •ì˜ëœ parameterized
control policyì„ í›ˆë ¨í•  ìˆ˜ ìˆê²Œ í•œë‹¤. 
![image](https://user-images.githubusercontent.com/76824611/219166269-d8909dd5-af30-41b3-bfbe-c086f5da449b.png)


ìš°ë¦¬ì˜ ë²¤ì¹˜ë§ˆí¬ ì‹¤í—˜ì€ agentâ€™s policy $$Ï€_Î¸ = Ï€_0$$ì„ ì´ˆê¸°í™”í•˜ëŠ” **"$$Ï€_0$$"ìœ¼ë¡œ í‘œì‹œëœ  pre-trained LM ì„ fine-tuning**í•˜ëŠ” ë° ì¤‘ì ì„ ë‘”ë‹¤.     

ë§ˆì°¬ê°€ì§€ë¡œ, value functionë¥¼ ì¶”ì •í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” **value network $$V_Ï†$$ ë„ $$Ï€_0$$ë¶€í„° ì´ˆê¸°í™”**ëœë‹¤.    
except) ë‹¨ì¼ ìŠ¤ì¹¼ë¼ ê°’ì„ ì¶œë ¥í•˜ê¸° ìœ„í•´ ì„ì˜ë¡œ ì´ˆê¸°í™”ë˜ëŠ” ìµœì¢… ê³„ì¸µì€ ì´ˆê¸°í™” ì œì™¸     

ë‹¤ë¥¸ deep RL actor-critic algorithmsê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ value($$V^Ï€_t$$)ì™€ Q-value($$Q^Ï€_t$$) í•¨ìˆ˜ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.     

![image](https://user-images.githubusercontent.com/76824611/219172882-757709ec-1549-445e-bd6f-9104aa1038a5.png)

ìœ„ì˜ ì‹ì€ ì•„ë˜ì˜ advantage functionì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤.    
(ì´ê²Œ ì´í•´ê°€ ì•ˆëœë‹¤...? ê·¸ëŸ¼ ê°•í™”í•™ìŠµ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìµíˆê³  ì˜¤ì,,,)     
![image](https://user-images.githubusercontent.com/76824611/219173244-d3b44b3e-5a3b-401a-9ef7-78e5b4ba228b.png)

â¡ **training stabilityë¥¼ ë†’ì´ê¸°** ìœ„í•´ì„  Generalized Advantage Estimationë¥¼ ì´ìš©í•˜ì—¬ **advantageì„ ê·¼ì‚¬í™”**í•œë‹¤.    



agentì˜ input-output pair(x, y)ê³¼ generation ì˜ˆì¸¡ì„ ê³ ë ¤í•  ë•Œ,     
environment rewardsì€ sequence-levelì´ê³  í¬ì†Œí•˜ë‹¤.    

ê·¸ëŸ¬ë¯€ë¡œ, ëª¨ë“  on-policy algorithmsì— ëŒ€í•´ **[token-level KL penalty](https://arxiv.org/pdf/2109.10862.pdf)ë¥¼ ì‚¬ìš©í•´ reward ê¸°ëŠ¥ì„ ì •ê·œí™”**í•˜ì—¬,    
**ëª¨ë¸ì´ ì´ˆê¸°í™”ëœ LM â‰¥ 0ì—ì„œ ë„ˆë¬´ ë©€ë¦¬ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ í•œë‹¤**.     

í˜•ì‹ì ìœ¼ë¡œ ì •ê·œí™”ëœ reward í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤:    
![image](https://user-images.githubusercontent.com/76824611/219179124-fb673285-5e22-4dff-8b57-d383fc043074.png)
* $$R hat$$: ì •ê·œí™”ëœ KL reward    
* $$y$$: gold-truth predictions       
* $$KL(Ï€_Î¸(a_t \|s_t) \| Ï€_0(a_t \|s_t)) = (log Ï€_0(a_t \|s_t) âˆ’ log Ï€_Î¸(a_t \|s_t))$$ì„ ë§Œì¡±í•œë‹¤.    
* KL coefficient Î²ëŠ” ë™ì ìœ¼ë¡œ ì ìš©(dynamically)    \


----

# 4 NLPO: NATURAL LANGUAGE POLICY OPTIMIZATION
ì–¸ì–´ ìƒì„± ì‘ì—… ê³µê°„ì€ ëŒ€ë¶€ë¶„ì˜ ì´ì‚° ì‘ì—… ê³µê°„ RL ì•Œê³ ë¦¬ë“¬ì´ ì„¤ê³„ëœ ê²ƒë³´ë‹¤ í›¨ì”¬ í° ê·œëª¨ì´ë‹¤(Ranzato et al., 2016; Ammanabrolu, 2021). ì˜ˆë¥¼ ë“¤ì–´ GPT-2/3ê³¼ T5ì˜ ì–´íœ˜ í¬ê¸°ëŠ” ê°ê° 50Kì™€ 32Kì´ë‹¤. ìš°ë¦¬ëŠ” ê¸°ì¡´ RL ë°©ë²•ìœ¼ë¡œ LMì„ í›ˆë ¨í•  ë•Œ ë™ì‘ ê³µê°„ì˜ í¬ê¸°ê°€ ë¶ˆì•ˆì •ì˜ í•µì‹¬ ì›ì¸ì´ë¼ê³  ê°€ì •í•œë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¡°ì¹˜ ì œê±°/ë¬´íš¨ ì¡°ì¹˜ ë§ˆìŠ¤í‚¹ì— ëŒ€í•œ ì‘ì—…ì—ì„œ ì˜ê°ì„ ì–»ì€ NLPO(ìì—°ì–´ ì •ì±… ìµœì í™”)ë¥¼ ì†Œê°œí•œë‹¤(Zahavy et al., 2018; Huang & OntÃ¡nÃ³n, 2020; Ammanabrolu & Hausknecht, 2020). PPOì˜ ë§¤ê°œ ë³€ìˆ˜í™”ëœ ë§ˆìŠ¤í‚¹ í™•ì¥ì¸ NLPOëŠ” í›ˆë ¨í•  ë•Œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ëœ ê´€ë ¨ëœ í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•œë‹¤. NLPOëŠ” top-p ìƒ˜í”Œë§ì„ í†µí•´ ì´ë¥¼ ë‹¬ì„±í•˜ëŠ”ë°, ì´ëŠ” í† í°ì„ í™•ë¥  ë§¤ê°œ ë³€ìˆ˜ pë³´ë‹¤ ëˆ„ì  í™•ë¥ ì´ í° ê°€ëŠ¥í•œ ê°€ì¥ ì‘ì€ ì„¸íŠ¸ë¡œ ì œí•œí•œë‹¤(Holtzman et al., 2018).
Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2016; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively. We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods. To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & OntaÃ±Ã³n, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018).


íŠ¹íˆ, NLPOëŠ” ë§ˆìŠ¤í‚¹ ì •ì±… Â»ë¥¼ ìœ ì§€ ê´€ë¦¬í•©ë‹ˆë‹¤. ë§ˆìŠ¤í‚¹ ì •ì±…ì€ í˜„ì¬ ì •ì±…(Â»)ì˜ ë³µì‚¬ë³¸ì´ì§€ë§Œ Â» ë‹¨ê³„ë§ˆë‹¤ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤. ë§¤ê°œ ë³€ìˆ˜í™”ëœ ìœ íš¨í•˜ì§€ ì•Šì€ ë§ˆìŠ¤í¬ëŠ” ë¨¼ì € ì–´íœ˜ 4ì—ì„œ top-p í† í°ì„ ì„ íƒí•œ ë‹¤ìŒ ë‚˜ë¨¸ì§€ í† í°ì— ìœ íš¨í•˜ì§€ ì•Šì€ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ì—¬ ìƒì„±ëœë‹¤. ì¦‰, í›ˆë ¨ ì¤‘ì— Ï€ì—ì„œ ë™ì‘ì„ ìƒ˜í”Œë§í•  ë•Œ í™•ë¥ ì„ 0ìœ¼ë¡œ ì„¤ì •í•œë‹¤. ì´ ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ ì •ì±… Ï€ëŠ” ì •ì±… ì™¸ Q-ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜(ì•ˆë“œë¦¬ì½”ë¹„ì¹˜)ì—ì„œ ì˜ê°ì„ ë°›ì•˜ë‹¤et al., 2017), Â§0ì—ì„œ ë„ì¶œëœ KL í˜ë„í‹°ë³´ë‹¤ ë” ë§ì€ ì‘ì—… ê´€ë ¨ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì˜ ì´ì ê³¼ ë³´ìƒ í•´í‚¹ì˜ ìœ„í—˜ ì‚¬ì´ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ì¶”ê°€ ì œì•½ ì¡°ê±´ì„ ì •ì±… Â»ì— ì œê³µí•œë‹¤. ìš°ë¦¬ëŠ” ì•Œê³ ë¦¬ì¦˜ 1ì—ì„œ ìœ ì‚¬ ì½”ë“œë¥¼ ì œê³µí•œë‹¤(ë…¹ìƒ‰ ë¶€ë¶„ì€ PPOì™€ì˜ ì°¨ì´ì ì„ ê°•ì¡°í•œë‹¤).

Specifically, NLPO maintains a masking policy Ï€Ïˆ: the masking policy is a copy of the current policy (Ï€Î¸), but is updated only every Âµ steps. A parameterized-invalid-mask is created from Ï€Ïˆ by first selecting the top-p tokens from the vocabulary, 4 and then applying an invalid-mask to the remaining tokensâ€”i.e. setting their probabilities to zero when sampling actions from Ï€Î¸ during training; this periodic updating policy Ï€Ïˆ is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017), providing the policy Ï€Î¸ with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from Ï€0 and the risk of reward hacking. We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).


# GRUE (GENERAL REINFORCED-LANGUAGE UNDERSTANDING EVAL)
GRUE is a collection of 7 generative NLP tasks. To combat reward hacking for any single metric,
each task is evaluated at test time according to a task-specific mix of metrics, detailed in Table 1.
The metrics span two categories. Task preference metrics capture how well the models produce
generations that satisfy the desiderata of the specific generation task, e.g., for Commongen, if the
generations contain all the required words, or for IMDB, how positive the generated completions
are. Naturalness metrics capture fluency, readability, etc. and provide perspective on factors beyond
semantics. At training time, there are no special restrictions: models are free to use the supervised
data, compute metrics on intermediate generations, etc. Train/val/test splits follow the original works.
All results are averaged over multiple seeds, with exact counts being found in Appendix B.  



Experimental Setup. We use RL4LMs to test a large range of algorithms on the GRUE benchmark.
Specifically: We compare 3 algorithms for direct fine-tuning â€” Supervised, PPO,5
and NLPO. In addition, we consider a hybrid approach of supervised learning and our RL methods by applying
PPO and NLPO on checkpoints that have been fine-tuned in a supervised fashionâ€”we call these
Supervised+PPO, Supervised+NLPO. As an additional baseline, we additionally run zero-shot
evaluations where we design prompts which aim to elicit task-specific generations, but with no
training data or parameter updates.


For each task, to isolate the effect of training method, we select a single pre-trained LM backbone.
For IMDB text continuation we use GPT-2 (117m parameters), and for the rest of the tasks we use
T5-base (220m parameters). For our RL models (PPO, NLPO, Supervised+PPO, Supervised+NLPO),
for a thorough investigation of how reward-hacking might interplay with GRUE, we run a separate set
of experiments optimizing multiple task rewards for each task independently, e.g., for Commongen
which has 6 task rewards (CIDER, ROUGE-2, ROUGE-L, BLEU-3, BLEU-4, METEOR) we run 6
different experiments optimizing each metric independently and report all possible metrics seen in
Table 1 regardless of which individual metric was being optimized for.


Human Participant Study. We gather human judgments for five of the tasks in GRUE. In doing so,
our goals are 1) to validate that the automated metrics we selected for GRUE correlate with human
judgments with respect to relative ranking between models; and 2) to provide additional empirical
comparisons regarding NLPO vs. PPO, ablations to study the effects of the KL naturalness penalty,
etc. We specifically consider IMDB, Commongen, ToTTo, DailyDialog, and CNN Daily Mail. For
each individual sample in a task, we ask 3 unique human raters to provide Likert judgments of 1)
quality, i.e., for the specific task, how correct/appropriate is the generation, given the context, and
2) fluency, i.e., how well-written is the generation. We used Amazon Mechanical Turk, and paid
crowdworkers a minimum of $15/hr. More details, including qualification information, interface
screenshots, instructions, etc. are given in the corresponding Appendicies.



5.1 RESULTS ON GRUE: WHICH ALGORITHM SHOULD BE USED TO LEARN PREFERENCES?
Figures 2(a), 2(b) present the results on GRUE, split into task metrics and naturalness metrics, and
Tables 2, 3 highlight key results via ablation studies. Full results are available in Appendix B. For text
continuation and summarization, with non-trivial zero-shot performance, RL tends to perform better
than supervised training, but for tasks like Commongen and ToTTo, which have very low zero-shot
performance, supervised training performs bestâ€”with both approaches outperforming zero-shot.
However, using RL+Supervised learning in conjunction works best; NLPO+supervised and
PPO+supervised usually always outperforms NLPO/PPO (or supervised in isolation) across both task
metrics and naturalness metrics. Supervised warm-starting is particularly effective for Commongen
and ToTTo, which our results suggest are more prone to reward hacking. The one exception to this
trend is DailyDialog where the RL models outperform warm-started Supervised+RL models likely

due to the low performance of the Supervised models. We note that Supervised+NLPO using a
T5-base (220m parameter) LM currently outperforms all the models on the ToTTo leaderboard, many
of which have â‰¥ 3b parameter supervised modelsâ€”suggesting that RL is parameter efficient as well.
In these cases, it is critical that the initial policy already contain (some) signal for the task due to it
being used as a KL constraint and masking constraint in NLPO. If the mask contains no initial priors
about task specific language, it will be eliminating the wrong actionsâ€”a better initial policy leads to
better RL performance downstream.

Human agreement with automated metrics. As human judgments can be noisy, we run additional
statistical analysis such as measuring inter-annotator agreement, via Krippendorfâ€™s alpha score,
and using a one-way ANOVA followed by a post-hoc Tukey HSD test to measure if differences in
means of average scores between pairs of models are significant. We find that trends in our human
evaluations generally match those seen in the automated metrics for both task and naturalness metrics
(see Figures 2(c), 2(d) which summarize Appendix Tables 10,15,21,26, 35â€”Supervised+NLPO >
Supervised â‰¥ Supervised+PPO > NLPO â‰¥ PPO > Zero-shotâ€”with the exception of Supervised
outperforming Supervised+PPO on 2 out of 5 tasks when automated metrics would indicate that
Supervised+PPO outperforms Supervised on all of the tasks. We draw two conclusions from this:
(1) if the generated text is above a certain threshold of naturalness, the automated metrics usually
correlate with human judgements; (2) usually but not always as seen in the relative performance of
Supervised and Supervised+PPO, potentially indicating reward hacking behaviors undetected by
automated metrics but caught by human preference feedback.



5.2 PREFERENCE REWARD LEARNING, SELECTION, AND HACKING
While the GRUE benchmarkâ€™s metric for each task is an average over several measures, the RL
models we trained optimized only a single metric independently. Thus, we can empirically investigate
which metric for which GRUE produces the best results. We observe that many possible single metric
rewards provide task performance gains over supervised methods (results shown in Fig. 3(a), 2(c) are
averaged across these reward functions) with the condition that the text is also coherent and natural.



Which constraints best prevent reward hacking? The reward function in Equation 1 balances a
task-specific reward with a KL constraint â€” models are penalized from straying too far from a base
LM in their pursuit of high reward (Table 3 and Appendix Table 5) clearly show that if KL constraints
are removed entirely, models reward hack). But which model works best as a base regularizing LM?
When the initial policy (i.e., the raw, pretrained model) has low performance on the task, the KL
penalty pushes the policy towards nonsense, e.g. on Commongen and ToTTo the trained policy learns
to simply repeat portions of the input (as seen in Tables B.4.5, B.6.4). This behavior is mitigated
if the base regularizing LM is the supervised modelâ€”the reward encourages the policy to balance
the task-specific reward and a more reasonable regularization term. Deriving KL penalties from
warm-started initial policies is critical for performance on such tasks.


PPO vs. NLPO. Figure 2 shows that NLPO generally outperforms PPO and supervised, especially
when applied after supervised training. We hypothesize that the primary reason for NLPOâ€™s improved
performance and stability is because the masking policy provides an additional constraint for the
current policy. This constraint is not based on the initial untuned policy like the KL penalty but of
the policy from Âµ iterations ago and likely contains more task-relevant information learned during
RL training. Table 3 (and Appendix Table 8) shows how performance increases up to a point and
then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by
eliminating less tokens at each step, implying that there is a balance to be found in how much the
model should be constrained during RL training.


Human Preference Reward Learning. To this point, our experiments have largely focused on
optimizing evaluation metrics that correlate with human judgments, e.g., METEOR. Here: we
additionally test how well preferences can be learned from direct human feedback. For this, we focus
on Commongen â€” a GRUE dataset well-suited for displaying differences due to human preferences.
First, we randomly select prompts from the Commongen train dataset and sample a single completion
from both the Supervised and Supervised+NLPO models. We then present the prompt and the two
completion candidates to 3 unique crowdworkers and ask them to select which one they prefer with
respect to commonsense/fluency for 417 unique pairs (Krippendorf Î± = .28). We use this data
to train a reward model, T5-11B Raffel et al. (2020), on the balanced binary classification task of
predicting which of the pair was preferred by a majority of 3 annotators, conditioned on the prompt
and completion. The resulting model achieved 69.5 test ROC AUC suggesting it indeed captures
average human preferences. Additional details on this process are found in Appendix B.4.4. We train
Supervised+RL with a METEOR-only reward as a baseline, and compare it to a reward function
that uses the fine-tuned T5-11B model. Finally, we rerun the same pairwise preference collection
procedureâ€”this time sampling from Commongen testâ€”with human participants to compare the
generations from a preference optimized RL policy to the previously best Supervised+NLPO policy.
Comparing the METEOR-only to the preference model, the generations produced by the human
feedback model are preferred in 682 cases, compared to the METEOR-only model which is preferred
in 587 cases (p < 0.01 the models are equally preferred). This implies that this pipeline of collecting
preferences, training a reward, and further tuning the policy improves alignment to human preferences.





5.3 DATA BUDGET: IMPROVE YOUR REWARD OR GATHER MORE DEMONSTRATION?
Given a fixed data collection budget, is it more efficient to gather feedback to improve a learned
reward function or to gather more expert demonstrations? We use the IMDB text continuation task as
a case study. In the IMDB task, a model is given a partial movie review as a prompt, and is asked to
continue it as positively as possible (even if the prompt was negative). The original dataset consists of
movie reviews and sentiment labels of positive, negative, or neutral. A DistilBERT (Sanh et al., 2019)
classifier is trained on these labels and used to provide sentiment scores on how positive a given piece
of text is, which serves as the task reward. The trade-off is between gathering more: 1) sentiment
labels (improving the reward); or 2) positive sentiment reviews (improving supervised training).
We train a classifier on varying amounts of training data and evaluate on the held out test datasetâ€”
finding as expected that more training data improves test accuracy and so results in a higher quality
reward. We then use each of these rewards of varying quality during RL training, and evaluate using
the same metric as GRUE (i.e., a classifier trained with the entire training set). As seen in Table 3,
we find that improving the reward quality improves LM performance as well. Further, we trained a
supervised model with at least as many samples used to train each of these reward classifiers. We find
that a learned reward function enables greater performance when used as a signal for an RL
method than a supervised method trained with 5 times more data. This implies that improving
reward models can be more data efficient than collection expert demonstrations for a taskâ€”and thatâ€™s
not accounting for the fact that assigning sentiment labels is likely a simpler task than writing full
demonstrations. Further details on this ablation are found in Appendix Table 7.



5.4 PRACTICAL CONSIDERATIONS: WHICH IMPLEMENTATION DETAILS MATTER MOST?
Generation as a token-level MDP, not a bandit environment. Most recent works that tune LMs
using RL do so by calculating a reward for all the tokens in the sentence (Wu et al., 2021; Ouyang
et al., 2022; Lu et al., 2022). This setting is equivalent to a bandit feedback environment where the
action space is the space of all possible generations for the task (Sutton & Barto, 2018). This type of
environment can be simulated within our RL formulation by setting the discount factor Î³ = 1. Table 3
(and Appendix Table 6) shows that this causes instability in training with respect to naturalness
in both PPO and NLPO for IMDB. Our standard setting is Î³ = 0.95 when calculating discounted
rewards-to-go in the token-level MDP formulation, which reduces the magnitude of the reward that is
applied to tokens selected at the beginning. The sentiment scores are approximately the same between
both settings but the naturalness of language in the bandit setting is significantly less â€”indicating
that discounting rewards with Î³ < 1 via a token-level MDP formulation is at least sometimes more
effective for language generation.


Dropout and Sampling. We found two other implementation details to be critical for stability of
RL training. The first is dropout, which in its standard form was found to cause instability in policy
gradient methods in continuous control settings by Hausknecht & Wagener (2022). We find a similar
effect when using dropout when RL training LMs as well, with training loss often diverging for
dropout > 0 in training. The second important detail, particularly affecting the machine translation
task, is sampling methods. We find that using the same sampling methods during exploration and
inference is critical to translating training performance to test performanceâ€“else the model exhibits
high train rewards but low test metrics.



6 CONCLUSIONS
Weâ€™re hopeful that the GRUE benchmark and the RL4LMs library can push progress in aligning
language models to human preferences via RL methods by providing the community with a standard
means of comparing methods. Furthermore, weâ€™re optimistic that, as the stability and consistency of
training improves, our methods provide a path towards iterative improvement of language technologies, with deployment, user feedback collection, and re-optimization enabling better user experiences
when interacting with generative models.


