---
title: "(T5) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ì •ë¦¬"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: #[Paper]
layout: post
tag:
#- Paper
order: 0

comments: true
---


# Abstract
<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ ë°°ê²½]**</span>    
ëª¨ë¸ì´ Downstream Taskì—ì„œ fine-tuningë˜ê¸° ì „ì— data-rich taskì—ì„œ pre-trainì„ í•˜ëŠ” ê²ƒì€  ìì—°ì–´ ì²˜ë¦¬(NLP)ì˜ ê°•ë ¥í•œ ê¸°ìˆ ë¡œ ë¶€ìƒí–ˆë‹¤.     
ì¦‰ ì´ëŸ¬í•œ [transfer learning](https://yerimoh.github.io/DL12/)ì€ NLPì— í° ë°œì „ì„ ê°€ì ¸ì™”ë‹¤.    

<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ì˜ ëª©ì ]**</span>    
* ëª¨ë“  í…ìŠ¤íŠ¸ ê¸°ë°˜ ì–¸ì–´ ë¬¸ì œë¥¼ text-to-text í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ <span style="background-color:#fff5b1">NLPì— ëŒ€í•œ [transfer learning](https://yerimoh.github.io/DL12/)ë¥¼ ì „ë°˜ì ìœ¼ë¡œ íƒêµ¬</span>í•œë‹¤.     
* ìœ„ íƒêµ¬ë¥¼ í†µí•´ ì–»ì€ í†µì°°ë ¥ê³¼ ê·œëª¨ ë° ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•  <span style="background-color:#fff5b1">"Colossal Clean Crawled Corpus"ë¥¼ ê²°í•©</span>í•˜ì—¬ ë§ì€ NLP evaluationì—ì„œ SOTA ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.        
â¡ ê³µê°œí•œ [data set, pre-trained models, and code](https://github.com/google-research/text-to-text-transfer-transformer) ë§í¬ë¥¼ ì²¨ë¶€í•˜ì˜€ë‹¤.        


<details>
<summary>ğŸ“œ Downstream Task ì˜ë¯¸ ë³´ê¸°</summary>
<div markdown="1">
  

êµ¬ì²´ì ìœ¼ë¡œ í’€ê³  ì‹¶ì€ ë¬¸ì œë“¤ì„ ë§í•œë‹¤.

NLPì—ì„œëŠ” ì–¸ì–´ëª¨ë¸ì„ pre-trainë°©ì‹ì„ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ê³ ,    
ê·¸ í›„ì— ì›í•˜ê³ ì í•˜ëŠ” taskë¥¼ fine-tuningí•˜ëŠ” ë°©ì‹ì„ í†µí•´ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ë•Œ, taskë¥¼ Downstream Taskë¼ í•œë‹¤.

ì˜ˆë¥¼ë“¤ì–´, BERTì˜ ì–¸ì–´ëª¨ë¸ì„ ì§ˆì˜ì‘ë‹µ Taskë¼ì¸ squadë¥¼ í•™ìŠµí•œë‹¤ê³  í• ë•Œ, ì´ë•Œ ì§ˆì˜ì‘ë‹µ Taskë¥¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ Taskë¡œ ë³¼ ìˆ˜ ìˆì„ê²ƒì´ë‹¤.  
  
  
</div>
</details>  

----
----



# Introduction

<span style="background-color:#F5F5F5">**[ìµœê·¼ ì—°êµ¬ ë™í–¥]**</span>    
ìì—°ì–´ ì²˜ë¦¬(NLP)ì‘ì„ ì„ ìœ„í•œ trainì´ ê°€ëŠ¥í•˜ì—¬ë©´ ëª¨ë¸ì´ **downstream learningì— ì í•©í•œ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬**í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.              
ê·¸ëŸ°ë° ì´ëŠ” **ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ "ì´í•´"í•  ìˆ˜ ìˆê²Œ í•™ìŠµí•œë‹¤ê³  ë³´ê¸°ì—” í˜ë“¤ë‹¤**.               
â¡ í˜„ëŒ€ ê¸°ê³„ í•™ìŠµ ê´€í–‰ì—ì„œ ì´ëŸ¬í•œ trainì´ ëª…ì‹œì ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” ê²½ìš°ê°€ ê±°ì˜ ì—†ì–´, ëŒ€ì‹  **Auxiliary Taskì˜ ì¼ë¶€**ë¡œ í•™ìŠµë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.        


<details>
<summary>ğŸ“œ Auxiliary Task ì˜ë¯¸ ë³´ê¸°</summary>
<div markdown="1">

ë³¸ taskëŠ” ì•„ë‹ˆì§€ë§Œ, ë³¸ taskì—ì„œì˜ ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë³´ì¡° task  

</div>
</details> 
  
ìµœê·¼ì—ëŠ” **ë°ì´í„°ê°€ í’ë¶€í•œ ì‘ì—…**ì— ëŒ€í•´ **ì „ì²´ ëª¨ë¸ì„ pre-trainí•˜ëŠ” ê²ƒ**ì´ ì ì  ë” **ì¼ë°˜í™”**ë˜ê³  ìˆë‹¤.     
ì´ìƒì ìœ¼ë¡œ, ì´ pre-trainì€ ëª¨ë¸ì´ ë²”ìš© ëŠ¥ë ¥ê³¼ ì§€ì‹ì„ ê°œë°œí•˜ê²Œ í•˜ê³ , ì´ë¥¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ìœ¼ë¡œ ì´ì „í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.     
â¡ ë” í° ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë” í° ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒë§Œìœ¼ë¡œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.    




<span style="background-color:#F5F5F5">**[í˜„ ê²½í–¥ìœ¼ë¡œ ì¸í•œ í•œê³„]**</span>    
ì´ëŸ¬í•œ ê²½í–¥ìœ¼ë¡œ ì¸í•´ NLPì— ëŒ€í•œ **transfer learning ë°©ë²•ë¡ ì„ ê°œë°œ**í•˜ëŠ” ì—°êµ¬ë“¤ì´ í™œë°œí•˜ê²Œ ì´ë£¨ì–´ì¡Œë‹¤.      
ì´ë ‡ê²Œ ì´ ë¶„ì•¼ê°€ ê¸‰ì„±ì¥í•˜ì—¬ ì—°êµ¬ê°€ í™œë°œí•´ì§€ë‹ˆ ì•„ë˜ì™€ ê°™ì€ ì‘ì—…ë“¤ì´ ì–´ë ¤ì›Œì¡Œë‹¤.       
* ì—¬ëŸ¬ algorithmsì„ ë¹„êµ    
* ìƒˆë¡œìš´ contributionsì˜ íš¨ê³¼ íŒŒì•…    
* transfer learningì„ ìœ„í•œ ê¸°ì¡´ ë°©ë²•ì˜ space ì´í•´        


âœ” ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì€ ì´ ë¶„ì•¼ì˜ ì›í™œí•œ ì´í•´ë¥¼ ìœ„í•´, **ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ì„ ì²´ê³„ì ìœ¼ë¡œ ì—°êµ¬**í•˜ê³  **í•„ë“œì˜ í˜„ì¬ í•œê³„ë¥¼ ë°€ì–´ë‚¼ ìˆ˜ ìˆëŠ” transfer learningì— ëŒ€í•œ í†µí•©ëœ ì ‘ê·¼ë²•ì„ í™œìš©**í•œë‹¤. 
 


<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ í•´ê²°ì±…]**</span>      
ë³¸ ë…¼ë¬¸ workì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” <span style="background-color:#fff5b1">ëª¨ë“  í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¬¸ì œë¥¼ **â€œtext-to-textâ€ ë¬¸ì œë¡œ ì²˜ë¦¬**</span>í•˜ëŠ” ê²ƒì´ë‹¤.             
ì¦‰, **í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ë“¤ì´ê³  ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥ìœ¼ë¡œ ìƒì„±**í•˜ëŠ” ê²ƒì´ë‹¤.          
* ì´ëŸ¬í•œ í…ìŠ¤íŠ¸ ê°„ í”„ë ˆì„ì›Œí¬ëŠ” ìš°ë¦¬ê°€ ê³ ë ¤í•˜ëŠ” ëª¨ë“  ì‘ì—…ì— **ë™ì¼í•œ ëª¨ë¸, ëª©í‘œ, í›ˆë ¨ ì ˆì°¨ ë° decoding í”„ë¡œì„¸ìŠ¤ë¥¼ ì§ì ‘ ì ìš©**í•  ìˆ˜ ìˆê²Œ í•œë‹¤.          
* ë³¸ ë…¼ë¬¸ì€ ì§ˆë¬¸ ë‹µë³€, ë¬¸ì„œ ìš”ì•½ ë° ê°ì • ë¶„ë¥˜ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ì˜ì–´ ê¸°ë°˜ NLP ë¬¸ì œì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€í•˜ì—¬ ì´ëŸ¬í•œ **ìœ ì—°ì„±ì„ í™œìš©**í•œë‹¤.        
* ì´ í†µí•© ì ‘ê·¼ë²•ì„ í†µí•´, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ transfer learningì˜ target, ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°ì´í„° ì„¸íŠ¸ ë° ê¸°íƒ€ ìš”ì¸ì˜ íš¨ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ë™ì‹œì— ì´ì „ì— ê³ ë ¤ë˜ì—ˆë˜ ê²ƒ ì´ìƒìœ¼ë¡œ **ëª¨ë¸ê³¼ ë°ì´í„° ì„¸íŠ¸ë¥¼ í™•ì¥**í•˜ì—¬ NLPì— ëŒ€í•œ **transfer learningì˜ í•œê³„ë¥¼ íƒìƒ‰**í•  ìˆ˜ ìˆë‹¤.    




ë³¸ ë…¼ë¬¸ì€ ë³¸ ë…¼ë¬¸ì˜ ëª©í‘œê°€ ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ê·¸ ë¶„ì•¼ê°€ ì–´ë””ì— ì„œ ìˆëŠ”ì§€ì— ëŒ€í•œ í¬ê´„ì ì¸ ê´€ì ì„ ì œê³µí•˜ëŠ” ê²ƒ**ì„ì„ ê°•ì¡°í•œë‹¤.      
ì¦‰, ìš°ë¦¬ì˜ ì‘ì—…ì€ ë³¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ taskë¡œ êµ¬ì„±ëœë‹¤.         
â¡ ì£¼ë¡œ ê¸°ì¡´ ê¸°ìˆ ì˜ ì¡°ì‚¬, íƒêµ¬ ë° ê²½í—˜ì  ë¹„êµ       


ë˜í•œ ë³¸ ë…¼ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ **í˜„ì¬ ì ‘ê·¼ ë°©ì‹ì˜ í•œê³„ë¥¼ íƒêµ¬**í•œë‹¤.        
ë³¸ ë…¼ë¬¸ì´ ê³ ë ¤í•˜ëŠ” ë§ì€ taskì—ì„œ SOTA ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì²´ê³„ì ì¸ ì—°êµ¬(train ëª¨ë¸ì„ ìµœëŒ€ 110ì–µ ê°œ parametersê¹Œì§€ í™•ì¥)ìˆ˜í–‰       
* ì´ ê·œëª¨ì˜ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì›¹ì—ì„œ ê¸ì–´ë‚¸ ìˆ˜ë°± ê¸°ê°€ë°”ì´íŠ¸ì˜ clean ì˜ì–´ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ë°ì´í„° ì„¸íŠ¸ì¸ "Colossal Clean Crawled Corpus"**(C4)** ë¥¼ ì†Œê°œí•œë‹¤.        
* **transfer learningì˜ í•µì‹¬ ê¸°ëŠ¥**ì€, **ë°ì´í„°ê°€ ë¶€ì¡±í•œ í™˜ê²½ì—ì„œ  pre-trained modelsì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±**ì´ë¼ëŠ” ê²ƒì„ ì¸ì‹í•˜ì—¬,      
[ì½”ë“œ, ë°ì´í„° ì„¸íŠ¸ ë° ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸](https://github.com/google-research/text-to-text-transfer-transformer
)ì„ ë¦´ë¦¬ìŠ¤í–ˆë‹¤.    




<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ êµ¬ì„±]**</span>            
* base modelê³¼ ê·¸ êµ¬í˜„    
* ëª¨ë“  text processing ë¬¸ì œë¥¼ text-to-text ì‘ì—…ìœ¼ë¡œ ê³µì‹í™”í•˜ëŠ” ì ˆì°¨ ë° ê³ ë ¤í•˜ëŠ” ì‘ì—… ëª¨ìŒì— ëŒ€í•œ ë…¼ì˜    
* ì„¹ì…˜ 3ì—ì„œ, NLPì— ëŒ€í•œ transfer learning ë¶„ì•¼ë¥¼ íƒêµ¬í•˜ëŠ” ëŒ€ê·œëª¨ ì‹¤í—˜ ì„¸íŠ¸ë¥¼ ì œì‹œ          
* ì„¹ì…˜(ì„¹ì…˜ 3.7)ì˜ ëì—ì„œ, ìš°ë¦¬ëŠ” ì²´ê³„ì ì¸ ì—°êµ¬ì˜ í†µì°°ë ¥ì„ ê²°í•©í•˜ì—¬ ê´‘ë²”ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ì–»ìŒ    
* ì„¹ì…˜ 4ì—ì„œ, ê²°ê³¼ì— ëŒ€í•œ ìš”ì•½ì„ ì œê³µ í•œ ë’¤, ë¯¸ë˜ì— ëŒ€í•œ ì „ë§ìœ¼ë¡œ ë§ˆë¬´ë¦¬             




---
---


# 2. Setup    



ë³¸ ë…¼ë¬¸ì€ large-scale ê²½í—˜ì  ì—°êµ¬ì˜ ê²°ê³¼ë¥¼ ì œì‹œí•˜ê¸° ì „ì— ì•„ë˜ì™€ ê°™ì€ ê²ƒë“¤ì„ ë¨¼ì € ì œì‹œí•œë‹¤,   
* [Transformer ëª¨ë¸ ì•„í‚¤í…ì²˜](https://yerimoh.github.io/Lan/)ì™€ í‰ê°€í•˜ëŠ” downstream tasksì„ í¬í•¨í•˜ì—¬ ê²°ê³¼ë¥¼ ì´í•´í•˜ëŠ” ë° í•„ìš”í•œ **ë°°ê²½ ì£¼ì œë¥¼ ê²€í† **í•œë‹¤.        
* **ëª¨ë“  ë¬¸ì œë¥¼ text-to-text taskìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì ‘ê·¼ ë°©ì‹**ì„ ì†Œê°œ    
* **"Colossal Clean Crawled Corpus"(C4) ì„¤ëª…:** ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì†ŒìŠ¤ë¡œ ìƒì„±í•œ ê³µí†µ í¬ë¡¤ ê¸°ë°˜ ë°ì´í„° ì„¸íŠ¸ì„     




ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ëª¨ë¸ê³¼ í”„ë ˆì„ì›Œí¬ë¥¼  <span style="background-color:#fff5b1">â€œText-to-Text Transfer Transformerâ€ (T5)</span> ë¼ê³  ë¶€ë¥¸ë‹¤.      





---


## 2.1 Model

### base architecture: â€œTransformerâ€
NLPì— ëŒ€í•œ ì „ì´ í•™ìŠµì— ëŒ€í•œ ì´ˆê¸° ê²°ê³¼ëŠ” ë°˜ë³µ ì‹ ê²½ë§ì„ í™œìš©í–ˆì§€ë§Œ,  
ìµœê·¼ì—ëŠ” "Transformer" ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¼ë°˜í™”ë˜ì—ˆë‹¤.     


TransformerëŠ” transfer learningì— íš¨ê³¼ì ì´ë¯€ë¡œ, ì´í›„ ë‹¤ì–‘í•œ NLP ì„¤ì •ì— ì‚¬ìš©ë˜ì—ˆë‹¤.          
ë³¸ ë…¼ë¬¸ì—ì„œë„ **ëª¨ë“  ëª¨ë¸ì˜ baseë¥¼ Transformer ì•„í‚¤í…ì²˜**ë¡œ í•˜ì˜€ë‹¤.         

ì•„ë˜ì— ì–¸ê¸‰ëœ ì„¸ë¶€ì‚¬í•­ê³¼ ì„¹ì…˜ 3.2ì—ì„œ íƒêµ¬í•œ ë³€í˜•ì„ ì œì™¸í•˜ê³ ,     
ë³¸ ë…¼ë¬¸ì€ Transformer ì•„í‚¤í…ì²˜ì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.      


<details>
<summary>ğŸ“œ Transformerë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´? (ì°¸ê³ ìë£Œ) </summary>
<div markdown="1">

ì´ ë…¼ë¬¸(ì•„í‚¤í…ì²˜)ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³ ì‹¶ë‹¤ë©´ ì•„ë˜ ìë£Œë“¤ì„ ì°¸ê³ í•˜ì,    
* [ì›ë³¸ ë…¼ë¬¸](https://arxiv.org/abs/1706.03762)    
* í›„ì† íŠœí† ë¦¬ì–¼ 3,4(ë’¤ì— ë‚˜ì˜µë‹ˆë‹¤.)    
* [ë³¸ í¬ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•œ í•„ìê°€ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸](https://yerimoh.github.io/Lan/)         

</div>
</details> 
  85.5


ê·¸ë¦¬ê³  ë³¸ ë…¼ë¬¸ì—ì„œ Transformerì—ëŒ€í•´ ê°„ë‹¨ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ì¤¬ëŠ”ë° ì›í•œë‹¤ë©´ ì•„ë˜ ìì„¸í•œ ì„¤ëª… ë³´ê¸°ë¥¼ ëˆŒëŸ¬ í•œ ë²ˆ ì½ì–´ë³´ê¸¸ ë°”ë€ë‹¤.(ê·¸ë ‡ì§€ë§Œ ì› ë…¼ë¬¸ì„ ì½ì–´ë´ì•¼ ì•„ë˜ ë‚´ìš©ë“¤ë„ ì´í•´ê°€ ê°ˆ ê²ƒì´ë‹¤.)   

<details>
<summary>ğŸ“œ Transformerë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´?(ë³¸ ë…¼ë¬¸ì˜ ì„¤ëª…) </summary>
<div markdown="1">




 
**Transformerì˜ ì¤‘ìš”í•œ ìš”ì†ŒëŠ”,** [self-attention](https://yerimoh.github.io/Lan/#sub-layer1-self-attention)ì´ë‹¤.         
self-attentionì€ ê° ìš”ì†Œë¥¼ ë‚˜ë¨¸ì§€ sequenceì˜ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” attentionì˜ ë³€í˜•ì´ë‹¤.    

**Transformerì˜ êµ¬ì¡°ëŠ”,** [encoder-decoder ì•„í‚¤í…ì²˜](https://yerimoh.github.io/Lan/#transformer-%EB%AA%A8%EB%8D%B8-%EA%B0%9C%EC%9A%94)ë¡œ êµ¬ì„±ë˜ì—ˆìœ¼ë©° sequence-to-sequence ì‘ì—…ì„ ìœ„í•´ ê³ ì•ˆë˜ì—ˆë‹¤.        

ì „ë°˜ì ìœ¼ë¡œ, ë³¸ ë…¼ë¬¸ ëª¨ë¸ì˜ encoder-decoder Transformerì˜ êµ¬í˜„ì€ ì›ë˜ Transformerì˜ í˜•íƒœë¥¼ ë°€ì ‘í•˜ê²Œ ë”°ë¥¸ë‹¤.               

**[Transformerì˜ ë™ì‘ ê³¼ì •]**       
* **1)** í† í°ì˜ input sequenceë¥¼ embeddings sequenceë¥¼ì— ë§¤í•‘í•œ ë‹¤ìŒ encoderë¡œ ì „ë‹¬í•œë‹¤.      
* **2)** encoderëŠ”  â€œblocksâ€ì˜ ìŠ¤íƒìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê° ìŠ¤íƒë§ˆë‹¤ self-attention layer ê³„ì¸µê³¼ small feed-forward networkë¥¼ ê°–ê³ ìˆë‹¤.     
* **3)** ê° ìŠ¤íƒ ì•ˆì˜ 2ê°€ì§€ ìš”ì†Œë“¤ì˜ ì…ë ¥ì—ëŠ” Layer normalizationê°€ ì ìš©ëœë‹¤.     
* **4)** ì´í›„, activationsì´ ì¬ì¡°ì •ë˜ê³  ì¶”ê°€ biasì´ ì ìš©ë˜ì§€ ì•ŠëŠ” ë‹¨ìˆœí™”ëœ ë²„ì „ì˜ Layer normalizationë¥¼ ì‚¬ìš©í•œë‹¤.      
* **5)** layer normalization í›„, residual skip connectionì€ ê° ìŠ¤íƒ ì•ˆì˜ 2ê°€ì§€ ìš”ì†Œì˜ ì…ë ¥ì„ ì¶œë ¥ì— ì¶”ê°€í•œë‹¤.      
* **6)** ë“œë¡­ì•„ì›ƒì€ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë‚´ì—ì„œ ìŠ¤í‚µ ì—°ê²°, ì£¼ì˜ ê°€ì¤‘ì¹˜ ë° ì „ì²´ ìŠ¤íƒì˜ ì…ë ¥ ë° ì¶œë ¥ì— ì ìš©ëœë‹¤.   
* **7)** decoderëŠ” encoderì˜ ì¶œë ¥ì— ì°¸ì—¬í•˜ëŠ” ê° self-attention layer ë‹¤ìŒì— standard attention ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•œë‹¤ëŠ” ì ì„ ì œì™¸í•˜ê³ ëŠ” ì¸ì½”ë”ì™€ êµ¬ì¡°ê°€ ìœ ì‚¬í•˜ë‹¤.       
decoderì˜ self-attention ë©”ì»¤ë‹ˆì¦˜ì€ ëª¨ë¸ì´ ê³¼ê±° ì¶œë ¥ì—ë§Œ ì£¼ì˜ë¥¼ ê¸°ìš¸ì¼ ìˆ˜ ìˆë„ë¡ í•œë‹¤.     
* **8)** ìµœì¢… decoder ë¸”ë¡ì˜ ì¶œë ¥ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥ì„ ê°€ì§„  dense ë ˆì´ì–´ë¡œ ë“¤ì–´ê°€ë©°, ê·¸ ê°€ì¤‘ì¹˜ëŠ” ì…ë ¥ ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ì™€ ê³µìœ ëœë‹¤.      
* âœ¨ ì—¬ê¸°ì„œ, íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ëª¨ë“  attention ë©”ì»¤ë‹ˆì¦˜ì€ ì¶”ê°€ë¡œ ì²˜ë¦¬ë˜ê¸° ì „ì— ì¶œë ¥ì´ ì—°ê²°ë˜ëŠ” ë…ë¦½ì ì¸ â€œheadsâ€ë¡œ ë‚˜ë‰˜ì–´ìˆë‹¤.          
![image](https://user-images.githubusercontent.com/76824611/132572962-94a60e8b-2182-466a-8d1d-47a86ee83a14.gif)
</div>
</details> 
  


### relative position embeddings
Transformerì˜ self-attentionëŠ” ë³‘ë ¬ì²˜ë¦¬ë¥¼í•˜ì—¬ ìˆœì„œ ì •ë³´ë¥¼ ê°–ì§€ ëª»í•˜ë¯€ë¡œ, ì„ë² ë”©ì— ìˆœì„œì •ë³´ë¥¼ ë„£ì–´ì¤€ë‹¤.    
ì´ ë…¼ë¬¸ì—ì„œëŠ” ê¸°ì¡´ Transformerì™€ ë‹¤ë¥¸ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.     
* **ê¸°ì¡´ ëª¨ë¸**: sinusoidal position signal or learned position embeddings (ë‹¨ì–´ì˜ ì ˆëŒ€ì  ìœ„ì¹˜ ì •ë³´ í‘œí˜„)           
* **T5**: [relative position embeddings](https://yerimoh.github.io/LAN18/) (ë‹¨ì–´ì˜ ìƒëŒ€ì  ìœ„ì¹˜ í‘œí˜„)     


      



<details>
<summary>ğŸ“œ relative position embeddingsë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´?(ë³¸ ë…¼ë¬¸ì˜ ì„¤ëª…) </summary>
<div markdown="1">

ê° ìœ„ì¹˜ì— ëŒ€í•´ ê³ ì • ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , relative position embeddingsì€ self-attentionì—ì„œ ë¹„êµë˜ëŠ” "key"ì™€ "query" ì‚¬ì´ì˜ ì˜¤í”„ì…‹ì— ë”°ë¼ ë‹¤ë¥¸ í•™ìŠµëœ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤.    

ë˜í•œ íš¨ìœ¨ì„±ì„ ìœ„í•´ **ëª¨ë¸ì˜ ëª¨ë“  ë ˆì´ì–´**ì— ê±¸ì³ **position embeddings parametersë¥¼ ê³µìœ **í•˜ì§€ë§Œ,     
**ì£¼ì–´ì§„ layer ë‚´**ì—ì„œ **ê° attention head**ëŠ” **ì„œë¡œ ë‹¤ë¥¸ í•™ìŠµëœ position embeddingì„ ì‚¬ìš©**í•œë‹¤.      

ì¼ë°˜ì ìœ¼ë¡œ, ê°ê° ê°€ëŠ¥í•œ "key"ì™€ "query" ì˜¤í”„ì…‹ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” **ê³ ì •ëœ ìˆ˜ì˜ embeddings**ì´ í•™ìŠµëœë‹¤.      

ì´ ì—°êµ¬ì—ì„œ, ìš°ë¦¬ëŠ” ë¡œê·¸ì ìœ¼ë¡œ **ìµœëŒ€ 128ì˜ ì˜¤í”„ì…‹ê¹Œì§€ í¬ê¸°ê°€ ì¦ê°€**í•˜ëŠ” ë²”ìœ„ë¥¼ ê°€ì§„ **ëª¨ë“  ëª¨ë¸**ì—    
**32ê°œì˜ embeddingì„ ì‚¬ìš©**í•˜ì—¬ ëª¨ë“  relative positionë¥¼ ë™ì¼í•œ ì„ë² ë”©ì— í• ë‹¹í•œë‹¤.      

íŠ¹ì • ê³„ì¸µì€ 128ê°œ í† í°ì„ ì´ˆê³¼í•˜ëŠ” relative positionì— ë¯¼ê°í•˜ì§€ ì•Šì§€ë§Œ,     
subsequent ê³„ì¸µì€ ì´ì „ ê³„ì¸µì˜ ë¡œì»¬ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ë” í° ì˜¤í”„ì…‹ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•  ìˆ˜ ìˆë‹¤.

</div>
</details> 
  
**[summary: T5ì™€ ê¸°ì¡´ Transformerì˜ ì°¨ì´ì ]**       
ì•„ë˜ë¥¼ ì œì™¸í•˜ê³  ê¸°ì¡´ Transformerì™€ ë™ì¼       
* T5ëŠ” Layer Norm biasë¥¼ ì œê±°í•¨    
* Layer normalizationë¥¼ residual path ì™¸ë¶€ì— ë°°ì¹˜     
* ë‹¤ë¥¸ position embedding ë°©ì‹ ì‚¬ìš©(relative position embeddings)    

ì´ëŸ¬í•œ ì•„í‚¤í…ì²˜ ë³€í™”ëŠ” transfer learningì— ëŒ€í•œ ê²½í—˜ì  ì¡°ì‚¬ì—ì„œ ê³ ë ¤í•˜ëŠ” ì‹¤í—˜ ìš”ì†Œì™€ ì§êµí•˜ê¸° ë•Œë¬¸ì—,  
ìš°ë¦¬ëŠ” í–¥í›„ ì‘ì—…ì„ ìœ„í•´ ì˜í–¥ì˜ ì ˆì œë¥¼ ë‚¨ê²¨ë‘ ë‘ .  
  
### ì‹¤í—˜  
* T5ì˜ í™•ì¥ì„±(scalability)ì‹¤í—˜    
â¡ ì¦‰ ë” ë§ì€ parametersë‚˜ layersì„ ê°€ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‹¤í—˜í•œë‹¤.      
* ê²°ê³¼ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ëª¨ë¸ê³¼ ë°ì´í„° ë³‘ë ¬í™”ë¥¼ ê²°í•©í•˜ì—¬ â€œslicesâ€ of Cloud TPU Pods"ì—ì„œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚´.       



----

##  2.2 The Colossal Clean Crawled Corpus
Much of the previous work on transfer learning for NLP makes use of large unlabeled data
sets for unsupervised learning. In this paper, we are interested in measuring the effect of the
quality, characteristics, and size of this unlabeled data. To generate data sets that satisfy
our needs, we leverage Common Crawl as a source of text scraped from the web. Common
Crawl has previously been used as a source of text data for NLP, for example to train an
n-gram language model (Buck et al., 2014), as training data for commonsense reasoning
(Trinh and Le, 2018), for mining parallel texts for machine translation (Smith et al., 2013),
as a pre-training data set (Grave et al., 2018; Zellers et al., 2019; Liu et al., 2019c), and
even simply as a giant text corpus for testing optimizers (Anil et al., 2019).



Common Crawl is a publicly-available web archive that provides â€œweb extracted textâ€
by removing markup and other non-text content from the scraped HTML files. This process
produces around 20TB of scraped text data each month. Unfortunately, the majority of the
resulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate
text like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped
text contains content that is unlikely to be helpful for any of the tasks we consider (offensive
language, placeholder text, source code, etc.). To address these issues, we used the following
heuristics for cleaning up Common Crawlâ€™s web extracted text:

â€¢ We only retained lines that ended in a terminal punctuation mark (i.e. a period,
exclamation mark, question mark, or end quotation mark).

â€¢ We discarded any page with fewer than 5 sentences and only retained lines that
contained at least 3 words.

â€¢ We removed any page that contained any word on the â€œList of Dirty, Naughty, Obscene
or Otherwise Bad Wordsâ€.6

â€¢ Many of the scraped pages contained warnings stating that Javascript should be
enabled so we removed any line with the word Javascript.

â€¢ Some pages had placeholder â€œlorem ipsumâ€ text; we removed any page where the
phrase â€œlorem ipsumâ€ appeared.

â€¢ Some pages inadvertently contained code. Since the curly bracket â€œ{â€ appears in
many programming languages (such as Javascript, widely used on the web) but not in
natural text, we removed any pages that contained a curly bracket.

â€¢ To deduplicate the data set, we discarded all but one of any three-sentence span
occurring more than once in the data set.


Additionally, since most of our downstream tasks are focused on English-language text,
we used langdetect7
to filter out any pages that were not classified as English with a
probability of at least 0.99. Our heuristics are inspired by past work on using Common
Crawl as a source of data for NLP: For example, Grave et al. (2018) also filter text using an
automatic language detector and discard short lines and Smith et al. (2013); Grave et al.
(2018) both perform line-level deduplication. However, we opted to create a new data set
because prior data sets use a more limited set of filtering heuristics, are not publicly available,
and/or are different in scope (e.g. are limited to News data (Zellers et al., 2019; Liu et al.,
2019c), comprise only Creative Commons content (Habernal et al., 2016), or are focused on
parallel training data for machine translation (Smith et al., 2013)).

To assemble our base data set, we downloaded the web extracted text from April 2019
and applied the aforementioned filtering. This produces a collection of text that is not only
orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also
comprises reasonably clean and natural English text. We dub this data set the â€œColossal
Clean Crawled Corpusâ€ (or C4 for short) and release it as part of TensorFlow Datasets.8
We consider the impact of using various alternative versions of this data set in Section 3.4.

