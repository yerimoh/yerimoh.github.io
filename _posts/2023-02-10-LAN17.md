---
title: "(T5) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ì •ë¦¬"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: #[Paper]
layout: post
tag:
#- Paper
order: 0

comments: true
---


# Abstract
<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ ë°°ê²½]**</span>    
ëª¨ë¸ì´ Downstream Taskì—ì„œ fine-tuningë˜ê¸° ì „ì— data-rich taskì—ì„œ pre-trainì„ í•˜ëŠ” ê²ƒì€  ìì—°ì–´ ì²˜ë¦¬(NLP)ì˜ ê°•ë ¥í•œ ê¸°ìˆ ë¡œ ë¶€ìƒí–ˆë‹¤.     
ì¦‰ ì´ëŸ¬í•œ [transfer learning](https://yerimoh.github.io/DL12/)ì€ NLPì— í° ë°œì „ì„ ê°€ì ¸ì™”ë‹¤.    

<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ì˜ ëª©ì ]**</span>    
* ëª¨ë“  í…ìŠ¤íŠ¸ ê¸°ë°˜ ì–¸ì–´ ë¬¸ì œë¥¼ text-to-text í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ <span style="background-color:#fff5b1">NLPì— ëŒ€í•œ [transfer learning](https://yerimoh.github.io/DL12/)ë¥¼ ì „ë°˜ì ìœ¼ë¡œ íƒêµ¬</span>í•œë‹¤.     
* ìœ„ íƒêµ¬ë¥¼ í†µí•´ ì–»ì€ í†µì°°ë ¥ê³¼ ê·œëª¨ ë° ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•  <span style="background-color:#fff5b1">"Colossal Clean Crawled Corpus"ë¥¼ ê²°í•©</span>í•˜ì—¬ ë§ì€ NLP evaluationì—ì„œ SOTA ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.        
â¡ ê³µê°œí•œ [data set, pre-trained models, and code](https://github.com/google-research/text-to-text-transfer-transformer) ë§í¬ë¥¼ ì²¨ë¶€í•˜ì˜€ë‹¤.        


<details>
<summary>ğŸ“œ Downstream Task ì˜ë¯¸ ë³´ê¸°</summary>
<div markdown="1">
  

êµ¬ì²´ì ìœ¼ë¡œ í’€ê³  ì‹¶ì€ ë¬¸ì œë“¤ì„ ë§í•œë‹¤.

NLPì—ì„œëŠ” ì–¸ì–´ëª¨ë¸ì„ pre-trainë°©ì‹ì„ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ê³ ,    
ê·¸ í›„ì— ì›í•˜ê³ ì í•˜ëŠ” taskë¥¼ fine-tuningí•˜ëŠ” ë°©ì‹ì„ í†µí•´ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ë•Œ, taskë¥¼ Downstream Taskë¼ í•œë‹¤.

ì˜ˆë¥¼ë“¤ì–´, BERTì˜ ì–¸ì–´ëª¨ë¸ì„ ì§ˆì˜ì‘ë‹µ Taskë¼ì¸ squadë¥¼ í•™ìŠµí•œë‹¤ê³  í• ë•Œ, ì´ë•Œ ì§ˆì˜ì‘ë‹µ Taskë¥¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ Taskë¡œ ë³¼ ìˆ˜ ìˆì„ê²ƒì´ë‹¤.  
  
  
</div>
</details>  

----
----



# Introduction

<span style="background-color:#F5F5F5">**[ìµœê·¼ ì—°êµ¬ ë™í–¥]**</span>    
ìì—°ì–´ ì²˜ë¦¬(NLP)ì‘ì„ ì„ ìœ„í•œ trainì´ ê°€ëŠ¥í•˜ì—¬ë©´ ëª¨ë¸ì´ **downstream learningì— ì í•©í•œ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬**í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.              
ê·¸ëŸ°ë° ì´ëŠ” **ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ "ì´í•´"í•  ìˆ˜ ìˆê²Œ í•™ìŠµí•œë‹¤ê³  ë³´ê¸°ì—” í˜ë“¤ë‹¤**.               
â¡ í˜„ëŒ€ ê¸°ê³„ í•™ìŠµ ê´€í–‰ì—ì„œ ì´ëŸ¬í•œ trainì´ ëª…ì‹œì ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” ê²½ìš°ê°€ ê±°ì˜ ì—†ì–´, ëŒ€ì‹  **Auxiliary Taskì˜ ì¼ë¶€**ë¡œ í•™ìŠµë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.        


<details>
<summary>ğŸ“œ Auxiliary Task ì˜ë¯¸ ë³´ê¸°</summary>
<div markdown="1">

ë³¸ taskëŠ” ì•„ë‹ˆì§€ë§Œ, ë³¸ taskì—ì„œì˜ ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë³´ì¡° task  

</div>
</details> 
  
ìµœê·¼ì—ëŠ” **ë°ì´í„°ê°€ í’ë¶€í•œ ì‘ì—…**ì— ëŒ€í•´ **ì „ì²´ ëª¨ë¸ì„ pre-trainí•˜ëŠ” ê²ƒ**ì´ ì ì  ë” **ì¼ë°˜í™”**ë˜ê³  ìˆë‹¤.     
ì´ìƒì ìœ¼ë¡œ, ì´ pre-trainì€ ëª¨ë¸ì´ ë²”ìš© ëŠ¥ë ¥ê³¼ ì§€ì‹ì„ ê°œë°œí•˜ê²Œ í•˜ê³ , ì´ë¥¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ìœ¼ë¡œ ì´ì „í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.     
â¡ ë” í° ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë” í° ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒë§Œìœ¼ë¡œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.    




<span style="background-color:#F5F5F5">**[í˜„ ê²½í–¥ìœ¼ë¡œ ì¸í•œ í•œê³„]**</span>    
ì´ëŸ¬í•œ ê²½í–¥ìœ¼ë¡œ ì¸í•´ NLPì— ëŒ€í•œ **transfer learning ë°©ë²•ë¡ ì„ ê°œë°œ**í•˜ëŠ” ì—°êµ¬ë“¤ì´ í™œë°œí•˜ê²Œ ì´ë£¨ì–´ì¡Œë‹¤.      
ì´ë ‡ê²Œ ì´ ë¶„ì•¼ê°€ ê¸‰ì„±ì¥í•˜ì—¬ ì—°êµ¬ê°€ í™œë°œí•´ì§€ë‹ˆ ì•„ë˜ì™€ ê°™ì€ ì‘ì—…ë“¤ì´ ì–´ë ¤ì›Œì¡Œë‹¤.       
* ì—¬ëŸ¬ algorithmsì„ ë¹„êµ    
* ìƒˆë¡œìš´ contributionsì˜ íš¨ê³¼ íŒŒì•…    
* transfer learningì„ ìœ„í•œ ê¸°ì¡´ ë°©ë²•ì˜ space ì´í•´        


âœ” ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì€ ì´ ë¶„ì•¼ì˜ ì›í™œí•œ ì´í•´ë¥¼ ìœ„í•´, **ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ì„ ì²´ê³„ì ìœ¼ë¡œ ì—°êµ¬**í•˜ê³  **í•„ë“œì˜ í˜„ì¬ í•œê³„ë¥¼ ë°€ì–´ë‚¼ ìˆ˜ ìˆëŠ” transfer learningì— ëŒ€í•œ í†µí•©ëœ ì ‘ê·¼ë²•ì„ í™œìš©**í•œë‹¤. 
 


<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ í•´ê²°ì±…]**</span>      
ë³¸ ë…¼ë¬¸ workì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” <span style="background-color:#fff5b1">ëª¨ë“  í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¬¸ì œë¥¼ **â€œtext-to-textâ€ ë¬¸ì œë¡œ ì²˜ë¦¬**</span>í•˜ëŠ” ê²ƒì´ë‹¤.             
ì¦‰, **í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ë“¤ì´ê³  ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥ìœ¼ë¡œ ìƒì„±**í•˜ëŠ” ê²ƒì´ë‹¤.          
* ì´ëŸ¬í•œ í…ìŠ¤íŠ¸ ê°„ í”„ë ˆì„ì›Œí¬ëŠ” ìš°ë¦¬ê°€ ê³ ë ¤í•˜ëŠ” ëª¨ë“  ì‘ì—…ì— **ë™ì¼í•œ ëª¨ë¸, ëª©í‘œ, í›ˆë ¨ ì ˆì°¨ ë° decoding í”„ë¡œì„¸ìŠ¤ë¥¼ ì§ì ‘ ì ìš©**í•  ìˆ˜ ìˆê²Œ í•œë‹¤.          
* ë³¸ ë…¼ë¬¸ì€ ì§ˆë¬¸ ë‹µë³€, ë¬¸ì„œ ìš”ì•½ ë° ê°ì • ë¶„ë¥˜ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ì˜ì–´ ê¸°ë°˜ NLP ë¬¸ì œì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€í•˜ì—¬ ì´ëŸ¬í•œ **ìœ ì—°ì„±ì„ í™œìš©**í•œë‹¤.        
* ì´ í†µí•© ì ‘ê·¼ë²•ì„ í†µí•´, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ transfer learningì˜ target, ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°ì´í„° ì„¸íŠ¸ ë° ê¸°íƒ€ ìš”ì¸ì˜ íš¨ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ë™ì‹œì— ì´ì „ì— ê³ ë ¤ë˜ì—ˆë˜ ê²ƒ ì´ìƒìœ¼ë¡œ **ëª¨ë¸ê³¼ ë°ì´í„° ì„¸íŠ¸ë¥¼ í™•ì¥**í•˜ì—¬ NLPì— ëŒ€í•œ **transfer learningì˜ í•œê³„ë¥¼ íƒìƒ‰**í•  ìˆ˜ ìˆë‹¤.    




ë³¸ ë…¼ë¬¸ì€ ë³¸ ë…¼ë¬¸ì˜ ëª©í‘œê°€ ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ê·¸ ë¶„ì•¼ê°€ ì–´ë””ì— ì„œ ìˆëŠ”ì§€ì— ëŒ€í•œ í¬ê´„ì ì¸ ê´€ì ì„ ì œê³µí•˜ëŠ” ê²ƒ**ì„ì„ ê°•ì¡°í•œë‹¤.      
ì¦‰, ìš°ë¦¬ì˜ ì‘ì—…ì€ ë³¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ taskë¡œ êµ¬ì„±ëœë‹¤.         
â¡ ì£¼ë¡œ ê¸°ì¡´ ê¸°ìˆ ì˜ ì¡°ì‚¬, íƒêµ¬ ë° ê²½í—˜ì  ë¹„êµ       


ë˜í•œ ë³¸ ë…¼ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ **í˜„ì¬ ì ‘ê·¼ ë°©ì‹ì˜ í•œê³„ë¥¼ íƒêµ¬**í•œë‹¤.        
ë³¸ ë…¼ë¬¸ì´ ê³ ë ¤í•˜ëŠ” ë§ì€ taskì—ì„œ SOTA ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì²´ê³„ì ì¸ ì—°êµ¬(train ëª¨ë¸ì„ ìµœëŒ€ 110ì–µ ê°œ parametersê¹Œì§€ í™•ì¥)ìˆ˜í–‰       
* ì´ ê·œëª¨ì˜ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì›¹ì—ì„œ ê¸ì–´ë‚¸ ìˆ˜ë°± ê¸°ê°€ë°”ì´íŠ¸ì˜ clean ì˜ì–´ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ë°ì´í„° ì„¸íŠ¸ì¸ "Colossal Clean Crawled Corpus"**(C4)** ë¥¼ ì†Œê°œí•œë‹¤.        
* **transfer learningì˜ í•µì‹¬ ê¸°ëŠ¥**ì€, **ë°ì´í„°ê°€ ë¶€ì¡±í•œ í™˜ê²½ì—ì„œ  pre-trained modelsì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±**ì´ë¼ëŠ” ê²ƒì„ ì¸ì‹í•˜ì—¬,      
[ì½”ë“œ, ë°ì´í„° ì„¸íŠ¸ ë° ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸](https://github.com/google-research/text-to-text-transfer-transformer
)ì„ ë¦´ë¦¬ìŠ¤í–ˆë‹¤.    




<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ êµ¬ì„±]**</span>            
* base modelê³¼ ê·¸ êµ¬í˜„    
* ëª¨ë“  text processing ë¬¸ì œë¥¼ text-to-text ì‘ì—…ìœ¼ë¡œ ê³µì‹í™”í•˜ëŠ” ì ˆì°¨ ë° ê³ ë ¤í•˜ëŠ” ì‘ì—… ëª¨ìŒì— ëŒ€í•œ ë…¼ì˜    
* ì„¹ì…˜ 3ì—ì„œ, NLPì— ëŒ€í•œ transfer learning ë¶„ì•¼ë¥¼ íƒêµ¬í•˜ëŠ” ëŒ€ê·œëª¨ ì‹¤í—˜ ì„¸íŠ¸ë¥¼ ì œì‹œ          
* ì„¹ì…˜(ì„¹ì…˜ 3.7)ì˜ ëì—ì„œ, ìš°ë¦¬ëŠ” ì²´ê³„ì ì¸ ì—°êµ¬ì˜ í†µì°°ë ¥ì„ ê²°í•©í•˜ì—¬ ê´‘ë²”ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ì–»ìŒ    
* ì„¹ì…˜ 4ì—ì„œ, ê²°ê³¼ì— ëŒ€í•œ ìš”ì•½ì„ ì œê³µ í•œ ë’¤, ë¯¸ë˜ì— ëŒ€í•œ ì „ë§ìœ¼ë¡œ ë§ˆë¬´ë¦¬             




---
---


# 2. Setup    



ë³¸ ë…¼ë¬¸ì€ large-scale ê²½í—˜ì  ì—°êµ¬ì˜ ê²°ê³¼ë¥¼ ì œì‹œí•˜ê¸° ì „ì— ì•„ë˜ì™€ ê°™ì€ ê²ƒë“¤ì„ ë¨¼ì € ì œì‹œí•œë‹¤,   
* [Transformer ëª¨ë¸ ì•„í‚¤í…ì²˜](https://yerimoh.github.io/Lan/)ì™€ í‰ê°€í•˜ëŠ” downstream tasksì„ í¬í•¨í•˜ì—¬ ê²°ê³¼ë¥¼ ì´í•´í•˜ëŠ” ë° í•„ìš”í•œ **ë°°ê²½ ì£¼ì œë¥¼ ê²€í† **í•œë‹¤.        
* **ëª¨ë“  ë¬¸ì œë¥¼ text-to-text taskìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì ‘ê·¼ ë°©ì‹**ì„ ì†Œê°œ    
* **"Colossal Clean Crawled Corpus"(C4) ì„¤ëª…:** ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì†ŒìŠ¤ë¡œ ìƒì„±í•œ ê³µí†µ í¬ë¡¤ ê¸°ë°˜ ë°ì´í„° ì„¸íŠ¸ì„     




ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ëª¨ë¸ê³¼ í”„ë ˆì„ì›Œí¬ë¥¼  <span style="background-color:#fff5b1">â€œText-to-Text Transfer Transformerâ€ (T5)</span> ë¼ê³  ë¶€ë¥¸ë‹¤.      





---


## 2.1 Model

### base architecture: â€œTransformerâ€
NLPì— ëŒ€í•œ ì „ì´ í•™ìŠµì— ëŒ€í•œ ì´ˆê¸° ê²°ê³¼ëŠ” ë°˜ë³µ ì‹ ê²½ë§ì„ í™œìš©í–ˆì§€ë§Œ,  
ìµœê·¼ì—ëŠ” "Transformer" ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¼ë°˜í™”ë˜ì—ˆë‹¤.     


TransformerëŠ” transfer learningì— íš¨ê³¼ì ì´ë¯€ë¡œ, ì´í›„ ë‹¤ì–‘í•œ NLP ì„¤ì •ì— ì‚¬ìš©ë˜ì—ˆë‹¤.          
ë³¸ ë…¼ë¬¸ì—ì„œë„ **ëª¨ë“  ëª¨ë¸ì˜ baseë¥¼ Transformer ì•„í‚¤í…ì²˜**ë¡œ í•˜ì˜€ë‹¤.         

ì•„ë˜ì— ì–¸ê¸‰ëœ ì„¸ë¶€ì‚¬í•­ê³¼ ì„¹ì…˜ 3.2ì—ì„œ íƒêµ¬í•œ ë³€í˜•ì„ ì œì™¸í•˜ê³ ,     
ë³¸ ë…¼ë¬¸ì€ Transformer ì•„í‚¤í…ì²˜ì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.      


<details>
<summary>ğŸ“œ Transformerë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´? (ì°¸ê³ ìë£Œ) </summary>
<div markdown="1">

ì´ ë…¼ë¬¸(ì•„í‚¤í…ì²˜)ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³ ì‹¶ë‹¤ë©´ ì•„ë˜ ìë£Œë“¤ì„ ì°¸ê³ í•˜ì,    
* [ì›ë³¸ ë…¼ë¬¸](https://arxiv.org/abs/1706.03762)    
* í›„ì† íŠœí† ë¦¬ì–¼ 3,4(ë’¤ì— ë‚˜ì˜µë‹ˆë‹¤.)    
* [ë³¸ í¬ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•œ í•„ìê°€ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸](https://yerimoh.github.io/Lan/)         

</div>
</details> 
  85.5


ê·¸ë¦¬ê³  ë³¸ ë…¼ë¬¸ì—ì„œ Transformerì—ëŒ€í•´ ê°„ë‹¨ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ì¤¬ëŠ”ë° ì›í•œë‹¤ë©´ ì•„ë˜ ìì„¸í•œ ì„¤ëª… ë³´ê¸°ë¥¼ ëˆŒëŸ¬ í•œ ë²ˆ ì½ì–´ë³´ê¸¸ ë°”ë€ë‹¤.(ê·¸ë ‡ì§€ë§Œ ì› ë…¼ë¬¸ì„ ì½ì–´ë´ì•¼ ì•„ë˜ ë‚´ìš©ë“¤ë„ ì´í•´ê°€ ê°ˆ ê²ƒì´ë‹¤.)   

<details>
<summary>ğŸ“œ Transformerë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´?(ë³¸ ë…¼ë¬¸ì˜ ì„¤ëª…) </summary>
<div markdown="1">




 
**Transformerì˜ ì¤‘ìš”í•œ ìš”ì†ŒëŠ”,** [self-attention](https://yerimoh.github.io/Lan/#sub-layer1-self-attention)ì´ë‹¤.         
self-attentionì€ ê° ìš”ì†Œë¥¼ ë‚˜ë¨¸ì§€ sequenceì˜ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” attentionì˜ ë³€í˜•ì´ë‹¤.    

**Transformerì˜ êµ¬ì¡°ëŠ”,** [encoder-decoder ì•„í‚¤í…ì²˜](https://yerimoh.github.io/Lan/#transformer-%EB%AA%A8%EB%8D%B8-%EA%B0%9C%EC%9A%94)ë¡œ êµ¬ì„±ë˜ì—ˆìœ¼ë©° sequence-to-sequence ì‘ì—…ì„ ìœ„í•´ ê³ ì•ˆë˜ì—ˆë‹¤.        

ì „ë°˜ì ìœ¼ë¡œ, ë³¸ ë…¼ë¬¸ ëª¨ë¸ì˜ encoder-decoder Transformerì˜ êµ¬í˜„ì€ ì›ë˜ Transformerì˜ í˜•íƒœë¥¼ ë°€ì ‘í•˜ê²Œ ë”°ë¥¸ë‹¤.               

**[Transformerì˜ ë™ì‘ ê³¼ì •]**       
* **1)** í† í°ì˜ input sequenceë¥¼ embeddings sequenceë¥¼ì— ë§¤í•‘í•œ ë‹¤ìŒ encoderë¡œ ì „ë‹¬í•œë‹¤.      
* **2)** encoderëŠ”  â€œblocksâ€ì˜ ìŠ¤íƒìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê° ìŠ¤íƒë§ˆë‹¤ self-attention layer ê³„ì¸µê³¼ small feed-forward networkë¥¼ ê°–ê³ ìˆë‹¤.     
* **3)** ê° ìŠ¤íƒ ì•ˆì˜ 2ê°€ì§€ ìš”ì†Œë“¤ì˜ ì…ë ¥ì—ëŠ” Layer normalizationê°€ ì ìš©ëœë‹¤.     
* **4)** ì´í›„, activationsì´ ì¬ì¡°ì •ë˜ê³  ì¶”ê°€ biasì´ ì ìš©ë˜ì§€ ì•ŠëŠ” ë‹¨ìˆœí™”ëœ ë²„ì „ì˜ Layer normalizationë¥¼ ì‚¬ìš©í•œë‹¤.      
* **5)** layer normalization í›„, residual skip connectionì€ ê° ìŠ¤íƒ ì•ˆì˜ 2ê°€ì§€ ìš”ì†Œì˜ ì…ë ¥ì„ ì¶œë ¥ì— ì¶”ê°€í•œë‹¤.      
* **6)** ë“œë¡­ì•„ì›ƒì€ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë‚´ì—ì„œ ìŠ¤í‚µ ì—°ê²°, ì£¼ì˜ ê°€ì¤‘ì¹˜ ë° ì „ì²´ ìŠ¤íƒì˜ ì…ë ¥ ë° ì¶œë ¥ì— ì ìš©ëœë‹¤.   
* **7)** decoderëŠ” encoderì˜ ì¶œë ¥ì— ì°¸ì—¬í•˜ëŠ” ê° self-attention layer ë‹¤ìŒì— standard attention ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•œë‹¤ëŠ” ì ì„ ì œì™¸í•˜ê³ ëŠ” ì¸ì½”ë”ì™€ êµ¬ì¡°ê°€ ìœ ì‚¬í•˜ë‹¤.       
decoderì˜ self-attention ë©”ì»¤ë‹ˆì¦˜ì€ ëª¨ë¸ì´ ê³¼ê±° ì¶œë ¥ì—ë§Œ ì£¼ì˜ë¥¼ ê¸°ìš¸ì¼ ìˆ˜ ìˆë„ë¡ í•œë‹¤.     
* **8)** ìµœì¢… decoder ë¸”ë¡ì˜ ì¶œë ¥ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥ì„ ê°€ì§„  dense ë ˆì´ì–´ë¡œ ë“¤ì–´ê°€ë©°, ê·¸ ê°€ì¤‘ì¹˜ëŠ” ì…ë ¥ ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ì™€ ê³µìœ ëœë‹¤.      
* âœ¨ ì—¬ê¸°ì„œ, íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ëª¨ë“  attention ë©”ì»¤ë‹ˆì¦˜ì€ ì¶”ê°€ë¡œ ì²˜ë¦¬ë˜ê¸° ì „ì— ì¶œë ¥ì´ ì—°ê²°ë˜ëŠ” ë…ë¦½ì ì¸ â€œheadsâ€ë¡œ ë‚˜ë‰˜ì–´ìˆë‹¤.          
![image](https://user-images.githubusercontent.com/76824611/132572962-94a60e8b-2182-466a-8d1d-47a86ee83a14.gif)
</div>
</details> 
  


### relative position embeddings
Transformerì˜ self-attentionëŠ” ë³‘ë ¬ì²˜ë¦¬ë¥¼í•˜ì—¬ ìˆœì„œ ì •ë³´ë¥¼ ê°–ì§€ ëª»í•˜ë¯€ë¡œ, ì„ë² ë”©ì— ìˆœì„œì •ë³´ë¥¼ ë„£ì–´ì¤€ë‹¤.    
ì´ ë…¼ë¬¸ì—ì„œëŠ” ê¸°ì¡´ Transformerì™€ ë‹¤ë¥¸ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.     
* **ê¸°ì¡´ ëª¨ë¸**: sinusoidal position signal or learned position embeddings (ë‹¨ì–´ì˜ ì ˆëŒ€ì  ìœ„ì¹˜ ì •ë³´ í‘œí˜„)           
* **T5**: [relative position embeddings](https://yerimoh.github.io/LAN18/) (ë‹¨ì–´ì˜ ìƒëŒ€ì  ìœ„ì¹˜ í‘œí˜„)     


      



<details>
<summary>ğŸ“œ relative position embeddingsë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´?(ë³¸ ë…¼ë¬¸ì˜ ì„¤ëª…) </summary>
<div markdown="1">

ê° ìœ„ì¹˜ì— ëŒ€í•´ ê³ ì • ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , relative position embeddingsì€ self-attentionì—ì„œ ë¹„êµë˜ëŠ” "key"ì™€ "query" ì‚¬ì´ì˜ ì˜¤í”„ì…‹ì— ë”°ë¼ ë‹¤ë¥¸ í•™ìŠµëœ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤.    

ë˜í•œ íš¨ìœ¨ì„±ì„ ìœ„í•´ **ëª¨ë¸ì˜ ëª¨ë“  ë ˆì´ì–´**ì— ê±¸ì³ **position embeddings parametersë¥¼ ê³µìœ **í•˜ì§€ë§Œ,     
**ì£¼ì–´ì§„ layer ë‚´**ì—ì„œ **ê° attention head**ëŠ” **ì„œë¡œ ë‹¤ë¥¸ í•™ìŠµëœ position embeddingì„ ì‚¬ìš©**í•œë‹¤.      

ì¼ë°˜ì ìœ¼ë¡œ, ê°ê° ê°€ëŠ¥í•œ "key"ì™€ "query" ì˜¤í”„ì…‹ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” **ê³ ì •ëœ ìˆ˜ì˜ embeddings**ì´ í•™ìŠµëœë‹¤.      

ì´ ì—°êµ¬ì—ì„œ, ìš°ë¦¬ëŠ” ë¡œê·¸ì ìœ¼ë¡œ **ìµœëŒ€ 128ì˜ ì˜¤í”„ì…‹ê¹Œì§€ í¬ê¸°ê°€ ì¦ê°€**í•˜ëŠ” ë²”ìœ„ë¥¼ ê°€ì§„ **ëª¨ë“  ëª¨ë¸**ì—    
**32ê°œì˜ embeddingì„ ì‚¬ìš©**í•˜ì—¬ ëª¨ë“  relative positionë¥¼ ë™ì¼í•œ ì„ë² ë”©ì— í• ë‹¹í•œë‹¤.      

íŠ¹ì • ê³„ì¸µì€ 128ê°œ í† í°ì„ ì´ˆê³¼í•˜ëŠ” relative positionì— ë¯¼ê°í•˜ì§€ ì•Šì§€ë§Œ,     
subsequent ê³„ì¸µì€ ì´ì „ ê³„ì¸µì˜ ë¡œì»¬ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ë” í° ì˜¤í”„ì…‹ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•  ìˆ˜ ìˆë‹¤.

</div>
</details> 
  
**[summary: T5ì™€ ê¸°ì¡´ Transformerì˜ ì°¨ì´ì ]**       
ì•„ë˜ë¥¼ ì œì™¸í•˜ê³  ê¸°ì¡´ Transformerì™€ ë™ì¼       
* T5ëŠ” Layer Norm biasë¥¼ ì œê±°í•¨    
* Layer normalizationë¥¼ residual path ì™¸ë¶€ì— ë°°ì¹˜     
* ë‹¤ë¥¸ position embedding ë°©ì‹ ì‚¬ìš©(relative position embeddings)    

ì´ëŸ¬í•œ ì•„í‚¤í…ì²˜ ë³€í™”ëŠ” transfer learningì— ëŒ€í•œ ê²½í—˜ì  ì¡°ì‚¬ì—ì„œ ê³ ë ¤í•˜ëŠ” ì‹¤í—˜ ìš”ì†Œì™€ ì§êµí•˜ê¸° ë•Œë¬¸ì—,  
ìš°ë¦¬ëŠ” í–¥í›„ ì‘ì—…ì„ ìœ„í•´ ì˜í–¥ì˜ ì ˆì œë¥¼ ë‚¨ê²¨ë‘ ë‘ .  
  
### ì‹¤í—˜  
* T5ì˜ í™•ì¥ì„±(scalability)ì‹¤í—˜    
â¡ ì¦‰ ë” ë§ì€ parametersë‚˜ layersì„ ê°€ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‹¤í—˜í•œë‹¤.      
* ê²°ê³¼ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ëª¨ë¸ê³¼ ë°ì´í„° ë³‘ë ¬í™”ë¥¼ ê²°í•©í•˜ì—¬ â€œslicesâ€ of Cloud TPU Pods"ì—ì„œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚´.       



----

##  2.2 The Colossal Clean Crawled Corpus
ë°ì´í„°ëŠ”  **large unlabeled** data sets for **unsupervised learning**ë¥¼ ì‚¬ìš©í•œë‹¤.    

í…ìŠ¤íŠ¸ ë°ì´í„° ì†ŒìŠ¤ëŠ” **Common Crawl**ì„ ì‚¬ìš©í•œë‹¤


<details>
<summary>ğŸ“œ Common Crawlì„ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´? </summary>
<div markdown="1">
  
**Common Crawlì´ë€**    
* ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ì›¹ ì•„ì¹´ì´ë¸Œì„     
* ìŠ¤í¬ë©ëœ HTML íŒŒì¼ì—ì„œ ë§ˆí¬ì—… ë° ê¸°íƒ€ ë¹„í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì œê±°í•˜ì—¬ "ì›¹ ì¶”ì¶œ í…ìŠ¤íŠ¸"ë¥¼ ì œê³µ      
* ì´ í”„ë¡œì„¸ìŠ¤ëŠ” ë§¤ë‹¬ ì•½ 20TBì˜ ìŠ¤í¬ë©ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìƒì„±.     


**preprocessing**     
* Common Crawlì˜ ë¬¸ì œ     
   * ë¶ˆí–‰í•˜ê²Œë„, Common Crawlì˜ ê²°ê³¼ í…ìŠ¤íŠ¸ì˜ ëŒ€ë¶€ë¶„ì€ ìì—°ì–´ê°€ ì•„ë‹ˆë‹¤.     
   * ì˜¤ë¥˜ ë©”ì‹œì§€ ë˜ëŠ” ì¤‘ë³µ í…ìŠ¤íŠ¸ì™€ ê°™ì€ íš¡ì„¤ìˆ˜ì„¤í•˜ê±°ë‚˜ boiler-plate í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë¨    
* í•´ê²°: ë‹¤ìŒ íœ´ë¦¬ìŠ¤í‹±ì„ ì‚¬ìš©     
   * ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë˜ëŠ” ë ë”°ì˜´í‘œë¡œ ëë‚˜ëŠ” í–‰ë§Œ ì¶”ì¶œ     
   * 5ê°œ ë¯¸ë§Œì˜ ë¬¸ì¥ì´ ìˆëŠ” í˜ì´ì§€ëŠ” íê¸°í•˜ê³  ìµœì†Œ 3ê°œì˜ ë‹¨ì–´ê°€ í¬í•¨ëœ í–‰ë§Œ ì¶”ì¶œ      
   * "ë”í‹°, ì¥ë‚œ, ì™¸ì„¤ ë˜ëŠ” ê¸°íƒ€ ë‚˜ìœ ë‹¨ì–´ ëª©ë¡"ì— ìˆëŠ” ëª¨ë“  ë‹¨ì–´ê°€ í¬í•¨ëœ í˜ì´ì§€ë¥¼ ì œê±°          
   * ëŒ€ë¶€ë¶„ì˜ ìŠ¤í¬ë© í˜ì´ì§€ì—ëŠ” Javascriptë¥¼ í™œì„±í™”í•´ì•¼ í•œë‹¤ëŠ” ê²½ê³ ê°€ í¬í•¨ë˜ì–´ ìˆì–´ Javascriptë¼ëŠ” ë‹¨ì–´ê°€ ìˆëŠ” ì¤„ì€ ëª¨ë‘ ì œê±°          
   * ì¼ë¶€ í˜ì´ì§€ì—ëŠ” í”Œë ˆì´ìŠ¤í™€ë” "lorem ipsum" í…ìŠ¤íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, "lorem ipsum"ì´ë¼ëŠ” ë¬¸êµ¬ê°€ ë‚˜íƒ€ë‚˜ëŠ” í˜ì´ì§€ëŠ” ëª¨ë‘ ì œê±°      
   *  "{"ê°€ í¬í•¨ëœ ëª¨ë“  í˜ì´ì§€ë¥¼ ì œê±°     
   *  ë°ì´í„° ì„¸íŠ¸ì˜ ì¤‘ë³µì„ ì œê±°í•˜ê¸° ìœ„í•´ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë‘ ë²ˆ ì´ìƒ ë°œìƒí•˜ëŠ” ì„¸ ë¬¸ì¥ ë²”ìœ„ ì¤‘ í•˜ë‚˜ë¥¼ ì œì™¸í•˜ê³  ëª¨ë‘ ì‚­ì œ     


</div>
</details> 

-----

## 2.3 Downstream Tasks
* Sentence acceptability judgment (CoLA (Warstadt et al., 2018))     
* Sentiment analysis (SST-2 (Socher et al., 2013))    
* Paraphrasing/sentence similarity (MRPC (Dolan and Brockett, 2005), STS-B (Ceret al., 2017), QQP (Iyer et al., 2017))    
* Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al.,2016), RTE (Dagan et al., 2005), CB (De Marneff et al., 2019))      
* Coreference resolution (WNLI and WSC (Levesque et al., 2012))   
* Sentence completion (COPA (Roemmele et al., 2011))   
* Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))   
* Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), BoolQ (Clark et al., 2019))      


------

## 2.4 Input and Output Format


**â€œtext-to-textâ€**      
* context ë˜ëŠ” conditioningì„ ìœ„í•´, ëª¨ë¸ì— some textë¥¼ ì œê³µí•œ ë‹¤ìŒ some output textë¥¼ ìƒì„±í•˜ë„ë¡ ìš”ì²­í•˜ëŠ” ì‘ì—…     
* ì¦‰ **input, outputì´ ëª¨ë‘ text**      



**add a task-specific (text) prefix**      
* ëª¨ë¸ì´ **ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ì‘ì—…ì„ ì§€ì •**í•˜ê¸° ìœ„í•´ ì…ë ¥ ì‹œí€€ìŠ¤ì— ì¶”ê°€      
* ex       
  * **translate task**       
  ex) ì˜ì–´ì—ì„œ ë…ì¼ì–´ë¡œ ë¬¸ì¥ "That is good"ë¥¼ ë²ˆì—­í•˜ëŠ” taskëŠ”,    
  ëª¨ë¸ì˜ fed the sequenceê°€ ```â€œtranslate English to German: That is good."```ì´ê³ ,         
  ëª¨ë¸ì€ ```â€œDas ist gut.â€```ë¥¼ ì¶œë ¥í•˜ë„ë¡ í›ˆë ¨ë  ê²ƒì´ë‹¤.       
  * **text classification tasks**
  ëª¨ë¸ì€ target labelì— í•´ë‹¹í•˜ëŠ” single wordë§Œ ì˜ˆì¸¡           
  ex) MNLI benchmarkì˜ ëª©í‘œëŠ” ì „ì œê°€ ê°€ì„¤ì„ ì•”ì‹œí•˜ëŠ”ì§€(â€œentailmentâ€), ëª¨ìˆœë˜ëŠ”ì§€(â€œcontradictionâ€), ë˜ëŠ” ë‘˜ ë‹¤(â€œneutralâ€)ì¸ì§€ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„.       
  ì…ë ¥ ì‹œí€€ìŠ¤ëŠ” ```â€œmnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.â€```ì´ë©´,     
  ëª¨ë¸ì€ target ë‹¨ì–´ "entailment"ì„ ì˜ˆì¸¡í•´ì•¼í•œë‹¤.     
* task-specific (text) prefixì˜ ì„ íƒì€ **í•˜ì´í¼íŒŒë¼ë¯¸í„°**       
* ì•„ë˜ "ë” ì•Œì•„ë³´ê¸°"ì—ì„œ ì—°êµ¬í•œ ëª¨ë“  ì‘ì—…ì— ëŒ€í•´ prefix ì…ë ¥ì˜ ëª¨ë“  ì˜ˆë¥¼ ì œê³µ    




<img width="340" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/dd4e89c3-64a0-4245-a206-527cc0a22a34">


    

<details>
<summary>ğŸ“œ ë” ì•Œì•„ë³´ê¸°: prefix ì…ë ¥ì˜ ëª¨ë“  ì˜ˆ </summary>
<div markdown="1">
  
**Text-to-text**     
Our text-to-text framework provides a simple way to train a single model
on a wide variety of text tasks using the same loss function and decoding procedure.
We showed how this approach can be successfully applied to generative tasks like
abstractive summarization, classification tasks like natural language inference, and
even regression tasks like STS-B. In spite of its simplicity, we found the text-totext framework obtained comparable performance to task-specific architectures and
ultimately produced state-of-the-art results when combined with scale.


**Architectures**        
While some work on transfer learning for NLP has considered architectural
variants of the Transformer, we found the original encoder-decoder form worked
best in our text-to-text framework. Though an encoder-decoder model uses twice as
many parameters as â€œencoder-onlyâ€ (e.g. BERT) or â€œdecoder-onlyâ€ (language model)
architectures, it has a similar computational cost. We also showed that sharing the
parameters in the encoder and decoder did not result in a substantial performance
drop while halving the total parameter count.



**Unsupervised objectives**      
Overall, we found that most â€œdenoisingâ€ objectives, which train
the model to reconstruct randomly corrupted text, performed similarly in the text-totext setup. As a result, we suggest using objectives that produce short target sequences
so that unsupervised pre-training is more computationally efficient.


**Data sets**         
We introduced the â€œColossal Clean Crawled Corpusâ€ (C4), which comprises
heuristically-cleaned text from the Common Crawl web dump. When comparing C4 to
data sets that use additional filtering, we found that training on in-domain unlabeled
data could boost performance in a few downstream tasks. However, constraining to
a single domain typically results in a smaller data set. We separately showed that
performance can degrade when an unlabeled data set is small enough that it is repeated
many times over the course of pre-training. This motivates the use of a large and
diverse data set like C4 for generic language understanding tasks.


**Training strategies**      
We found that the basic approach of updating all of a pre-trained
modelâ€™s parameters during fine-tuning outperformed methods that are designed to
update fewer parameters, although updating all parameters is most expensive. We also
experimented with various approaches for training the model on multiple tasks at once,
which in our text-to-text setting simply corresponds to mixing examples from different
data sets when constructing batches. The primary concern in multi-task learning is
setting the proportion of each task to train on. We ultimately did not find a strategy
for setting mixing proportions that matched the performance of the basic approach of
unsupervised pre-training followed by supervised fine-tuning. However, we found that
fine-tuning after pre-training on a mixture of tasks produced comparable performance
to unsupervised pre-training.


**Scaling**       
We compared various strategies for taking advantage of additional compute, including training the model on more data, training a larger model, and using an ensemble
of models. We found each approach conferred a significant boost in performance,
though training a smaller model on more data was often outperformed by training
a larger model for fewer steps. We also showed an ensemble of models can provide
substantially better results than a single model, which provides an orthogonal means
of leveraging additional computation. Ensembling models that were fine-tuned from
the same base pre-trained model performed worse than pre-training and fine-tuning
all models completely separately, though fine-tune-only ensembling still substantially
outperformed a single model.


**Pushing the limits**     
We combined our above insights and trained substantially larger
models (up to 11 billion parameters) to achieve state-of-the-art results across many of
the benchmarks we considered. For unsupervised training, we extracted text from our
C4 data set and applied a denoising objective that corrupts contiguous spans of tokens.
We pre-trained on a multi-task mixture before fine-tuning on individual tasks. Overall,
our models were trained on over 1 trillion tokens. In the interest of facilitating the
replication, extension, and application of our results, we release our code, the C4 data
set, and pre-trained model weights for each T5 variant.1

</div>
</details> 




---
---


# 3. Experiments 


## 3.1 Baseline

baselineì— ëŒ€í•œ ëª©í‘œëŠ” **typical modern ê´€í–‰ì„ ë°˜ì˜**í•˜ëŠ” ê²ƒì´ë‹¤.        
simple denoising objectiveë¥¼ ì‚¬ìš©í•˜ì—¬ Transformerë¥¼ pre-trainí•œ ë‹¤ìŒ ê° downstream tasksì„ ë³„ë„ë¡œ fine-tuneí•œë‹¤.       

### 3.1.1 Model


* model: standard encoder-decoder Transformerë¥¼ ì‚¬ìš©      
* baseline model: ì¸ì½”ë”ì™€ ë””ì½”ë”ê°€ ê°ê° "$$BERT_{BASE}$$" ìŠ¤íƒê³¼ í¬ê¸°ì™€ êµ¬ì„±ì´ ìœ ì‚¬í•˜ë„ë¡ ì„¤ê³„ë¨.      


<details>
<summary>ğŸ“œ baseline modelì„ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´? </summary>
<div markdown="1">
  

**baseline model**    
* íŠ¹íˆ, ì¸ì½”ë”ì™€ ë””ì½”ë”ëŠ” ëª¨ë‘ 12ê°œì˜ ë¸”ë¡(ê° ë¸”ë¡ì€ ìê¸° ì£¼ì˜, ì„ íƒì  ì¸ì½”ë”-ë””ì½”ë” ì£¼ì˜ ë° í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ë¡œ êµ¬ì„±ë¨)ìœ¼ë¡œ êµ¬ì„±      
* encoder and decoder consist of 12 blocks     
(each block comprising self-attention, optional encoder-decoder attention, and a feed-forward network)     
* The feed-forward networks in each block consist of a dense layer (output dimensionality of dff = 3072)    
* followed by a ReLU nonlinearity and another dense layer.        
* The â€œkeyâ€ and â€œvalueâ€ matrices of all attention mechanisms have an inner dimensionality of dkv = 64         
* all attention mechanisms have 12 heads        
* All other sub-layers and embeddings have a dimensionality of dmodel = 768        
* In total, this results in a model with about 220 million parameters.        
* ê¸°ì¤€ ëª¨ë¸ì— í•˜ë‚˜ì˜ ë ˆì´ì–´ ìŠ¤íƒì´ ì•„ë‹Œ ë‘ ê°œì˜ ë ˆì´ì–´ ìŠ¤íƒì´ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì´ëŠ” BERTBASE ë§¤ê°œ ë³€ìˆ˜ì˜ ì•½ ë‘ ë°°     
* ì •ê·œí™”ë¥¼ ìœ„í•´ ëª¨ë¸ì— ë“œë¡­ì•„ì›ƒì´ ì ìš©ë˜ëŠ” ëª¨ë“  ê³³ì—ì„œ ë“œë¡­ì•„ì›ƒ í™•ë¥  0.1ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
  

</div>
</details> 





### 3.1.2 Training
Section 2.4ì—ì„œ ì„¤ëª…í•œëŒ€ë¡œ all tasksëŠ” **text-to-text tasks**ë‹¤.     
* ì´ë¥¼ í†µí•´ ìš°ë¦¬ëŠ” **standard maximum likelihood**ë¥¼ ì‚¬ìš©í•˜ì—¬ trainning ê°€ëŠ¥      
  * i.e. teacher forcing (Williams and Zipser, 1989)            
  * cross-entropy loss        
* **optimization**ë¥¼ ìœ„í•´ **AdaFactor**(Shazeer and Stern, 2018)ë¥¼ ì‚¬ìš©    
* **test time**ì— ìš°ë¦¬ëŠ” **greedy decoding**ì‚¬ìš©    
  * ì¦‰, ë§¤ timestepì—ì„œ highest-probability logit ì„ íƒ)ì„ ì‚¬ìš©       
* $$2^35$$ â‰ˆ 34B í† í°ì— ëŒ€í•´ pre-train (**BERTì™€ RoBERTaì— ë¹„í•´ ìƒë‹¹íˆ ì‘ì€ ìˆ˜ì¹˜**)        
* **pre-training** ë™ì•ˆ          
  * **â€œinverse square rootâ€ learning rate schedule** ( $$1/ \sqrt{max(n, k)} $$)   ì‚¬ìš©          
      * n: current training iteration          
      * k: number of warm-up steps (set to $$10^4$$ in all of our experiments)      
      * ì‚¬ìš©í•˜ëŠ” ì´ìœ : ì—¬ê¸°ì„œ ì´ê²ƒì€ ì²˜ìŒ $$10^4$$ë‹¨ê³„ì— ëŒ€í•´ 0.01ì˜ ì¼ì •í•œ í•™ìŠµë¥ ì„ ì„¤ì •í•œ ë‹¤ìŒ pre-trainì´ ëë‚  ë•Œê¹Œì§€ **í•™ìŠµë¥ ì„ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ê°ì†Œ**ì‹œí‚´    
  * **triangular learning rate** (Howard and Ruder, 2018)ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜       
      * ì´ëŠ” ì•½ê°„ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë‚´ì§€ë§Œ ì‚¬ì „ì— ì´ training steps ìˆ˜ë¥¼ ì•Œì•„ì•¼ í•¨     
  * ìš°ë¦¬ëŠ” ì¼ë¶€ ì‹¤í—˜ì—ì„œ í›ˆë ¨ ë‹¨ê³„ì˜ ìˆ˜ë¥¼ ë³€ê²½í•  ê²ƒì´ê¸° ë•Œë¬¸ì—, ìš°ë¦¬ëŠ” ë” ì¼ë°˜ì ì¸ **â€œinverse square rootâ€ì„ ì„ íƒ**í•¨       
  


<details>
<summary>ğŸ“œ pre-train each modelì„ ë” ìì„¸íˆ ì•Œê³ ì‹¶ë‹¤ë©´? </summary>
<div markdown="1">
  




* pre-train each model for $$2^19$$ = 524,288 steps on C4 before fine-tuning.        
* We use a maximum sequence length of 512     
* batch size of 128 sequences.     
* ê°€ëŠ¥í•  ë•Œë§ˆë‹¤, ë°°ì¹˜ì˜ ê° í•­ëª©ì— ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ "pack"í•˜ì—¬ ë°°ì¹˜ì— ì•½ $$2^16 = 65,536$$ê°œì˜ í† í°ì´ í¬í•¨ë˜ë„ë¡ í•¨      
* ì´ ë°°ì¹˜ í¬ê¸°ì™€ ë‹¨ê³„ ìˆ˜ëŠ” $$2^35$$ â‰ˆ 34B í† í°ì— ëŒ€í•œ pre-trainì— í•´ë‹¹í•¨      
* ì´ëŠ” ì•½ 137B í† í°ì„ ì‚¬ìš©í•œ BERT(Devlin et al., 2018)ë‚˜ ì•½ 2.2Tê°œì˜ í† í°ì„ ì‚¬ìš©í•œ RoBERTa(Lu et al., 2019c)ì— ë¹„í•´ ìƒë‹¹íˆ ì ì€ ìˆ˜ì¹˜         
* 235ê°œì˜ í† í°ì€ ì „ì²´ C4 ë°ì´í„° ì„¸íŠ¸ì˜ ì¼ë¶€ë§Œ í¬í•¨í•˜ë¯€ë¡œ pre-trainìœ¡ ì¤‘ì—ëŠ” ë°ì´í„°ë¥¼ **ë°˜ë³µí•˜ì§€ ì•ŠìŒ**             

</div>
</details> 

<details>
<summary>ğŸ“œ pre-train each modelì„ ë” ìì„¸íˆ ì•Œê³ ì‹¶ë‹¤ë©´? </summary>
<div markdown="1">
  
fine-tuningì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³ ì‹¶ë‹¤ë©´?

* ëª¨ë“  ì‘ì—…ì—ì„œ $$2^18 = 262,196$$ ë‹¨ê³„ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •ëœë‹¤.        
* 128ê°œì˜ 128 length-512 sequence(ì¦‰, ë°°ì¹˜ë‹¹ 216ê°œì˜ í† í°)ë¥¼ ê°€ì§„ ë°°ì¹˜ë¥¼ ê³„ì† ì‚¬ìš©.     
* fine-tuning ì‹œ 0.001ì˜ ì¼ì •í•œ í•™ìŠµë¥ ì„ ì‚¬ìš©í•¨    
* ìš°ë¦¬ëŠ” 5,000 ë‹¨ê³„ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ê³  ê°€ì¥ ë†’ì€ validation ì„±ëŠ¥ì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì— ê²°ê³¼ë¥¼ ë³´ê³         
* ì„¹ì…˜ 3.7ì˜ ì‹¤í—˜ì„ ì œì™¸í•œ ëª¨ë“  ì‹¤í—˜ì— ëŒ€í•´, ìš°ë¦¬ëŠ” ì‹œí—˜ ì„¸íŠ¸ì—ì„œ ëª¨ë¸ ì„ íƒì„ ìˆ˜í–‰í•˜ì§€ ì•Šê¸° ìœ„í•´ validation set ì„¸íŠ¸ì— ê²°ê³¼ë¥¼ ë³´ê³ í•©ë‹ˆë‹¤       


  
  
</div>
</details> 






### 3.1.3 Vocabulary
* **SentencePiece**(Kudo and Richardson, 2018)ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ WordPiece í† í°ìœ¼ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.       
* ëª¨ë“  ì‹¤í—˜ì—ì„œ ìš°ë¦¬ëŠ” 32,000ê°œì˜ ë‹¨ì–´ë¥¼ ì‚¬ìš©       



### 3.1.4 Unsupervised Objective
* **unlabeled dataë¥¼ í™œìš©**í•˜ì—¬ ëª¨ë¸ì„ pretrain downstream ì‘ì—…ì— ìœ ìš©í•œ ëª¨ë¸ **ì¼ë°˜í™” ê°€ëŠ¥í•œ ì§€ì‹**ì„ ê°€ë¥´ì³ ì¤ë‹ˆë‹¤.       
* <span style="background-color:#fff5b1">**denoising objective**ì‚¬ìš©</span>: ëª¨ë¸ì€ ì…ë ¥ì—ì„œ **ëˆ„ë½ë˜ê±°ë‚˜ ì†ìƒëœ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨**ëœë‹¤.      
  * BERTì˜ **"masked language modeling"** ëª©í‘œì™€ **â€œword dropoutâ€**ì •ê·œí™” ê¸°ë²•ì—ì„œ ì˜ê°ì„ ë°›ì•„ ì…ë ¥ ì‹œí€€ìŠ¤ì—ì„œ í† í°ì˜ 15%ë¥¼ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•œ í›„ ë“œë¡­í•˜ëŠ” ëª©í‘œë¥¼ ì„¤ê³„.     
  * ëª¨ë“  ì—°ì†ëœ ì‚­ì œëœ í† í° ë²”ìœ„ëŠ” ë‹¨ì¼ **sentinel tokenìœ¼ë¡œ ëŒ€ì²´**ë¨.     
     * ê° Sentinel í† í°ì—ëŠ” ì‹œí€€ìŠ¤ì— ê³ ìœ í•œ í† í° IDê°€ í• ë‹¹ë¨     
     *  Sentinel IDëŠ” ì–´íœ˜ì— ì¶”ê°€ë˜ëŠ” íŠ¹ìˆ˜ í† í°ì´ë©° ì›Œë“œí”¼ìŠ¤ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ.       
  *  ê·¸ëŸ¬ë©´ **target**ì€ **ì…ë ¥ ì‹œí€€ìŠ¤ì— ì‚¬ìš©ëœ ê²ƒê³¼ ë™ì¼í•œ Sentinel í† í°**ê³¼ **targert ì‹œí€€ìŠ¤ì˜ ë**ì„ í‘œì‹œí•˜ëŠ” **ìµœì¢… ë™ì¼í•œ í† í°**ìœ¼ë¡œ êµ¬ë¶„ëœ ëª¨ë“  í† í° ë²”ìœ„ì— í•´ë‹¹í•¨       
* ì¦‰ ì•„ë˜ì˜ ê·¸ë¦¼ì„ ì˜ˆì‹œë¡œ ë³´ì.    
<img width="241" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/285fe79c-1d4e-4e37-98d1-880d1ceafe60">
   * **1)** words â€œforâ€, â€œinvitingâ€ and â€œlastâ€ (marked with an Ã—) are **randomly chosen** for **corruption(ì†ìƒ)**    
   * **2)** ì†ìƒ(corruption)ëœ í† í°ì˜ ê° ì—°ì† ë²”ìœ„ëŠ” ì˜ˆì œì—ì„œ ê³ ìœ í•œ sentinel í† í°(```<X>``` ë° ```<Y>```ë¡œ í‘œì‹œë¨)ìœ¼ë¡œ ëŒ€ì²´ë¨     
   * **3)** "for"ì™€ "inviting"ì´ ì—°ì†ì ìœ¼ë¡œ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— ì´ë“¤ì€ í•˜ë‚˜ì˜ sentinel ```<X>```ë¡œ ëŒ€ì²´ë¨.       
   * **4)** ê·¸ëŸ° ë‹¤ìŒ ì¶œë ¥ ì‹œí€€ìŠ¤ëŠ”  dropped-outë¡œ êµ¬ì„±ë¨ (ì¦‰ ë³´ë©´ ì˜ˆì¸¡í•´ì•¼ë  ê²ƒ(ë§ˆìŠ¤íŒ… í•œ ë‹¨ì–´ë“¤)ë§Œ tragetê°’ìœ¼ë¡œ ë‚˜ì™€ìˆë‹¤.)     
    ì…ë ¥ì—ì„œ ì´ë¥¼ ëŒ€ì²´í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” sentinel í† í°ê³¼ ìµœì¢… sentinel í† í° ```<Z>```ë¡œ êµ¬ë¶„ë¨       
    
  

### 3.1.5 Baseline Performance
* ì´ ì„¹ì…˜ì—ì„œëŠ” ìœ„ì—ì„œ ì„¤ëª…í•œ baseline ì‹¤í—˜ ì ˆì°¨ë¥¼ ì‚¬ìš©í•˜ì—¬ **downstream tasksì—ì„œ ì–´ë–¤ ì„±ëŠ¥ì„ ê¸°ëŒ€í•˜ëŠ”ì§€ íŒŒì•…**í•¨      
* baseline configurationì´ ë‚˜íƒ€ë‚˜ëŠ” ê³³ë§ˆë‹¤ â˜…ë¡œ í‘œì‹œí•¨          
* ë˜í•œ ì£¼ì–´ì§„ ì‹¤í—˜ì—ì„œ ìµœëŒ€(ìµœìƒ)ì˜ ë‘ í‘œì¤€ í¸ì°¨ ë‚´ì— ìˆëŠ” ì ìˆ˜ì— ëŒ€í•´ì„œë„ êµµì€ ê¸€ì”¨ë¡œ í‘œì‹œí•¨     
<img width="311" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/3a99b93b-8341-47e2-a9f3-6d8fa97c8e68">
  * <span style="background-color:#fff5b1">**pre trainì´** ê±°ì˜ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ **ìƒë‹¹í•œ ì´ì **ì„ ì œê³µí•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬</span>í•¨           
  * ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„ íƒí•˜ì—¬ early stoppingë¥¼ ìˆ˜í–‰í•˜         
  â¡ our baselineê³¼ "no pre-training" ì‚¬ì´ì˜ í° ì°¨ì´: **pre-trainingì´ ì œí•œëœ ë°ì´í„°ê°€ ìˆëŠ” ì‘ì—…**ì—ì„œ **ì„±ëŠ¥ì„ ì–¼ë§ˆë‚˜ í–¥ìƒ**ì‹œí‚¤ëŠ”ì§€ë¥¼ **ê°•ì¡°**         
  
-----


## 3.2 Architectures
íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì›ë˜ encoder-decoder architectureì™€ í•¨ê»˜ ë„ì…ë˜ì—ˆì§€ë§Œ,       
NLPì— ëŒ€í•œ transfer learningì— ëŒ€í•œ ë§ì€ í˜„ëŒ€ ì—°êµ¬ëŠ” ëŒ€ì²´ architectureë¥¼ ì‚¬ìš©í•œë‹¤.              

ì´ ì„¹ì…˜ì—ì„œëŠ” ì´ëŸ¬í•œ architectureì˜ ë³€í˜•ì„ ê²€í† í•˜ê³  ë¹„êµ            


### 3.2.1 Model Structures
* ë‹¤ì–‘í•œ architectureì˜ ì£¼ìš” ì°¨ë³„í™” ìš”ì†ŒëŠ” ëª¨ë¸ì˜ ë‹¤ì–‘í•œ attention mechanisms ì˜í•´ ì‚¬ìš©ë˜ëŠ” "mask"ë‹¤.        
* Transformerì˜ self-attention operation     
   * **input:** sequence      
   * **output**:ë™ì¼í•œ ê¸¸ì´ì˜ ìƒˆë¡œìš´ sequenceë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. (ì¶œë ¥ sequenceì˜ ê° í•­ëª©ì€ ì…ë ¥ sequenceì˜ ê°€ì¤‘ì¹˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ìƒì„±ë¨)       

<img width="269" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/6f2ae2d2-c17b-476d-9aac-34feab52bcef">
* êµ¬ì„±     
  * **$$y_i$$**: output sequenceì˜ ië²ˆì§¸ ìš”ì†Œ      
  * **$$x_j$$**: input sequenceì˜ jë²ˆì§¸ í•­ëª©             
* **$$y_i$$ì˜ ê³„ì‚°:** $$\displaystyle\sum_{j}{w_{i,j}}x_{j}$$,       
  * **$$w_{i,j}$$:** $$x_i$$ì™€ $$x_j$$ì˜ í•¨ìˆ˜ë¡œì„œ self-attention mechanismì— ì˜í•´ ìƒì„±ëœ scalar weightì„       
  * ê·¸ëŸ° ë‹¤ìŒ attention maskë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬,   
   íŠ¹ì • output timestepì—ì„œ inputì˜ ì–´ë–¤ í•­ëª©ì— attention ê¸°ìš¸ì¼ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì œí•œ(constrain)í•¨.        
  * eg) **the causal mask (Figure 3, middle) sets**:ëŠ” j > iì¼ ê²½ìš° ì„ì˜ì˜ $$w_{i,j}$$ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•¨      


* <span style="background-color:#F5F5F5">**distinguishing factor for different architecture: encoder-decoder Transformer**</span>     
  * **ë‘ ê°œì˜ ë ˆì´ì–´ ìŠ¤íƒìœ¼ë¡œ êµ¬ì„±ë¨:** encoder(input sequenceê°€ ê³µê¸‰ë¨) ë° decoder(ìƒˆ output sequenceë¥¼ ìƒì„±)     
  * Figure 4ì˜ ì²«ë²ˆì§¸      
  * **encoder:** **â€œfully-visibleâ€ attention mask**ë¥¼ ì‚¬ìš© (Figure 3)      
        * ê° ì¶œë ¥ í•­ëª©ì„ ìƒì„±í•  ë•Œ self-attention mechanismì´ input í•­ëª©ì— attentioní•  ìˆ˜ ìˆë‹¤.     
        * ì´ ì´ëŸ¬í•œ ë§ˆìŠ¤í‚¹ í˜•ì‹ì€ "prefix", ì¦‰ ë‚˜ì¤‘ì— ì˜ˆì¸¡í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ì— ì œê³µë˜ëŠ” ì¼ë¶€ contextì •ë³´ë¥¼ ì œê³µí•  ë•Œ ì í•©.       * **decoder:**  **â€œcausalâ€ masking patter**ì„ ì‚¬ìš© (Figure 3)                    
        * ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ië²ˆì§¸ í•­ëª©ì„ ìƒì„±í•  ë•Œ â€œcausalâ€ maskingì€ ëª¨ë¸ì´ j > iì— ëŒ€í•œ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ jë²ˆì§¸ í•­ëª©ì— ì°¸ì—¬í•˜ëŠ” ê²ƒì„ ë°©ì§€.          
        * ì´ê²ƒì€ ëª¨ë¸ì´ ì¶œë ¥ì„ ìƒì„±í•  ë•Œ **"ë¯¸ë˜ë¥¼ ë‚´ë‹¤ë³¼ ìˆ˜" ì—†ë„ë¡ í›ˆë ¨ ì¤‘ì— ì‚¬ìš©**ë©ë‹ˆë‹¤.      


ì¸ì½”ë”-ë””ì½”ë” ë³€ì••ê¸°ì˜ ë””ì½”ë”ëŠ” ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì¦‰, ê° ì¶œë ¥ ì‹œê°„ ë‹¨ê³„ì—ì„œ í† í°ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§ë˜ê³  ìƒ˜í”Œì´ ëª¨ë¸ë¡œ í”¼ë“œë°±ë˜ì–´ ë‹¤ìŒ ì¶œë ¥ ì‹œê°„ ë‹¨ê³„ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìƒì„±í•˜ëŠ” ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ì™€ ê°™ì´, íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”(ì¸ì½”ë” ì—†ìŒ)ëŠ” ì–¸ì–´ ëª¨ë¸(LM), ì¦‰ ë‹¤ìŒ ë‹¨ê³„ ì˜ˆì¸¡ë§Œì„ ìœ„í•´ í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤(Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). ì´ê²ƒì€ ìš°ë¦¬ê°€ ê³ ë ¤í•˜ëŠ” ë‘ ë²ˆì§¸ ëª¨ë¸ êµ¬ì¡°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ì•„í‚¤í…ì²˜ì˜ ê°œëµë„ëŠ” ê·¸ë¦¼ 4ì˜ ê°€ìš´ë°ì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤. ì‚¬ì‹¤, NLPì— ëŒ€í•œ ì „ì´ í•™ìŠµì— ëŒ€í•œ ì´ˆê¸° ì—°êµ¬ëŠ” ì‚¬ì „ í›ˆë ¨ ë°©ë²•ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ë§ ëª©í‘œì™€ í•¨ê»˜ ì´ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤(Radford et al., 2018).



The decoder in an encoder-decoder Transformer is used to autoregressively produce an
output sequence. That is, at each output timestep, a token is sampled from the modelâ€™s
predicted distribution and the sample is fed back into the model to produce a prediction for
the next output timestep, and so on. As such, a Transformer decoder (without an encoder)
can be used as a language model (LM), i.e. a model trained solely for next-step prediction
(Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). This constitutes the second
model structure we consider. A schematic of this architecture is shown in Figure 4, middle.
In fact, early work on transfer learning for NLP used this architecture with a language
modeling objective as a pre-training method (Radford et al., 2018).



Language models are typically used for compression or sequence generation (Graves,
2013). However, they can also be used in the text-to-text framework simply by concatenating
the inputs and targets. As an example, consider the case of English to German translation:
If we have a training datapoint with input sentence â€œThat is good.â€ and target â€œDas ist
gut.â€, we would simply train the model on next-step prediction over the concatenated input
sequence â€œtranslate English to German: That is good. target: Das ist gut.â€ If we wanted to
obtain the modelâ€™s prediction for this example, the model would be fed the prefix â€œtranslate
English to German: That is good. target:â€ and would be asked to generate the remainder
of the sequence autoregressively. In this way, the model can predict an output sequence
given an input, which satisfies the needs of text-to-text tasks. This approach was recently
used to show that language models can learn to perform some text-to-text tasks without
supervision (Radford et al., 2019).


A fundamental and frequently cited drawback of using a language model in the textto-text setting is that causal masking forces the modelâ€™s representation of the ith entry of
the input sequence to only depend on the entries up until i. To see why this is potentially
disadvantageous, consider the text-to-text framework where the model is provided with a
prefix/context before being asked to make predictions (e.g., the prefix is an English sentence
and the model is asked to predict the German translation). With fully causal masking, the
modelâ€™s representation of a prefix state can only depend on prior entries of the prefix. So,
when predicting an entry of the output, the model will attend to a representation of the
prefix that is unnecessarily limited. Similar arguments have been made against using a
unidirectional recurrent neural network encoder in sequence-to-sequence models (Bahdanau
et al., 2015).


This issue can be avoided in a Transformer-based language model simply by changing
the masking pattern. Instead of using a causal mask, we use fully-visible masking during
the prefix portion of the sequence. This masking pattern and a schematic of the resulting
â€œprefix LMâ€ (the third model structure we consider) are illustrated in the rightmost panels of
Figures 3 and 4, respectively. In the English to German translation example mentioned above,
fully-visible masking would be applied to the prefix â€œtranslate English to German: That is
good. target:â€ and causal masking would be used during training for predicting the target
â€œDas ist gut.â€ Using a prefix LM in the text-to-text framework was originally proposed by
Liu et al. (2018). More recently, Dong et al. (2019) showed that this architecture is effective
on a wide variety of text-to-text tasks. This architecture is similar to an encoder-decoder
model with parameters shared across the encoder and decoder and with the encoder-decoder
attention replaced with full attention across the input and target sequence.


We note that when following our text-to-text framework, the prefix LM architecture
closely resembles BERT (Devlin et al., 2018) for classification tasks. To see why, consider an
example from the MNLI benchmark where the premise is â€œI hate pigeons.â€, the hypothesis is
â€œMy feelings towards pigeons are filled with animosity.â€ and the correct label is â€œentailmentâ€.
To feed this example into a language model, we would transform it into the sequence â€œmnli
premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.
target: entailmentâ€. In this case, the fully-visible prefix would correspond to the entire input
sequence up to the word â€œtarget:â€, which can be seen as being analogous to the â€œclassificationâ€
token used in BERT. So, our model would have full visibility over the entire input, and then
would be tasked with making a classification by outputting the word â€œentailmentâ€. It is easy
for the model to learn to output one of the valid class labels given the task prefix (â€œmnliâ€ in
this case). As such, the main difference between a prefix LM and the BERT architecture is
that the classifier is simply integrated into the output layer of the Transformer decoder in
the prefix LM












