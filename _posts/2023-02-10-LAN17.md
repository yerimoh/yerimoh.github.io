---
title: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ì •ë¦¬"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: [Paper]
layout: post
tag:
- Paper
order: 0

comments: true
---


# Abstract
Transfer learning, where a model is first pre-trained on a data-rich task before being fineï¿½tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new â€œColossal Clean Crawled Corpusâ€, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.1

**[ë…¼ë¬¸ ë°°ê²½]**
ëª¨ë¸ì´ Downstream Taskì—ì„œ fine-tuningë˜ê¸° ì „ì— data-rich taskì—ì„œ pre-trainì„ í•˜ëŠ” ê²ƒì€  ìì—°ì–´ ì²˜ë¦¬(NLP)ì˜ ê°•ë ¥í•œ ê¸°ìˆ ë¡œ ë¶€ìƒí–ˆë‹¤.     
[transfer learning](https://yerimoh.github.io/DL12/)ì˜ íš¨ê³¼ëŠ” ë‹¤ì–‘í•œ ì ‘ê·¼ë²•, ë°©ë²•ë¡  ë° ì‹¤ì²œì„ ë‚³ì•˜ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ ê¸°ë°˜ ì–¸ì–´ ë¬¸ì œë¥¼ í…ìŠ¤íŠ¸ ëŒ€ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ NLPì— ëŒ€í•œ ì „ì´ í•™ìŠµ ê¸°ìˆ ì˜ í’ê²½ì„ íƒêµ¬í•œë‹¤. ìš°ë¦¬ì˜ ì²´ê³„ì ì¸ ì—°êµ¬ëŠ” ìˆ˜ì‹­ ê°€ì§€ ì–¸ì–´ ì´í•´ ê³¼ì œì— ëŒ€í•œ ì‚¬ì „ í›ˆë ¨ ëª©í‘œ, ì•„í‚¤í…ì²˜, ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°ì´í„° ì„¸íŠ¸, ì „ì†¡ ì ‘ê·¼ë²• ë° ê¸°íƒ€ ìš”ì†Œë¥¼ ë¹„êµí•œë‹¤. ìš°ë¦¬ì˜ íƒì‚¬ì—ì„œ ì–»ì€ í†µì°°ë ¥ê³¼ ê·œëª¨ ë° ìƒˆë¡œìš´ "Colossal Clean Crawled Corpus"ë¥¼ ê²°í•©í•˜ì—¬ ìš”ì•½, ì§ˆë¬¸ ë‹µë³€, í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë“±ì„ í¬í•¨í•˜ëŠ” ë§ì€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ë‹¬ì„±í•œë‹¤. NLPì— ëŒ€í•œ ì´ì „ í•™ìŠµì— ëŒ€í•œ í–¥í›„ ì‘ì—…ì„ ì´‰ì§„í•˜ê¸° ìœ„í•´ ë°ì´í„° ì„¸íŠ¸, ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë° ì½”ë“œë¥¼ ë¦´ë¦¬ìŠ¤í•œë‹¤.1


<details>
<summary>ğŸ“œ Downstream Task ì˜ë¯¸ ë³´ê¸°</summary>
<div markdown="1">
  

êµ¬ì²´ì ìœ¼ë¡œ í’€ê³  ì‹¶ì€ ë¬¸ì œë“¤ì„ ë§í•œë‹¤.

NLPì—ì„œëŠ” ì–¸ì–´ëª¨ë¸ì„ pre-trainë°©ì‹ì„ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ê³ ,    
ê·¸ í›„ì— ì›í•˜ê³ ì í•˜ëŠ” taskë¥¼ fine-tuningí•˜ëŠ” ë°©ì‹ì„ í†µí•´ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ë•Œ, taskë¥¼ Downstream Taskë¼ í•œë‹¤.

ì˜ˆë¥¼ë“¤ì–´, BERTì˜ ì–¸ì–´ëª¨ë¸ì„ ì§ˆì˜ì‘ë‹µ Taskë¼ì¸ squadë¥¼ í•™ìŠµí•œë‹¤ê³  í• ë•Œ, ì´ë•Œ ì§ˆì˜ì‘ë‹µ Taskë¥¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ Taskë¡œ ë³¼ ìˆ˜ ìˆì„ê²ƒì´ë‹¤.  
  
  
</div>
</details>  

