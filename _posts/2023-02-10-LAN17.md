---
title: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ì •ë¦¬"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: #[Paper]
layout: post
tag:
#- Paper
order: 0

comments: true
---


# Abstract
<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ ë°°ê²½]**</span>    
ëª¨ë¸ì´ Downstream Taskì—ì„œ fine-tuningë˜ê¸° ì „ì— data-rich taskì—ì„œ pre-trainì„ í•˜ëŠ” ê²ƒì€  ìì—°ì–´ ì²˜ë¦¬(NLP)ì˜ ê°•ë ¥í•œ ê¸°ìˆ ë¡œ ë¶€ìƒí–ˆë‹¤.     
ì¦‰ ì´ëŸ¬í•œ [transfer learning](https://yerimoh.github.io/DL12/)ì€ NLPì— í° ë°œì „ì„ ê°€ì ¸ì™”ë‹¤.    

<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ì˜ ëª©ì ]**</span>    
* ëª¨ë“  í…ìŠ¤íŠ¸ ê¸°ë°˜ ì–¸ì–´ ë¬¸ì œë¥¼ text-to-text í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ <span style="background-color:#fff5b1">NLPì— ëŒ€í•œ [transfer learning](https://yerimoh.github.io/DL12/)ë¥¼ ì „ë°˜ì ìœ¼ë¡œ íƒêµ¬</span>í•œë‹¤.     
* ìœ„ íƒêµ¬ë¥¼ í†µí•´ ì–»ì€ í†µì°°ë ¥ê³¼ ê·œëª¨ ë° ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•  <span style="background-color:#fff5b1">"Colossal Clean Crawled Corpus"ë¥¼ ê²°í•©</span>í•˜ì—¬ ë§ì€ NLP evaluationì—ì„œ SOTA ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.        
â¡ ê³µê°œí•œ [data set, pre-trained models, and code](https://github.com/google-research/text-to-text-transfer-transformer) ë§í¬ë¥¼ ì²¨ë¶€í•˜ì˜€ë‹¤.        


<details>
<summary>ğŸ“œ Downstream Task ì˜ë¯¸ ë³´ê¸°</summary>
<div markdown="1">
  

êµ¬ì²´ì ìœ¼ë¡œ í’€ê³  ì‹¶ì€ ë¬¸ì œë“¤ì„ ë§í•œë‹¤.

NLPì—ì„œëŠ” ì–¸ì–´ëª¨ë¸ì„ pre-trainë°©ì‹ì„ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ê³ ,    
ê·¸ í›„ì— ì›í•˜ê³ ì í•˜ëŠ” taskë¥¼ fine-tuningí•˜ëŠ” ë°©ì‹ì„ í†µí•´ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ë•Œ, taskë¥¼ Downstream Taskë¼ í•œë‹¤.

ì˜ˆë¥¼ë“¤ì–´, BERTì˜ ì–¸ì–´ëª¨ë¸ì„ ì§ˆì˜ì‘ë‹µ Taskë¼ì¸ squadë¥¼ í•™ìŠµí•œë‹¤ê³  í• ë•Œ, ì´ë•Œ ì§ˆì˜ì‘ë‹µ Taskë¥¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ Taskë¡œ ë³¼ ìˆ˜ ìˆì„ê²ƒì´ë‹¤.  
  
  
</div>
</details>  

----
----



# Introduction

<span style="background-color:#F5F5F5">**[ìµœê·¼ ì—°êµ¬ ë™í–¥]**</span>    
* ìì—°ì–´ ì²˜ë¦¬(NLP)ì‘ì„ ì„ ìœ„í•œ trainì´ ê°€ëŠ¥í•˜ì—¬ë©´ ëª¨ë¸ì´ **downstream learningì— ì í•©í•œ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬**í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.              
ê·¸ëŸ°ë° ì´ëŠ” **ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ "ì´í•´"í•  ìˆ˜ ìˆê²Œ í•™ìŠµí•œë‹¤ê³  ë³´ê¸°ì—” í˜ë“¤ë‹¤**.               
â¡ í˜„ëŒ€ ê¸°ê³„ í•™ìŠµ ê´€í–‰ì—ì„œ ì´ëŸ¬í•œ trainì´ ëª…ì‹œì ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” ê²½ìš°ê°€ ê±°ì˜ ì—†ì–´, ëŒ€ì‹  **Auxiliary Task**(ë³¸ taskëŠ” ì•„ë‹ˆì§€ë§Œ, ë³¸ taskì—ì„œì˜ ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë³´ì¡° task)**ì˜ ì¼ë¶€**ë¡œ í•™ìŠµë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.          
( ìµœê·¼ì—ëŠ” **ë°ì´í„°ê°€ í’ë¶€í•œ ì‘ì—…**ì— ëŒ€í•´ **ì „ì²´ ëª¨ë¸ì„ pre-trainí•˜ëŠ” ê²ƒ**ì´ ì ì  ë” **ì¼ë°˜í™”**ë˜ê³  ìˆë‹¤.     
ì´ìƒì ìœ¼ë¡œ, ì´ pre-trainì€ ëª¨ë¸ì´ ë²”ìš© ëŠ¥ë ¥ê³¼ ì§€ì‹ì„ ê°œë°œí•˜ê²Œ í•˜ê³ , ì´ë¥¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ìœ¼ë¡œ ì´ì „í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.     
â¡ ë” í° ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë” í° ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒë§Œìœ¼ë¡œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.    




<span style="background-color:#F5F5F5">**[í˜„ ê²½í–¥ìœ¼ë¡œ ì¸í•œ í•œê³„]**</span>    
* ì´ëŸ¬í•œ ê²½í–¥ìœ¼ë¡œ ì¸í•´ NLPì— ëŒ€í•œ **transfer learning ë°©ë²•ë¡ ì„ ê°œë°œ**í•˜ëŠ” ì—°êµ¬ë“¤ì´ í™œë°œí•˜ê²Œ ì´ë£¨ì–´ì¡Œë‹¤.      
ì´ë ‡ê²Œ ì´ ë¶„ì•¼ê°€ ê¸‰ì„±ì¥í•˜ì—¬ ì—°êµ¬ê°€ í™œë°œí•´ì§€ë‹ˆ ì•„ë˜ì™€ ê°™ì€ ì‘ì—…ë“¤ì´ ì–´ë ¤ì›Œì¡Œë‹¤.       
  * ì—¬ëŸ¬ algorithmsì„ ë¹„êµ    
  * ìƒˆë¡œìš´ contributionsì˜ íš¨ê³¼ íŒŒì•…    
  * transfer learningì„ ìœ„í•œ ê¸°ì¡´ ë°©ë²•ì˜ space ì´í•´        


â¡ <span style="background-color:#fff5b1">ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì€ ì´ ë¶„ì•¼ì˜ ì›í™œí•œ ì´í•´ë¥¼ ìœ„í•´, **ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ì„ ì²´ê³„ì ìœ¼ë¡œ ì—°êµ¬**í•˜ê³  **í•„ë“œì˜ í˜„ì¬ í•œê³„ë¥¼ ë°€ì–´ë‚¼ ìˆ˜ ìˆëŠ” transfer learningì— ëŒ€í•œ í†µí•©ëœ ì ‘ê·¼ë²•ì„ í™œìš©**í•œë‹¤.</span>    
 


<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ í•´ê²°ì±…]**</span>      
* ë³¸ ë…¼ë¬¸ workì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” <span style="background-color:#fff5b1">ëª¨ë“  í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¬¸ì œë¥¼ **â€œtext-to-textâ€ ë¬¸ì œë¡œ ì²˜ë¦¬**</span>í•˜ëŠ” ê²ƒì´ë‹¤.             
ì¦‰, **í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ë“¤ì´ê³  ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥ìœ¼ë¡œ ìƒì„±**í•˜ëŠ” ê²ƒì´ë‹¤.          
    * ì´ëŸ¬í•œ í…ìŠ¤íŠ¸ ê°„ í”„ë ˆì„ì›Œí¬ëŠ” ìš°ë¦¬ê°€ ê³ ë ¤í•˜ëŠ” ëª¨ë“  ì‘ì—…ì— **ë™ì¼í•œ ëª¨ë¸, ëª©í‘œ, í›ˆë ¨ ì ˆì°¨ ë° decoding í”„ë¡œì„¸ìŠ¤ë¥¼ ì§ì ‘ ì ìš©**í•  ìˆ˜ ìˆê²Œ í•œë‹¤.          
    * ë³¸ ë…¼ë¬¸ì€ ì§ˆë¬¸ ë‹µë³€, ë¬¸ì„œ ìš”ì•½ ë° ê°ì • ë¶„ë¥˜ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ì˜ì–´ ê¸°ë°˜ NLP ë¬¸ì œì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€í•˜ì—¬ ì´ëŸ¬í•œ **ìœ ì—°ì„±ì„ í™œìš©**í•œë‹¤.        
    * ì´ í†µí•© ì ‘ê·¼ë²•ì„ í†µí•´, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ transfer learningì˜ target, ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°ì´í„° ì„¸íŠ¸ ë° ê¸°íƒ€ ìš”ì¸ì˜ íš¨ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ë™ì‹œì— ì´ì „ì— ê³ ë ¤ë˜ì—ˆë˜ ê²ƒ ì´ìƒìœ¼ë¡œ **ëª¨ë¸ê³¼ ë°ì´í„° ì„¸íŠ¸ë¥¼ í™•ì¥**í•˜ì—¬ NLPì— ëŒ€í•œ **transfer learningì˜ í•œê³„ë¥¼ íƒìƒ‰**í•  ìˆ˜ ìˆë‹¤.    




We emphasize that our goal is not to propose new methods but instead to provide a
comprehensive perspective on where the field stands. As such, our work primarily comprises
a survey, exploration, and empirical comparison of existing techniques. We also explore the
limits of current approaches by scaling up the insights from our systematic study (training
models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks
we consider. In order to perform experiments at this scale, we introduce the â€œColossal Clean
Crawled Corpusâ€ (C4), a data set consisting of hundreds of gigabytes of clean English text
scraped from the web. Recognizing that the main utility of transfer learning is the possibility
of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and
pre-trained models.1




The remainder of the paper is structured as follows: In the following section, we discuss
our base model and its implementation, our procedure for formulating every text processing
problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a
large set of experiments that explore the field of transfer learning for NLP. At the end of the
section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art
results on a wide variety of benchmarks. Finally, we provide a summary of our results and
wrap up with a look towards the future in Section 4.
















