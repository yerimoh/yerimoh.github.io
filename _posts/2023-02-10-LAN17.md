---
title: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ì •ë¦¬"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: #[Paper]
layout: post
tag:
#- Paper
order: 0

comments: true
---


# Abstract
<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ ë°°ê²½]**</span>    
ëª¨ë¸ì´ Downstream Taskì—ì„œ fine-tuningë˜ê¸° ì „ì— data-rich taskì—ì„œ pre-trainì„ í•˜ëŠ” ê²ƒì€  ìì—°ì–´ ì²˜ë¦¬(NLP)ì˜ ê°•ë ¥í•œ ê¸°ìˆ ë¡œ ë¶€ìƒí–ˆë‹¤.     
ì¦‰ ì´ëŸ¬í•œ [transfer learning](https://yerimoh.github.io/DL12/)ì€ NLPì— í° ë°œì „ì„ ê°€ì ¸ì™”ë‹¤.    

<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ì˜ ëª©ì ]**</span>    
* ëª¨ë“  í…ìŠ¤íŠ¸ ê¸°ë°˜ ì–¸ì–´ ë¬¸ì œë¥¼ text-to-text í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ <span style="background-color:#fff5b1">NLPì— ëŒ€í•œ [transfer learning](https://yerimoh.github.io/DL12/)ë¥¼ ì „ë°˜ì ìœ¼ë¡œ íƒêµ¬</span>í•œë‹¤.     
* ìœ„ íƒêµ¬ë¥¼ í†µí•´ ì–»ì€ í†µì°°ë ¥ê³¼ ê·œëª¨ ë° ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•  <span style="background-color:#fff5b1">"Colossal Clean Crawled Corpus"ë¥¼ ê²°í•©</span>í•˜ì—¬ ë§ì€ NLP evaluationì—ì„œ SOTA ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.        
â¡ ê³µê°œí•œ [data set, pre-trained models, and code](https://github.com/google-research/text-to-text-transfer-transformer) ë§í¬ë¥¼ ì²¨ë¶€í•˜ì˜€ë‹¤.        


<details>
<summary>ğŸ“œ Downstream Task ì˜ë¯¸ ë³´ê¸°</summary>
<div markdown="1">
  

êµ¬ì²´ì ìœ¼ë¡œ í’€ê³  ì‹¶ì€ ë¬¸ì œë“¤ì„ ë§í•œë‹¤.

NLPì—ì„œëŠ” ì–¸ì–´ëª¨ë¸ì„ pre-trainë°©ì‹ì„ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ê³ ,    
ê·¸ í›„ì— ì›í•˜ê³ ì í•˜ëŠ” taskë¥¼ fine-tuningí•˜ëŠ” ë°©ì‹ì„ í†µí•´ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ë•Œ, taskë¥¼ Downstream Taskë¼ í•œë‹¤.

ì˜ˆë¥¼ë“¤ì–´, BERTì˜ ì–¸ì–´ëª¨ë¸ì„ ì§ˆì˜ì‘ë‹µ Taskë¼ì¸ squadë¥¼ í•™ìŠµí•œë‹¤ê³  í• ë•Œ, ì´ë•Œ ì§ˆì˜ì‘ë‹µ Taskë¥¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ Taskë¡œ ë³¼ ìˆ˜ ìˆì„ê²ƒì´ë‹¤.  
  
  
</div>
</details>  

----
----



# Introduction

<span style="background-color:#F5F5F5">**[ìµœê·¼ ì—°êµ¬ ë™í–¥]**</span>    
ìì—°ì–´ ì²˜ë¦¬(NLP)ì‘ì„ ì„ ìœ„í•œ trainì´ ê°€ëŠ¥í•˜ì—¬ë©´ ëª¨ë¸ì´ **downstream learningì— ì í•©í•œ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬**í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.              
ê·¸ëŸ°ë° ì´ëŠ” **ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ "ì´í•´"í•  ìˆ˜ ìˆê²Œ í•™ìŠµí•œë‹¤ê³  ë³´ê¸°ì—” í˜ë“¤ë‹¤**.               
â¡ í˜„ëŒ€ ê¸°ê³„ í•™ìŠµ ê´€í–‰ì—ì„œ ì´ëŸ¬í•œ trainì´ ëª…ì‹œì ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” ê²½ìš°ê°€ ê±°ì˜ ì—†ì–´, ëŒ€ì‹  **Auxiliary Taskì˜ ì¼ë¶€**ë¡œ í•™ìŠµë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.        


<details>
<summary>ğŸ“œ Auxiliary Task ì˜ë¯¸ ë³´ê¸°</summary>
<div markdown="1">

ë³¸ taskëŠ” ì•„ë‹ˆì§€ë§Œ, ë³¸ taskì—ì„œì˜ ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë³´ì¡° task  

</div>
</details> 
  
ìµœê·¼ì—ëŠ” **ë°ì´í„°ê°€ í’ë¶€í•œ ì‘ì—…**ì— ëŒ€í•´ **ì „ì²´ ëª¨ë¸ì„ pre-trainí•˜ëŠ” ê²ƒ**ì´ ì ì  ë” **ì¼ë°˜í™”**ë˜ê³  ìˆë‹¤.     
ì´ìƒì ìœ¼ë¡œ, ì´ pre-trainì€ ëª¨ë¸ì´ ë²”ìš© ëŠ¥ë ¥ê³¼ ì§€ì‹ì„ ê°œë°œí•˜ê²Œ í•˜ê³ , ì´ë¥¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ìœ¼ë¡œ ì´ì „í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.     
â¡ ë” í° ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë” í° ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒë§Œìœ¼ë¡œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.    




<span style="background-color:#F5F5F5">**[í˜„ ê²½í–¥ìœ¼ë¡œ ì¸í•œ í•œê³„]**</span>    
ì´ëŸ¬í•œ ê²½í–¥ìœ¼ë¡œ ì¸í•´ NLPì— ëŒ€í•œ **transfer learning ë°©ë²•ë¡ ì„ ê°œë°œ**í•˜ëŠ” ì—°êµ¬ë“¤ì´ í™œë°œí•˜ê²Œ ì´ë£¨ì–´ì¡Œë‹¤.      
ì´ë ‡ê²Œ ì´ ë¶„ì•¼ê°€ ê¸‰ì„±ì¥í•˜ì—¬ ì—°êµ¬ê°€ í™œë°œí•´ì§€ë‹ˆ ì•„ë˜ì™€ ê°™ì€ ì‘ì—…ë“¤ì´ ì–´ë ¤ì›Œì¡Œë‹¤.       
* ì—¬ëŸ¬ algorithmsì„ ë¹„êµ    
* ìƒˆë¡œìš´ contributionsì˜ íš¨ê³¼ íŒŒì•…    
* transfer learningì„ ìœ„í•œ ê¸°ì¡´ ë°©ë²•ì˜ space ì´í•´        


âœ” ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì€ ì´ ë¶„ì•¼ì˜ ì›í™œí•œ ì´í•´ë¥¼ ìœ„í•´, **ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ì„ ì²´ê³„ì ìœ¼ë¡œ ì—°êµ¬**í•˜ê³  **í•„ë“œì˜ í˜„ì¬ í•œê³„ë¥¼ ë°€ì–´ë‚¼ ìˆ˜ ìˆëŠ” transfer learningì— ëŒ€í•œ í†µí•©ëœ ì ‘ê·¼ë²•ì„ í™œìš©**í•œë‹¤. 
 


<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ í•´ê²°ì±…]**</span>      
ë³¸ ë…¼ë¬¸ workì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” <span style="background-color:#fff5b1">ëª¨ë“  í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¬¸ì œë¥¼ **â€œtext-to-textâ€ ë¬¸ì œë¡œ ì²˜ë¦¬**</span>í•˜ëŠ” ê²ƒì´ë‹¤.             
ì¦‰, **í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ë“¤ì´ê³  ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥ìœ¼ë¡œ ìƒì„±**í•˜ëŠ” ê²ƒì´ë‹¤.          
* ì´ëŸ¬í•œ í…ìŠ¤íŠ¸ ê°„ í”„ë ˆì„ì›Œí¬ëŠ” ìš°ë¦¬ê°€ ê³ ë ¤í•˜ëŠ” ëª¨ë“  ì‘ì—…ì— **ë™ì¼í•œ ëª¨ë¸, ëª©í‘œ, í›ˆë ¨ ì ˆì°¨ ë° decoding í”„ë¡œì„¸ìŠ¤ë¥¼ ì§ì ‘ ì ìš©**í•  ìˆ˜ ìˆê²Œ í•œë‹¤.          
* ë³¸ ë…¼ë¬¸ì€ ì§ˆë¬¸ ë‹µë³€, ë¬¸ì„œ ìš”ì•½ ë° ê°ì • ë¶„ë¥˜ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ì˜ì–´ ê¸°ë°˜ NLP ë¬¸ì œì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€í•˜ì—¬ ì´ëŸ¬í•œ **ìœ ì—°ì„±ì„ í™œìš©**í•œë‹¤.        
* ì´ í†µí•© ì ‘ê·¼ë²•ì„ í†µí•´, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ transfer learningì˜ target, ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°ì´í„° ì„¸íŠ¸ ë° ê¸°íƒ€ ìš”ì¸ì˜ íš¨ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ë™ì‹œì— ì´ì „ì— ê³ ë ¤ë˜ì—ˆë˜ ê²ƒ ì´ìƒìœ¼ë¡œ **ëª¨ë¸ê³¼ ë°ì´í„° ì„¸íŠ¸ë¥¼ í™•ì¥**í•˜ì—¬ NLPì— ëŒ€í•œ **transfer learningì˜ í•œê³„ë¥¼ íƒìƒ‰**í•  ìˆ˜ ìˆë‹¤.    




ë³¸ ë…¼ë¬¸ì€ ë³¸ ë…¼ë¬¸ì˜ ëª©í‘œê°€ ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ê·¸ ë¶„ì•¼ê°€ ì–´ë””ì— ì„œ ìˆëŠ”ì§€ì— ëŒ€í•œ í¬ê´„ì ì¸ ê´€ì ì„ ì œê³µí•˜ëŠ” ê²ƒ**ì„ì„ ê°•ì¡°í•œë‹¤.      
ì¦‰, ìš°ë¦¬ì˜ ì‘ì—…ì€ ë³¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ taskë¡œ êµ¬ì„±ëœë‹¤.         
â¡ ì£¼ë¡œ ê¸°ì¡´ ê¸°ìˆ ì˜ ì¡°ì‚¬, íƒêµ¬ ë° ê²½í—˜ì  ë¹„êµ       


ë˜í•œ ë³¸ ë…¼ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ **í˜„ì¬ ì ‘ê·¼ ë°©ì‹ì˜ í•œê³„ë¥¼ íƒêµ¬**í•œë‹¤.        
ë³¸ ë…¼ë¬¸ì´ ê³ ë ¤í•˜ëŠ” ë§ì€ taskì—ì„œ SOTA ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì²´ê³„ì ì¸ ì—°êµ¬(train ëª¨ë¸ì„ ìµœëŒ€ 110ì–µ ê°œ parametersê¹Œì§€ í™•ì¥)ìˆ˜í–‰       
* ì´ ê·œëª¨ì˜ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì›¹ì—ì„œ ê¸ì–´ë‚¸ ìˆ˜ë°± ê¸°ê°€ë°”ì´íŠ¸ì˜ clean ì˜ì–´ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ë°ì´í„° ì„¸íŠ¸ì¸ "Colossal Clean Crawled Corpus"**(C4)** ë¥¼ ì†Œê°œí•œë‹¤.        
* **transfer learningì˜ í•µì‹¬ ê¸°ëŠ¥**ì€, **ë°ì´í„°ê°€ ë¶€ì¡±í•œ í™˜ê²½ì—ì„œ  pre-trained modelsì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±**ì´ë¼ëŠ” ê²ƒì„ ì¸ì‹í•˜ì—¬,      
[ì½”ë“œ, ë°ì´í„° ì„¸íŠ¸ ë° ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸](https://github.com/google-research/text-to-text-transfer-transformer
)ì„ ë¦´ë¦¬ìŠ¤í–ˆë‹¤.    




<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ êµ¬ì„±]**</span>            
* base modelê³¼ ê·¸ êµ¬í˜„    
* ëª¨ë“  text processing ë¬¸ì œë¥¼ text-to-text ì‘ì—…ìœ¼ë¡œ ê³µì‹í™”í•˜ëŠ” ì ˆì°¨ ë° ê³ ë ¤í•˜ëŠ” ì‘ì—… ëª¨ìŒì— ëŒ€í•œ ë…¼ì˜    
* ì„¹ì…˜ 3ì—ì„œ, NLPì— ëŒ€í•œ transfer learning ë¶„ì•¼ë¥¼ íƒêµ¬í•˜ëŠ” ëŒ€ê·œëª¨ ì‹¤í—˜ ì„¸íŠ¸ë¥¼ ì œì‹œ          
* ì„¹ì…˜(ì„¹ì…˜ 3.7)ì˜ ëì—ì„œ, ìš°ë¦¬ëŠ” ì²´ê³„ì ì¸ ì—°êµ¬ì˜ í†µì°°ë ¥ì„ ê²°í•©í•˜ì—¬ ê´‘ë²”ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ì–»ìŒ    
* ì„¹ì…˜ 4ì—ì„œ, ê²°ê³¼ì— ëŒ€í•œ ìš”ì•½ì„ ì œê³µ í•œ ë’¤, ë¯¸ë˜ì— ëŒ€í•œ ì „ë§ìœ¼ë¡œ ë§ˆë¬´ë¦¬             




---
---


# 2. Setup    



ë³¸ ë…¼ë¬¸ì€ large-scale ê²½í—˜ì  ì—°êµ¬ì˜ ê²°ê³¼ë¥¼ ì œì‹œí•˜ê¸° ì „ì— ì•„ë˜ì™€ ê°™ì€ ê²ƒë“¤ì„ ë¨¼ì € ì œì‹œí•œë‹¤,   
* [Transformer ëª¨ë¸ ì•„í‚¤í…ì²˜](https://yerimoh.github.io/Lan/)ì™€ í‰ê°€í•˜ëŠ” downstream tasksì„ í¬í•¨í•˜ì—¬ ê²°ê³¼ë¥¼ ì´í•´í•˜ëŠ” ë° í•„ìš”í•œ **ë°°ê²½ ì£¼ì œë¥¼ ê²€í† **í•œë‹¤.        
* **ëª¨ë“  ë¬¸ì œë¥¼ text-to-text taskìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì ‘ê·¼ ë°©ì‹**ì„ ì†Œê°œ    
* **"Colossal Clean Crawled Corpus"(C4) ì„¤ëª…:** ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì†ŒìŠ¤ë¡œ ìƒì„±í•œ ê³µí†µ í¬ë¡¤ ê¸°ë°˜ ë°ì´í„° ì„¸íŠ¸ì„     




ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ëª¨ë¸ê³¼ í”„ë ˆì„ì›Œí¬ë¥¼  <span style="background-color:#fff5b1">â€œText-to-Text Transfer Transformerâ€ (T5)</span> ë¼ê³  ë¶€ë¥¸ë‹¤.      





---


## 2.1 Model


NLPì— ëŒ€í•œ ì „ì´ í•™ìŠµì— ëŒ€í•œ ì´ˆê¸° ê²°ê³¼ëŠ” ë°˜ë³µ ì‹ ê²½ë§ì„ í™œìš©í–ˆì§€ë§Œ,  
ìµœê·¼ì—ëŠ” "Transformer" ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¼ë°˜í™”ë˜ì—ˆë‹¤.     


TransformerëŠ” transfer learningì— íš¨ê³¼ì ì´ë¯€ë¡œ, ì´í›„ ë‹¤ì–‘í•œ NLP ì„¤ì •ì— ì‚¬ìš©ë˜ì—ˆë‹¤.          
ë³¸ ë…¼ë¬¸ì—ì„œë„ **ëª¨ë“  ëª¨ë¸ì˜ baseë¥¼ Transformer ì•„í‚¤í…ì²˜**ë¡œ í•˜ì˜€ë‹¤.         

ì•„ë˜ì— ì–¸ê¸‰ëœ ì„¸ë¶€ì‚¬í•­ê³¼ ì„¹ì…˜ 3.2ì—ì„œ íƒêµ¬í•œ ë³€í˜•ì„ ì œì™¸í•˜ê³ ,     
ë³¸ ë…¼ë¬¸ì€ Transformer ì•„í‚¤í…ì²˜ì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.      


<details>
<summary>ğŸ“œ Transformerë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´? </summary>
<div markdown="1">

ì´ ë…¼ë¬¸(ì•„í‚¤í…ì²˜)ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³ ì‹¶ë‹¤ë©´ ì•„ë˜ ìë£Œë“¤ì„ ì°¸ê³ í•˜ì,    
* [ì›ë³¸ ë…¼ë¬¸](https://arxiv.org/abs/1706.03762)    
* í›„ì† íŠœí† ë¦¬ì–¼ 3,4(ë’¤ì— ë‚˜ì˜µë‹ˆë‹¤.)    
* [ë³¸ í¬ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•œ í•„ìê°€ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸](https://yerimoh.github.io/Lan/)         

</div>
</details> 
  


The primary building block of the Transformer is self-attention (Cheng et al., 2016).
Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes
a sequence by replacing each element by a weighted average of the rest of the sequence.
The original Transformer consisted of an encoder-decoder architecture and was intended
for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has
recently also become common to use models consisting of a single Transformer layer stack,
with varying forms of self-attention used to produce architectures appropriate for language
modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction
tasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural
variants in Section 3.2.



Transformerì˜ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” self-attentionì´ë‹¤.         
ìê¸°ì£¼ì˜ëŠ” ê° ìš”ì†Œë¥¼ ë‚˜ë¨¸ì§€ ì‹œí€€ìŠ¤ì˜ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì£¼ì˜ì˜ ë³€í˜•ì´ë‹¤(Graves, 2013; Bahdanau et al., 2015). ì›ë˜ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì¸ì½”ë”-ë””ì½”ë” ì•„í‚¤í…ì²˜ë¡œ êµ¬ì„±ë˜ì—ˆìœ¼ë©° ì‹œí€€ìŠ¤ ëŒ€ ì‹œí€€ìŠ¤ ì‘ì—…ì„ ìœ„í•´ ê³ ì•ˆë˜ì—ˆë‹¤(Sutskever et al., 2014; Kalchbrenner et al., 2014). ë˜í•œ ìµœê·¼ì—ëŠ” ì–¸ì–´ ëª¨ë¸ë§(Radford et al., 2018; Al-Rfou et al., 2019) ë˜ëŠ” ë¶„ë¥˜ ë° ë²”ìœ„ ì˜ˆì¸¡ ì‘ì—…(Devlin et al., 2018; Yang et al., 2019)ì— ì í•©í•œ ì•„í‚¤í…ì²˜ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë‹¤ì–‘í•œ í˜•íƒœì˜ ìì²´ ì£¼ì˜ë¥¼ í†µí•´ ë‹¨ì¼ Transformer ë ˆì´ì–´ ìŠ¤íƒìœ¼ë¡œ êµ¬ì„±ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜í™”ë˜ì—ˆë‹¤. ìš°ë¦¬ëŠ” ì„¹ì…˜ 3.2ì—ì„œ ì´ëŸ¬í•œ ì•„í‚¤í…ì²˜ ë³€í˜•ì„ ê²½í—˜ì ìœ¼ë¡œ íƒêµ¬í•œë‹¤


Overall, our encoder-decoder Transformer implementation closely follows its originallyproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to
a sequence of embeddings, which is then passed into the encoder. The encoder consists
of a stack of â€œblocksâ€, each of which comprises two subcomponents: a self-attention layer
followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to
the input of each subcomponent. We use a simplified version of layer normalization where
the activations are only rescaled and no additive bias is applied. After layer normalization,
a residual skip connection (He et al., 2016) adds each subcomponentâ€™s input to its output.
Dropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip
connection, on the attention weights, and at the input and output of the entire stack. The
decoder is similar in structure to the encoder except that it includes a standard attention
mechanism after each self-attention layer that attends to the output of the encoder. The
self-attention mechanism in the decoder also uses a form of autoregressive or causal selfattention, which only allows the model to attend to past outputs. The output of the final
decoder block is fed into a dense layer with a softmax output, whose weights are shared with
the input embedding matrix. All attention mechanisms in the Transformer are split up into
independent â€œheadsâ€ whose outputs are concatenated before being further processed.



ì „ë°˜ì ìœ¼ë¡œ, ìš°ë¦¬ì˜ ì¸ì½”ë”-ë””ì½”ë” íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬í˜„ì€ ì›ë˜ ì œì•ˆëœ í˜•íƒœë¥¼ ë°€ì ‘í•˜ê²Œ ë”°ë¥¸ë‹¤(Vaswani et al., 2017). ë¨¼ì € í† í°ì˜ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì„ë² ë”© ì‹œí€€ìŠ¤ì— ë§¤í•‘í•œ ë‹¤ìŒ ì¸ì½”ë”ë¡œ ì „ë‹¬í•œë‹¤. ì¸ì½”ë”ëŠ” ë¸”ë¡ì˜ ìŠ¤íƒìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê°ê°ì€ ë‘ ê°œì˜ í•˜ìœ„ êµ¬ì„± ìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤: ìê¸° ì£¼ì˜ ê³„ì¸µê³¼ ì‘ì€ í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬. ê° í•˜ìœ„ êµ¬ì„±ìš”ì†Œì˜ ì…ë ¥ì—ëŠ” ê³„ì¸µ ì •ê·œí™”(Ba et al., 2016)ê°€ ì ìš©ëœë‹¤. í™œì„±í™”ê°€ ì¬ì¡°ì •ë˜ê³  ì¶”ê°€ í¸í–¥ì´ ì ìš©ë˜ì§€ ì•ŠëŠ” ë‹¨ìˆœí™”ëœ ë²„ì „ì˜ ê³„ì¸µ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•œë‹¤. ë ˆì´ì–´ ì •ê·œí™” í›„, ì”ì—¬ ìŠ¤í‚µ ì—°ê²°(He et al., 2016)ì€ ê° í•˜ìœ„ êµ¬ì„±ìš”ì†Œì˜ ì…ë ¥ì„ ì¶œë ¥ì— ì¶”ê°€í•œë‹¤. ë“œë¡­ì•„ì›ƒ(Srivastava et al., 2014)ì€ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë‚´ì—ì„œ ìŠ¤í‚µ ì—°ê²°, ì£¼ì˜ ê°€ì¤‘ì¹˜ ë° ì „ì²´ ìŠ¤íƒì˜ ì…ë ¥ ë° ì¶œë ¥ì— ì ìš©ëœë‹¤. ë””ì½”ë”ëŠ” ì¸ì½”ë”ì˜ ì¶œë ¥ì— ì°¸ì—¬í•˜ëŠ” ê° ìê¸° ì£¼ì˜ ê³„ì¸µ ë‹¤ìŒì— í‘œì¤€ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•œë‹¤ëŠ” ì ì„ ì œì™¸í•˜ê³ ëŠ” ì¸ì½”ë”ì™€ êµ¬ì¡°ê°€ ìœ ì‚¬í•˜ë‹¤. ë””ì½”ë”ì˜ ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ë˜í•œ ëª¨ë¸ì´ ê³¼ê±° ì¶œë ¥ì—ë§Œ ì£¼ì˜ë¥¼ ê¸°ìš¸ì¼ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ìê¸° íšŒê·€ ë˜ëŠ” ì¸ê³¼ì  ìê¸° ì£¼ì˜ì˜ í˜•íƒœë¥¼ ì‚¬ìš©í•œë‹¤. ìµœì¢… ë””ì½”ë” ë¸”ë¡ì˜ ì¶œë ¥ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥ì„ ê°€ì§„ ê³ ë°€ë„ ë ˆì´ì–´ë¡œ ê³µê¸‰ë˜ë©°, ê·¸ ê°€ì¤‘ì¹˜ëŠ” ì…ë ¥ ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ì™€ ê³µìœ ëœë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ëª¨ë“  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì¶”ê°€ë¡œ ì²˜ë¦¬ë˜ê¸° ì „ì— ì¶œë ¥ì´ ì—°ê²°ë˜ëŠ” ë…ë¦½ì ì¸ "í—¤ë“œ"ë¡œ ë¶„í• ë©ë‹ˆë‹¤.


Since self-attention is order-independent (i.e. it is an operation on sets), it is common
to provide an explicit position signal to the Transformer. While the original Transformer
used a sinusoidal position signal or learned position embeddings, it has recently become
more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).
Instead of using a fixed embedding for each position, relative position embeddings produce
a different learned embedding according to the offset between the â€œkeyâ€ and â€œqueryâ€ being
compared in the self-attention mechanism. We use a simplified form of position embeddings
where each â€œembeddingâ€ is simply a scalar that is added to the corresponding logit used
for computing the attention weights. For efficiency, we also share the position embedding
parameters across all layers in our model, though within a given layer each attention head
uses a different learned position embedding. Typically, a fixed number of embeddings are
learned, each corresponding to a range of possible key-query offsets. In this work, we use 32
embeddings for all of our models with ranges that increase in size logarithmically up to an
offset of 128 beyond which we assign all relative positions to the same embedding. Note
that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers
can build a sensitivity to larger offsets by combining local information from previous layers.
To summarize, our model is roughly equivalent to the original Transformer proposed by
Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer
normalization outside the residual path, and using a different position embedding scheme.
Since these architectural changes are orthogonal to the experimental factors we consider in
our empirical survey of transfer learning, we leave the ablation of their impact for future
work.




ìê°€ ì£¼ì˜ëŠ” ìˆœì„œì— ì˜ì¡´í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—(ì¦‰, ì„¸íŠ¸ì— ëŒ€í•œ ì‘ì—…), ë³€ì••ê¸°ì— ëª…ì‹œì  ìœ„ì¹˜ ì‹ í˜¸ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì›ë˜ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì‚¬ì¸íŒŒ ìœ„ì¹˜ ì‹ í˜¸ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ìœ„ì¹˜ ì„ë² ë”©ì„ í•™ìŠµí–ˆì§€ë§Œ, ìµœê·¼ì—ëŠ” ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¼ë°˜í™”ë˜ì—ˆë‹¤(Shaw et al., 2018; Huang et al., 2018a). ê° ìœ„ì¹˜ì— ê³ ì •ëœ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , ìƒëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì€ ìê¸° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì—ì„œ ë¹„êµë˜ëŠ” "í‚¤"ì™€ "ì¿¼ë¦¬" ì‚¬ì´ì˜ ì˜¤í”„ì…‹ì— ë”°ë¼ ë‹¤ë¥¸ í•™ìŠµ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤. ìš°ë¦¬ëŠ” ê° "ì„ë² ë”©"ì´ ì£¼ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í•´ë‹¹ ë¡œì§“ì— ì¶”ê°€ë˜ëŠ” ë‹¨ìˆœí•œ í˜•íƒœì˜ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•œë‹¤. íš¨ìœ¨ì„±ì„ ìœ„í•´, ìš°ë¦¬ëŠ” ë˜í•œ ì£¼ì–´ì§„ ê³„ì¸µ ë‚´ì—ì„œ ê° ì£¼ì˜ í—¤ë“œê°€ ë‹¤ë¥¸ í•™ìŠµëœ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ìš°ë¦¬ ëª¨ë¸ì˜ ëª¨ë“  ê³„ì¸µì— ê±¸ì³ ìœ„ì¹˜ ì„ë² ë”© ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê³µìœ í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ, ê°ê° ê°€ëŠ¥í•œ í‚¤ ì¿¼ë¦¬ ì˜¤í”„ì…‹ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ê³ ì •ëœ ìˆ˜ì˜ ì„ë² ë”©ì´ í•™ìŠµëœë‹¤. ì´ ì—°êµ¬ì—ì„œ, ìš°ë¦¬ëŠ” í¬ê¸°ê°€ ìµœëŒ€ 128ì˜ ì˜¤í”„ì…‹ê¹Œì§€ ëŒ€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ë²”ìœ„ë¥¼ ê°€ì§„ ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ 32ê°œì˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë©°, ì´ë¥¼ ì´ˆê³¼í•˜ì—¬ ëª¨ë“  ìƒëŒ€ ìœ„ì¹˜ë¥¼ ë™ì¼í•œ ì„ë² ë”©ì— í• ë‹¹í•œë‹¤. ì£¼ì–´ì§„ ê³„ì¸µì€ 128 í† í° ì´ìƒì˜ ìƒëŒ€ì  ìœ„ì¹˜ì— ë‘”ê°í•˜ì§€ë§Œ, í›„ì† ê³„ì¸µì€ ì´ì „ ê³„ì¸µì˜ ë¡œì»¬ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ë” í° ì˜¤í”„ì…‹ì— ëŒ€í•œ ë¯¼ê°ë„ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤. ìš”ì•½í•˜ë©´, ìš°ë¦¬ì˜ ëª¨ë¸ì€ ë ˆì´ì–´ í‘œì¤€ í¸í–¥ì„ ì œê±°í•˜ê³ , ë ˆì´ì–´ ì •ê·œí™”ë¥¼ ì”ë¥˜ ê²½ë¡œ ì™¸ë¶€ì— ë°°ì¹˜í•˜ê³ , ë‹¤ë¥¸ ìœ„ì¹˜ ì„ë² ë”© ì²´ê³„ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì ì„ ì œì™¸í•˜ê³  Vaswani ë“±(2017)ì´ ì œì•ˆí•œ ì›ë˜ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ëŒ€ëµ ë™ì¼í•˜ë‹¤. ì´ëŸ¬í•œ ì•„í‚¤í…ì²˜ ë³€í™”ëŠ” ì „ì´ í•™ìŠµì— ëŒ€í•œ ê²½í—˜ì  ì¡°ì‚¬ì—ì„œ ê³ ë ¤í•˜ëŠ” ì‹¤í—˜ì  ìš”ì¸ê³¼ ì§êµí•˜ê¸° ë•Œë¬¸ì— í–¥í›„ ì‘ì—…ì„ ìœ„í•´ ì˜í–¥ì˜ ì ˆì œë¥¼ ë‚¨ê²¨ë‘”ë‹¤.



As part of our study, we experiment with the scalability of these models, i.e. how their
performance changes as they are made to have more parameters or layers. Training large
models can be non-trivial since they might not fit on a single machine and require a great deal
of computation. As a result, we use a combination of model and data parallelism and train
models on â€œslicesâ€ of Cloud TPU Pods.5 TPU pods are are multi-rack ML supercomputers
that contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with
supporting CPU host machines. We leverage the Mesh TensorFlow library (Shazeer et al.,
2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky,
2014).



ìš°ë¦¬ì˜ ì—°êµ¬ì˜ ì¼í™˜ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ëª¨ë¸ì˜ í™•ì¥ì„±, ì¦‰ ë” ë§ì€ ë§¤ê°œ ë³€ìˆ˜ë‚˜ ê³„ì¸µì„ ê°€ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‹¤í—˜í•œë‹¤. ëŒ€ê·œëª¨ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒì€ ë‹¨ì¼ ì‹œìŠ¤í…œì— ë§ì§€ ì•Šì„ ìˆ˜ ìˆê³  ë§ì€ ê³„ì‚°ì´ í•„ìš”í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì‚¬ì†Œí•œ ì¼ì´ ì•„ë‹ ìˆ˜ ìˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ëª¨ë¸ê³¼ ë°ì´í„° ë³‘ë ¬í™”ë¥¼ ê²°í•©í•˜ì—¬ Cloud TPU Podì˜ "ì¡°ê°"ì—ì„œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¨ë‹¤.5 TPU í¬ë“œëŠ” CPU í˜¸ìŠ¤íŠ¸ ì‹œìŠ¤í…œì„ ì§€ì›í•˜ëŠ” ê³ ì† 2D ë©”ì‹œ ì¸í„°ì»¤ë„¥íŠ¸ë¥¼ í†µí•´ ì—°ê²°ëœ 1,024ê°œì˜ TPU v3 ì¹©ì„ í¬í•¨í•˜ëŠ” ë©€í‹° ë™ ML ìŠˆí¼ì»´í“¨í„°ì´ë‹¤. ëª¨ë¸ ë³‘ë ¬í™”ì™€ ë°ì´í„° ë³‘ë ¬í™”ì˜ ìš©ì´í•œ êµ¬í˜„ì„ ìœ„í•´ Mesh TensorFlow ë¼ì´ë¸ŒëŸ¬ë¦¬(Shazeer et al., 2018)ë¥¼ í™œìš©í•œë‹¤(Krizhevsky, 2014).
