---
title: "(T5) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ì •ë¦¬"
date:   2023-02-10
excerpt: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer paper review"
category: #[Paper]
layout: post
tag:
#- Paper
order: 0

comments: true
---


# Abstract
<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ ë°°ê²½]**</span>    
ëª¨ë¸ì´ Downstream Taskì—ì„œ fine-tuningë˜ê¸° ì „ì— data-rich taskì—ì„œ pre-trainì„ í•˜ëŠ” ê²ƒì€  ìì—°ì–´ ì²˜ë¦¬(NLP)ì˜ ê°•ë ¥í•œ ê¸°ìˆ ë¡œ ë¶€ìƒí–ˆë‹¤.     
ì¦‰ ì´ëŸ¬í•œ [transfer learning](https://yerimoh.github.io/DL12/)ì€ NLPì— í° ë°œì „ì„ ê°€ì ¸ì™”ë‹¤.    

<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ì˜ ëª©ì ]**</span>    
* ëª¨ë“  í…ìŠ¤íŠ¸ ê¸°ë°˜ ì–¸ì–´ ë¬¸ì œë¥¼ text-to-text í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í†µí•© í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ <span style="background-color:#fff5b1">NLPì— ëŒ€í•œ [transfer learning](https://yerimoh.github.io/DL12/)ë¥¼ ì „ë°˜ì ìœ¼ë¡œ íƒêµ¬</span>í•œë‹¤.     
* ìœ„ íƒêµ¬ë¥¼ í†µí•´ ì–»ì€ í†µì°°ë ¥ê³¼ ê·œëª¨ ë° ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•  <span style="background-color:#fff5b1">"Colossal Clean Crawled Corpus"ë¥¼ ê²°í•©</span>í•˜ì—¬ ë§ì€ NLP evaluationì—ì„œ SOTA ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.        
â¡ ê³µê°œí•œ [data set, pre-trained models, and code](https://github.com/google-research/text-to-text-transfer-transformer) ë§í¬ë¥¼ ì²¨ë¶€í•˜ì˜€ë‹¤.        


<details>
<summary>ğŸ“œ Downstream Task ì˜ë¯¸ ë³´ê¸°</summary>
<div markdown="1">
  

êµ¬ì²´ì ìœ¼ë¡œ í’€ê³  ì‹¶ì€ ë¬¸ì œë“¤ì„ ë§í•œë‹¤.

NLPì—ì„œëŠ” ì–¸ì–´ëª¨ë¸ì„ pre-trainë°©ì‹ì„ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ê³ ,    
ê·¸ í›„ì— ì›í•˜ê³ ì í•˜ëŠ” taskë¥¼ fine-tuningí•˜ëŠ” ë°©ì‹ì„ í†µí•´ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ”ë° ì´ë•Œ, taskë¥¼ Downstream Taskë¼ í•œë‹¤.

ì˜ˆë¥¼ë“¤ì–´, BERTì˜ ì–¸ì–´ëª¨ë¸ì„ ì§ˆì˜ì‘ë‹µ Taskë¼ì¸ squadë¥¼ í•™ìŠµí•œë‹¤ê³  í• ë•Œ, ì´ë•Œ ì§ˆì˜ì‘ë‹µ Taskë¥¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ Taskë¡œ ë³¼ ìˆ˜ ìˆì„ê²ƒì´ë‹¤.  
  
  
</div>
</details>  

----
----



# Introduction

<span style="background-color:#F5F5F5">**[ìµœê·¼ ì—°êµ¬ ë™í–¥]**</span>    
ìì—°ì–´ ì²˜ë¦¬(NLP)ì‘ì„ ì„ ìœ„í•œ trainì´ ê°€ëŠ¥í•˜ì—¬ë©´ ëª¨ë¸ì´ **downstream learningì— ì í•©í•œ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬**í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.              
ê·¸ëŸ°ë° ì´ëŠ” **ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ "ì´í•´"í•  ìˆ˜ ìˆê²Œ í•™ìŠµí•œë‹¤ê³  ë³´ê¸°ì—” í˜ë“¤ë‹¤**.               
â¡ í˜„ëŒ€ ê¸°ê³„ í•™ìŠµ ê´€í–‰ì—ì„œ ì´ëŸ¬í•œ trainì´ ëª…ì‹œì ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” ê²½ìš°ê°€ ê±°ì˜ ì—†ì–´, ëŒ€ì‹  **Auxiliary Taskì˜ ì¼ë¶€**ë¡œ í•™ìŠµë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.        


<details>
<summary>ğŸ“œ Auxiliary Task ì˜ë¯¸ ë³´ê¸°</summary>
<div markdown="1">

ë³¸ taskëŠ” ì•„ë‹ˆì§€ë§Œ, ë³¸ taskì—ì„œì˜ ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë³´ì¡° task  

</div>
</details> 
  
ìµœê·¼ì—ëŠ” **ë°ì´í„°ê°€ í’ë¶€í•œ ì‘ì—…**ì— ëŒ€í•´ **ì „ì²´ ëª¨ë¸ì„ pre-trainí•˜ëŠ” ê²ƒ**ì´ ì ì  ë” **ì¼ë°˜í™”**ë˜ê³  ìˆë‹¤.     
ì´ìƒì ìœ¼ë¡œ, ì´ pre-trainì€ ëª¨ë¸ì´ ë²”ìš© ëŠ¥ë ¥ê³¼ ì§€ì‹ì„ ê°œë°œí•˜ê²Œ í•˜ê³ , ì´ë¥¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ìœ¼ë¡œ ì´ì „í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.     
â¡ ë” í° ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë” í° ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒë§Œìœ¼ë¡œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.    




<span style="background-color:#F5F5F5">**[í˜„ ê²½í–¥ìœ¼ë¡œ ì¸í•œ í•œê³„]**</span>    
ì´ëŸ¬í•œ ê²½í–¥ìœ¼ë¡œ ì¸í•´ NLPì— ëŒ€í•œ **transfer learning ë°©ë²•ë¡ ì„ ê°œë°œ**í•˜ëŠ” ì—°êµ¬ë“¤ì´ í™œë°œí•˜ê²Œ ì´ë£¨ì–´ì¡Œë‹¤.      
ì´ë ‡ê²Œ ì´ ë¶„ì•¼ê°€ ê¸‰ì„±ì¥í•˜ì—¬ ì—°êµ¬ê°€ í™œë°œí•´ì§€ë‹ˆ ì•„ë˜ì™€ ê°™ì€ ì‘ì—…ë“¤ì´ ì–´ë ¤ì›Œì¡Œë‹¤.       
* ì—¬ëŸ¬ algorithmsì„ ë¹„êµ    
* ìƒˆë¡œìš´ contributionsì˜ íš¨ê³¼ íŒŒì•…    
* transfer learningì„ ìœ„í•œ ê¸°ì¡´ ë°©ë²•ì˜ space ì´í•´        


âœ” ê·¸ë˜ì„œ ë³¸ ë…¼ë¬¸ì€ ì´ ë¶„ì•¼ì˜ ì›í™œí•œ ì´í•´ë¥¼ ìœ„í•´, **ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ì„ ì²´ê³„ì ìœ¼ë¡œ ì—°êµ¬**í•˜ê³  **í•„ë“œì˜ í˜„ì¬ í•œê³„ë¥¼ ë°€ì–´ë‚¼ ìˆ˜ ìˆëŠ” transfer learningì— ëŒ€í•œ í†µí•©ëœ ì ‘ê·¼ë²•ì„ í™œìš©**í•œë‹¤. 
 


<span style="background-color:#F5F5F5">**[ë³¸ ë…¼ë¬¸ì˜ í•´ê²°ì±…]**</span>      
ë³¸ ë…¼ë¬¸ workì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” <span style="background-color:#fff5b1">ëª¨ë“  í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¬¸ì œë¥¼ **â€œtext-to-textâ€ ë¬¸ì œë¡œ ì²˜ë¦¬**</span>í•˜ëŠ” ê²ƒì´ë‹¤.             
ì¦‰, **í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ë“¤ì´ê³  ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥ìœ¼ë¡œ ìƒì„±**í•˜ëŠ” ê²ƒì´ë‹¤.          
* ì´ëŸ¬í•œ í…ìŠ¤íŠ¸ ê°„ í”„ë ˆì„ì›Œí¬ëŠ” ìš°ë¦¬ê°€ ê³ ë ¤í•˜ëŠ” ëª¨ë“  ì‘ì—…ì— **ë™ì¼í•œ ëª¨ë¸, ëª©í‘œ, í›ˆë ¨ ì ˆì°¨ ë° decoding í”„ë¡œì„¸ìŠ¤ë¥¼ ì§ì ‘ ì ìš©**í•  ìˆ˜ ìˆê²Œ í•œë‹¤.          
* ë³¸ ë…¼ë¬¸ì€ ì§ˆë¬¸ ë‹µë³€, ë¬¸ì„œ ìš”ì•½ ë° ê°ì • ë¶„ë¥˜ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ì˜ì–´ ê¸°ë°˜ NLP ë¬¸ì œì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€í•˜ì—¬ ì´ëŸ¬í•œ **ìœ ì—°ì„±ì„ í™œìš©**í•œë‹¤.        
* ì´ í†µí•© ì ‘ê·¼ë²•ì„ í†µí•´, ìš°ë¦¬ëŠ” ë‹¤ì–‘í•œ transfer learningì˜ target, ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°ì´í„° ì„¸íŠ¸ ë° ê¸°íƒ€ ìš”ì¸ì˜ íš¨ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ë™ì‹œì— ì´ì „ì— ê³ ë ¤ë˜ì—ˆë˜ ê²ƒ ì´ìƒìœ¼ë¡œ **ëª¨ë¸ê³¼ ë°ì´í„° ì„¸íŠ¸ë¥¼ í™•ì¥**í•˜ì—¬ NLPì— ëŒ€í•œ **transfer learningì˜ í•œê³„ë¥¼ íƒìƒ‰**í•  ìˆ˜ ìˆë‹¤.    




ë³¸ ë…¼ë¬¸ì€ ë³¸ ë…¼ë¬¸ì˜ ëª©í‘œê°€ ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ê·¸ ë¶„ì•¼ê°€ ì–´ë””ì— ì„œ ìˆëŠ”ì§€ì— ëŒ€í•œ í¬ê´„ì ì¸ ê´€ì ì„ ì œê³µí•˜ëŠ” ê²ƒ**ì„ì„ ê°•ì¡°í•œë‹¤.      
ì¦‰, ìš°ë¦¬ì˜ ì‘ì—…ì€ ë³¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ taskë¡œ êµ¬ì„±ëœë‹¤.         
â¡ ì£¼ë¡œ ê¸°ì¡´ ê¸°ìˆ ì˜ ì¡°ì‚¬, íƒêµ¬ ë° ê²½í—˜ì  ë¹„êµ       


ë˜í•œ ë³¸ ë…¼ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ **í˜„ì¬ ì ‘ê·¼ ë°©ì‹ì˜ í•œê³„ë¥¼ íƒêµ¬**í•œë‹¤.        
ë³¸ ë…¼ë¬¸ì´ ê³ ë ¤í•˜ëŠ” ë§ì€ taskì—ì„œ SOTA ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì²´ê³„ì ì¸ ì—°êµ¬(train ëª¨ë¸ì„ ìµœëŒ€ 110ì–µ ê°œ parametersê¹Œì§€ í™•ì¥)ìˆ˜í–‰       
* ì´ ê·œëª¨ì˜ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì›¹ì—ì„œ ê¸ì–´ë‚¸ ìˆ˜ë°± ê¸°ê°€ë°”ì´íŠ¸ì˜ clean ì˜ì–´ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ë°ì´í„° ì„¸íŠ¸ì¸ "Colossal Clean Crawled Corpus"**(C4)** ë¥¼ ì†Œê°œí•œë‹¤.        
* **transfer learningì˜ í•µì‹¬ ê¸°ëŠ¥**ì€, **ë°ì´í„°ê°€ ë¶€ì¡±í•œ í™˜ê²½ì—ì„œ  pre-trained modelsì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±**ì´ë¼ëŠ” ê²ƒì„ ì¸ì‹í•˜ì—¬,      
[ì½”ë“œ, ë°ì´í„° ì„¸íŠ¸ ë° ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸](https://github.com/google-research/text-to-text-transfer-transformer
)ì„ ë¦´ë¦¬ìŠ¤í–ˆë‹¤.    




<span style="background-color:#F5F5F5">**[ë…¼ë¬¸ êµ¬ì„±]**</span>            
* base modelê³¼ ê·¸ êµ¬í˜„    
* ëª¨ë“  text processing ë¬¸ì œë¥¼ text-to-text ì‘ì—…ìœ¼ë¡œ ê³µì‹í™”í•˜ëŠ” ì ˆì°¨ ë° ê³ ë ¤í•˜ëŠ” ì‘ì—… ëª¨ìŒì— ëŒ€í•œ ë…¼ì˜    
* ì„¹ì…˜ 3ì—ì„œ, NLPì— ëŒ€í•œ transfer learning ë¶„ì•¼ë¥¼ íƒêµ¬í•˜ëŠ” ëŒ€ê·œëª¨ ì‹¤í—˜ ì„¸íŠ¸ë¥¼ ì œì‹œ          
* ì„¹ì…˜(ì„¹ì…˜ 3.7)ì˜ ëì—ì„œ, ìš°ë¦¬ëŠ” ì²´ê³„ì ì¸ ì—°êµ¬ì˜ í†µì°°ë ¥ì„ ê²°í•©í•˜ì—¬ ê´‘ë²”ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ì–»ìŒ    
* ì„¹ì…˜ 4ì—ì„œ, ê²°ê³¼ì— ëŒ€í•œ ìš”ì•½ì„ ì œê³µ í•œ ë’¤, ë¯¸ë˜ì— ëŒ€í•œ ì „ë§ìœ¼ë¡œ ë§ˆë¬´ë¦¬             




---
---


# 2. Setup    



ë³¸ ë…¼ë¬¸ì€ large-scale ê²½í—˜ì  ì—°êµ¬ì˜ ê²°ê³¼ë¥¼ ì œì‹œí•˜ê¸° ì „ì— ì•„ë˜ì™€ ê°™ì€ ê²ƒë“¤ì„ ë¨¼ì € ì œì‹œí•œë‹¤,   
* [Transformer ëª¨ë¸ ì•„í‚¤í…ì²˜](https://yerimoh.github.io/Lan/)ì™€ í‰ê°€í•˜ëŠ” downstream tasksì„ í¬í•¨í•˜ì—¬ ê²°ê³¼ë¥¼ ì´í•´í•˜ëŠ” ë° í•„ìš”í•œ **ë°°ê²½ ì£¼ì œë¥¼ ê²€í† **í•œë‹¤.        
* **ëª¨ë“  ë¬¸ì œë¥¼ text-to-text taskìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì ‘ê·¼ ë°©ì‹**ì„ ì†Œê°œ    
* **"Colossal Clean Crawled Corpus"(C4) ì„¤ëª…:** ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ì†ŒìŠ¤ë¡œ ìƒì„±í•œ ê³µí†µ í¬ë¡¤ ê¸°ë°˜ ë°ì´í„° ì„¸íŠ¸ì„     




ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ëª¨ë¸ê³¼ í”„ë ˆì„ì›Œí¬ë¥¼  <span style="background-color:#fff5b1">â€œText-to-Text Transfer Transformerâ€ (T5)</span> ë¼ê³  ë¶€ë¥¸ë‹¤.      





---


## 2.1 Model

### base architecture: â€œTransformerâ€
NLPì— ëŒ€í•œ ì „ì´ í•™ìŠµì— ëŒ€í•œ ì´ˆê¸° ê²°ê³¼ëŠ” ë°˜ë³µ ì‹ ê²½ë§ì„ í™œìš©í–ˆì§€ë§Œ,  
ìµœê·¼ì—ëŠ” "Transformer" ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¼ë°˜í™”ë˜ì—ˆë‹¤.     


TransformerëŠ” transfer learningì— íš¨ê³¼ì ì´ë¯€ë¡œ, ì´í›„ ë‹¤ì–‘í•œ NLP ì„¤ì •ì— ì‚¬ìš©ë˜ì—ˆë‹¤.          
ë³¸ ë…¼ë¬¸ì—ì„œë„ **ëª¨ë“  ëª¨ë¸ì˜ baseë¥¼ Transformer ì•„í‚¤í…ì²˜**ë¡œ í•˜ì˜€ë‹¤.         

ì•„ë˜ì— ì–¸ê¸‰ëœ ì„¸ë¶€ì‚¬í•­ê³¼ ì„¹ì…˜ 3.2ì—ì„œ íƒêµ¬í•œ ë³€í˜•ì„ ì œì™¸í•˜ê³ ,     
ë³¸ ë…¼ë¬¸ì€ Transformer ì•„í‚¤í…ì²˜ì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.      


<details>
<summary>ğŸ“œ Transformerë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´? (ì°¸ê³ ìë£Œ) </summary>
<div markdown="1">

ì´ ë…¼ë¬¸(ì•„í‚¤í…ì²˜)ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³ ì‹¶ë‹¤ë©´ ì•„ë˜ ìë£Œë“¤ì„ ì°¸ê³ í•˜ì,    
* [ì›ë³¸ ë…¼ë¬¸](https://arxiv.org/abs/1706.03762)    
* í›„ì† íŠœí† ë¦¬ì–¼ 3,4(ë’¤ì— ë‚˜ì˜µë‹ˆë‹¤.)    
* [ë³¸ í¬ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•œ í•„ìê°€ ì •ë¦¬í•œ í¬ìŠ¤íŠ¸](https://yerimoh.github.io/Lan/)         

</div>
</details> 
  85.5


ê·¸ë¦¬ê³  ë³¸ ë…¼ë¬¸ì—ì„œ Transformerì—ëŒ€í•´ ê°„ë‹¨ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ì¤¬ëŠ”ë° ì›í•œë‹¤ë©´ ì•„ë˜ ìì„¸í•œ ì„¤ëª… ë³´ê¸°ë¥¼ ëˆŒëŸ¬ í•œ ë²ˆ ì½ì–´ë³´ê¸¸ ë°”ë€ë‹¤.(ê·¸ë ‡ì§€ë§Œ ì› ë…¼ë¬¸ì„ ì½ì–´ë´ì•¼ ì•„ë˜ ë‚´ìš©ë“¤ë„ ì´í•´ê°€ ê°ˆ ê²ƒì´ë‹¤.)   

<details>
<summary>ğŸ“œ Transformerë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´?(ë³¸ ë…¼ë¬¸ì˜ ì„¤ëª…) </summary>
<div markdown="1">




 
**Transformerì˜ ì¤‘ìš”í•œ ìš”ì†ŒëŠ”,** [self-attention](https://yerimoh.github.io/Lan/#sub-layer1-self-attention)ì´ë‹¤.         
self-attentionì€ ê° ìš”ì†Œë¥¼ ë‚˜ë¨¸ì§€ sequenceì˜ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” attentionì˜ ë³€í˜•ì´ë‹¤.    

**Transformerì˜ êµ¬ì¡°ëŠ”,** [encoder-decoder ì•„í‚¤í…ì²˜](https://yerimoh.github.io/Lan/#transformer-%EB%AA%A8%EB%8D%B8-%EA%B0%9C%EC%9A%94)ë¡œ êµ¬ì„±ë˜ì—ˆìœ¼ë©° sequence-to-sequence ì‘ì—…ì„ ìœ„í•´ ê³ ì•ˆë˜ì—ˆë‹¤.        

ì „ë°˜ì ìœ¼ë¡œ, ë³¸ ë…¼ë¬¸ ëª¨ë¸ì˜ encoder-decoder Transformerì˜ êµ¬í˜„ì€ ì›ë˜ Transformerì˜ í˜•íƒœë¥¼ ë°€ì ‘í•˜ê²Œ ë”°ë¥¸ë‹¤.               

**[Transformerì˜ ë™ì‘ ê³¼ì •]**       
* **1)** í† í°ì˜ input sequenceë¥¼ embeddings sequenceë¥¼ì— ë§¤í•‘í•œ ë‹¤ìŒ encoderë¡œ ì „ë‹¬í•œë‹¤.      
* **2)** encoderëŠ”  â€œblocksâ€ì˜ ìŠ¤íƒìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê° ìŠ¤íƒë§ˆë‹¤ self-attention layer ê³„ì¸µê³¼ small feed-forward networkë¥¼ ê°–ê³ ìˆë‹¤.     
* **3)** ê° ìŠ¤íƒ ì•ˆì˜ 2ê°€ì§€ ìš”ì†Œë“¤ì˜ ì…ë ¥ì—ëŠ” Layer normalizationê°€ ì ìš©ëœë‹¤.     
* **4)** ì´í›„, activationsì´ ì¬ì¡°ì •ë˜ê³  ì¶”ê°€ biasì´ ì ìš©ë˜ì§€ ì•ŠëŠ” ë‹¨ìˆœí™”ëœ ë²„ì „ì˜ Layer normalizationë¥¼ ì‚¬ìš©í•œë‹¤.      
* **5)** layer normalization í›„, residual skip connectionì€ ê° ìŠ¤íƒ ì•ˆì˜ 2ê°€ì§€ ìš”ì†Œì˜ ì…ë ¥ì„ ì¶œë ¥ì— ì¶”ê°€í•œë‹¤.      
* **6)** ë“œë¡­ì•„ì›ƒì€ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë‚´ì—ì„œ ìŠ¤í‚µ ì—°ê²°, ì£¼ì˜ ê°€ì¤‘ì¹˜ ë° ì „ì²´ ìŠ¤íƒì˜ ì…ë ¥ ë° ì¶œë ¥ì— ì ìš©ëœë‹¤.   
* **7)** decoderëŠ” encoderì˜ ì¶œë ¥ì— ì°¸ì—¬í•˜ëŠ” ê° self-attention layer ë‹¤ìŒì— standard attention ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•œë‹¤ëŠ” ì ì„ ì œì™¸í•˜ê³ ëŠ” ì¸ì½”ë”ì™€ êµ¬ì¡°ê°€ ìœ ì‚¬í•˜ë‹¤.       
decoderì˜ self-attention ë©”ì»¤ë‹ˆì¦˜ì€ ëª¨ë¸ì´ ê³¼ê±° ì¶œë ¥ì—ë§Œ ì£¼ì˜ë¥¼ ê¸°ìš¸ì¼ ìˆ˜ ìˆë„ë¡ í•œë‹¤.     
* **8)** ìµœì¢… decoder ë¸”ë¡ì˜ ì¶œë ¥ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥ì„ ê°€ì§„  dense ë ˆì´ì–´ë¡œ ë“¤ì–´ê°€ë©°, ê·¸ ê°€ì¤‘ì¹˜ëŠ” ì…ë ¥ ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ì™€ ê³µìœ ëœë‹¤.      
* âœ¨ ì—¬ê¸°ì„œ, íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ëª¨ë“  attention ë©”ì»¤ë‹ˆì¦˜ì€ ì¶”ê°€ë¡œ ì²˜ë¦¬ë˜ê¸° ì „ì— ì¶œë ¥ì´ ì—°ê²°ë˜ëŠ” ë…ë¦½ì ì¸ â€œheadsâ€ë¡œ ë‚˜ë‰˜ì–´ìˆë‹¤.          
![image](https://user-images.githubusercontent.com/76824611/132572962-94a60e8b-2182-466a-8d1d-47a86ee83a14.gif)
</div>
</details> 
  


### relative position embeddings
Transformerì˜ self-attentionëŠ” ë³‘ë ¬ì²˜ë¦¬ë¥¼í•˜ì—¬ ìˆœì„œ ì •ë³´ë¥¼ ê°–ì§€ ëª»í•˜ë¯€ë¡œ, ì„ë² ë”©ì— ìˆœì„œì •ë³´ë¥¼ ë„£ì–´ì¤€ë‹¤.    
ì´ ë…¼ë¬¸ì—ì„œëŠ” ê¸°ì¡´ Transformerì™€ ë‹¤ë¥¸ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.     
* **ê¸°ì¡´ ëª¨ë¸**: sinusoidal position signal or learned position embeddings (ë‹¨ì–´ì˜ ì ˆëŒ€ì  ìœ„ì¹˜ ì •ë³´ í‘œí˜„)           
* **T5**: [relative position embeddings](https://yerimoh.github.io/LAN18/) (ë‹¨ì–´ì˜ ìƒëŒ€ì  ìœ„ì¹˜ í‘œí˜„)     


      



<details>
<summary>ğŸ“œ relative position embeddingsë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´?(ë³¸ ë…¼ë¬¸ì˜ ì„¤ëª…) </summary>
<div markdown="1">

ê° ìœ„ì¹˜ì— ëŒ€í•´ ê³ ì • ì„ë² ë”©ì„ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , relative position embeddingsì€ self-attentionì—ì„œ ë¹„êµë˜ëŠ” "key"ì™€ "query" ì‚¬ì´ì˜ ì˜¤í”„ì…‹ì— ë”°ë¼ ë‹¤ë¥¸ í•™ìŠµëœ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤.    

ë˜í•œ íš¨ìœ¨ì„±ì„ ìœ„í•´ **ëª¨ë¸ì˜ ëª¨ë“  ë ˆì´ì–´**ì— ê±¸ì³ **position embeddings parametersë¥¼ ê³µìœ **í•˜ì§€ë§Œ,     
**ì£¼ì–´ì§„ layer ë‚´**ì—ì„œ **ê° attention head**ëŠ” **ì„œë¡œ ë‹¤ë¥¸ í•™ìŠµëœ position embeddingì„ ì‚¬ìš©**í•œë‹¤.      

ì¼ë°˜ì ìœ¼ë¡œ, ê°ê° ê°€ëŠ¥í•œ "key"ì™€ "query" ì˜¤í”„ì…‹ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” **ê³ ì •ëœ ìˆ˜ì˜ embeddings**ì´ í•™ìŠµëœë‹¤.      

ì´ ì—°êµ¬ì—ì„œ, ìš°ë¦¬ëŠ” ë¡œê·¸ì ìœ¼ë¡œ **ìµœëŒ€ 128ì˜ ì˜¤í”„ì…‹ê¹Œì§€ í¬ê¸°ê°€ ì¦ê°€**í•˜ëŠ” ë²”ìœ„ë¥¼ ê°€ì§„ **ëª¨ë“  ëª¨ë¸**ì—    
**32ê°œì˜ embeddingì„ ì‚¬ìš©**í•˜ì—¬ ëª¨ë“  relative positionë¥¼ ë™ì¼í•œ ì„ë² ë”©ì— í• ë‹¹í•œë‹¤.      

íŠ¹ì • ê³„ì¸µì€ 128ê°œ í† í°ì„ ì´ˆê³¼í•˜ëŠ” relative positionì— ë¯¼ê°í•˜ì§€ ì•Šì§€ë§Œ,     
subsequent ê³„ì¸µì€ ì´ì „ ê³„ì¸µì˜ ë¡œì»¬ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ë” í° ì˜¤í”„ì…‹ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•  ìˆ˜ ìˆë‹¤.

</div>
</details> 
  
**[summary: T5ì™€ ê¸°ì¡´ Transformerì˜ ì°¨ì´ì ]**       
ì•„ë˜ë¥¼ ì œì™¸í•˜ê³  ê¸°ì¡´ Transformerì™€ ë™ì¼       
* T5ëŠ” Layer Norm biasë¥¼ ì œê±°í•¨    
* Layer normalizationë¥¼ residual path ì™¸ë¶€ì— ë°°ì¹˜     
* ë‹¤ë¥¸ position embedding ë°©ì‹ ì‚¬ìš©(relative position embeddings)    

ì´ëŸ¬í•œ ì•„í‚¤í…ì²˜ ë³€í™”ëŠ” transfer learningì— ëŒ€í•œ ê²½í—˜ì  ì¡°ì‚¬ì—ì„œ ê³ ë ¤í•˜ëŠ” ì‹¤í—˜ ìš”ì†Œì™€ ì§êµí•˜ê¸° ë•Œë¬¸ì—,  
ìš°ë¦¬ëŠ” í–¥í›„ ì‘ì—…ì„ ìœ„í•´ ì˜í–¥ì˜ ì ˆì œë¥¼ ë‚¨ê²¨ë‘ ë‘ .  
  
### ì‹¤í—˜  
* T5ì˜ í™•ì¥ì„±(scalability)ì‹¤í—˜    
â¡ ì¦‰ ë” ë§ì€ parametersë‚˜ layersì„ ê°€ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‹¤í—˜í•œë‹¤.      
* ê²°ê³¼ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ëª¨ë¸ê³¼ ë°ì´í„° ë³‘ë ¬í™”ë¥¼ ê²°í•©í•˜ì—¬ â€œslicesâ€ of Cloud TPU Pods"ì—ì„œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚´.       



----

##  2.2 The Colossal Clean Crawled Corpus
ë°ì´í„°ëŠ”  **large unlabeled** data sets for **unsupervised learning**ë¥¼ ì‚¬ìš©í•œë‹¤.    

í…ìŠ¤íŠ¸ ë°ì´í„° ì†ŒìŠ¤ëŠ” **Common Crawl**ì„ ì‚¬ìš©í•œë‹¤


<details>
<summary>ğŸ“œ Common Crawlì„ ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´? </summary>
<div markdown="1">
  
Common Crawlì´ë€    
* ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ì›¹ ì•„ì¹´ì´ë¸Œì„     
* ìŠ¤í¬ë©ëœ HTML íŒŒì¼ì—ì„œ ë§ˆí¬ì—… ë° ê¸°íƒ€ ë¹„í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì œê±°í•˜ì—¬ "ì›¹ ì¶”ì¶œ í…ìŠ¤íŠ¸"ë¥¼ ì œê³µ      
* ì´ í”„ë¡œì„¸ìŠ¤ëŠ” ë§¤ë‹¬ ì•½ 20TBì˜ ìŠ¤í¬ë©ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìƒì„±.     


preprocessing     
* Common Crawlì˜ ë¬¸ì œ     
   * ë¶ˆí–‰í•˜ê²Œë„, Common Crawlì˜ ê²°ê³¼ í…ìŠ¤íŠ¸ì˜ ëŒ€ë¶€ë¶„ì€ ìì—°ì–´ê°€ ì•„ë‹ˆë‹¤.     
   * ì˜¤ë¥˜ ë©”ì‹œì§€ ë˜ëŠ” ì¤‘ë³µ í…ìŠ¤íŠ¸ì™€ ê°™ì€ íš¡ì„¤ìˆ˜ì„¤í•˜ê±°ë‚˜ boiler-plate í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë¨    
* í•´ê²°: ë‹¤ìŒ íœ´ë¦¬ìŠ¤í‹±ì„ ì‚¬ìš©     
   * ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë˜ëŠ” ë ë”°ì˜´í‘œë¡œ ëë‚˜ëŠ” í–‰ë§Œ ì¶”ì¶œ     
   * 5ê°œ ë¯¸ë§Œì˜ ë¬¸ì¥ì´ ìˆëŠ” í˜ì´ì§€ëŠ” íê¸°í•˜ê³  ìµœì†Œ 3ê°œì˜ ë‹¨ì–´ê°€ í¬í•¨ëœ í–‰ë§Œ ì¶”ì¶œ      
   * "ë”í‹°, ì¥ë‚œ, ì™¸ì„¤ ë˜ëŠ” ê¸°íƒ€ ë‚˜ìœ ë‹¨ì–´ ëª©ë¡"ì— ìˆëŠ” ëª¨ë“  ë‹¨ì–´ê°€ í¬í•¨ëœ í˜ì´ì§€ë¥¼ ì œê±°          
   * ëŒ€ë¶€ë¶„ì˜ ìŠ¤í¬ë© í˜ì´ì§€ì—ëŠ” Javascriptë¥¼ í™œì„±í™”í•´ì•¼ í•œë‹¤ëŠ” ê²½ê³ ê°€ í¬í•¨ë˜ì–´ ìˆì–´ Javascriptë¼ëŠ” ë‹¨ì–´ê°€ ìˆëŠ” ì¤„ì€ ëª¨ë‘ ì œê±°          
   * ì¼ë¶€ í˜ì´ì§€ì—ëŠ” í”Œë ˆì´ìŠ¤í™€ë” "lorem ipsum" í…ìŠ¤íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, "lorem ipsum"ì´ë¼ëŠ” ë¬¸êµ¬ê°€ ë‚˜íƒ€ë‚˜ëŠ” í˜ì´ì§€ëŠ” ëª¨ë‘ ì œê±°      
   *  "{"ê°€ í¬í•¨ëœ ëª¨ë“  í˜ì´ì§€ë¥¼ ì œê±°     
   *  ë°ì´í„° ì„¸íŠ¸ì˜ ì¤‘ë³µì„ ì œê±°í•˜ê¸° ìœ„í•´ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë‘ ë²ˆ ì´ìƒ ë°œìƒí•˜ëŠ” ì„¸ ë¬¸ì¥ ë²”ìœ„ ì¤‘ í•˜ë‚˜ë¥¼ ì œì™¸í•˜ê³  ëª¨ë‘ ì‚­ì œ     


</div>
</details> 

-----

## 2.3 Downstream Tasks
* Sentence acceptability judgment (CoLA (Warstadt et al., 2018))     
* Sentiment analysis (SST-2 (Socher et al., 2013))    
* Paraphrasing/sentence similarity (MRPC (Dolan and Brockett, 2005), STS-B (Ceret al., 2017), QQP (Iyer et al., 2017))    
* Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al.,2016), RTE (Dagan et al., 2005), CB (De Marneff et al., 2019))      
* Coreference resolution (WNLI and WSC (Levesque et al., 2012))   
* Sentence completion (COPA (Roemmele et al., 2011))   
* Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))   
* Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), BoolQ (Clark et al., 2019))      


------

## 2.4 Input and Output Format


**â€œtext-to-textâ€**      
* context ë˜ëŠ” conditioningì„ ìœ„í•´, ëª¨ë¸ì— some textë¥¼ ì œê³µí•œ ë‹¤ìŒ some output textë¥¼ ìƒì„±í•˜ë„ë¡ ìš”ì²­í•˜ëŠ” ì‘ì—…     
* ì¦‰ **input, outputì´ ëª¨ë‘ text**      



**add a task-specific (text) prefix**      
* ëª¨ë¸ì´ **ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ì‘ì—…ì„ ì§€ì •**í•˜ê¸° ìœ„í•´ ì…ë ¥ ì‹œí€€ìŠ¤ì— ì¶”ê°€      
* ex       
  * **translate task**       
  ex) ì˜ì–´ì—ì„œ ë…ì¼ì–´ë¡œ ë¬¸ì¥ "That is good"ë¥¼ ë²ˆì—­í•˜ëŠ” taskëŠ”,    
  ëª¨ë¸ì˜ fed the sequenceê°€ ```â€œtranslate English to German: That is good."```ì´ê³ ,         
  ëª¨ë¸ì€ ```â€œDas ist gut.â€```ë¥¼ ì¶œë ¥í•˜ë„ë¡ í›ˆë ¨ë  ê²ƒì´ë‹¤.       
  * **text classification tasks**
  ëª¨ë¸ì€ target labelì— í•´ë‹¹í•˜ëŠ” single wordë§Œ ì˜ˆì¸¡           
  ex) MNLI benchmarkì˜ ëª©í‘œëŠ” ì „ì œê°€ ê°€ì„¤ì„ ì•”ì‹œí•˜ëŠ”ì§€(â€œentailmentâ€), ëª¨ìˆœë˜ëŠ”ì§€(â€œcontradictionâ€), ë˜ëŠ” ë‘˜ ë‹¤(â€œneutralâ€)ì¸ì§€ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„.       
  ì…ë ¥ ì‹œí€€ìŠ¤ëŠ” ```â€œmnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.â€```ì´ë©´,     
  ëª¨ë¸ì€ target ë‹¨ì–´ "entailment"ì„ ì˜ˆì¸¡í•´ì•¼í•œë‹¤.     
* task-specific (text) prefixì˜ ì„ íƒì€ **í•˜ì´í¼íŒŒë¼ë¯¸í„°**       
* ì•„ë˜ "ë” ì•Œì•„ë³´ê¸°"ì—ì„œ ì—°êµ¬í•œ ëª¨ë“  ì‘ì—…ì— ëŒ€í•´ prefix ì…ë ¥ì˜ ëª¨ë“  ì˜ˆë¥¼ ì œê³µ    




<img width="340" alt="image" src="https://github.com/yerimoh/yerimoh.github.io/assets/76824611/dd4e89c3-64a0-4245-a206-527cc0a22a34">


    

<details>
<summary>ğŸ“œ ë” ì•Œì•„ë³´ê¸°: prefix ì…ë ¥ì˜ ëª¨ë“  ì˜ˆ </summary>
<div markdown="1">
  
**Text-to-text**     
Our text-to-text framework provides a simple way to train a single model
on a wide variety of text tasks using the same loss function and decoding procedure.
We showed how this approach can be successfully applied to generative tasks like
abstractive summarization, classification tasks like natural language inference, and
even regression tasks like STS-B. In spite of its simplicity, we found the text-totext framework obtained comparable performance to task-specific architectures and
ultimately produced state-of-the-art results when combined with scale.


**Architectures**        
While some work on transfer learning for NLP has considered architectural
variants of the Transformer, we found the original encoder-decoder form worked
best in our text-to-text framework. Though an encoder-decoder model uses twice as
many parameters as â€œencoder-onlyâ€ (e.g. BERT) or â€œdecoder-onlyâ€ (language model)
architectures, it has a similar computational cost. We also showed that sharing the
parameters in the encoder and decoder did not result in a substantial performance
drop while halving the total parameter count.



**Unsupervised objectives**      
Overall, we found that most â€œdenoisingâ€ objectives, which train
the model to reconstruct randomly corrupted text, performed similarly in the text-totext setup. As a result, we suggest using objectives that produce short target sequences
so that unsupervised pre-training is more computationally efficient.


**Data sets**         
We introduced the â€œColossal Clean Crawled Corpusâ€ (C4), which comprises
heuristically-cleaned text from the Common Crawl web dump. When comparing C4 to
data sets that use additional filtering, we found that training on in-domain unlabeled
data could boost performance in a few downstream tasks. However, constraining to
a single domain typically results in a smaller data set. We separately showed that
performance can degrade when an unlabeled data set is small enough that it is repeated
many times over the course of pre-training. This motivates the use of a large and
diverse data set like C4 for generic language understanding tasks.


**Training strategies**      
We found that the basic approach of updating all of a pre-trained
modelâ€™s parameters during fine-tuning outperformed methods that are designed to
update fewer parameters, although updating all parameters is most expensive. We also
experimented with various approaches for training the model on multiple tasks at once,
which in our text-to-text setting simply corresponds to mixing examples from different
data sets when constructing batches. The primary concern in multi-task learning is
setting the proportion of each task to train on. We ultimately did not find a strategy
for setting mixing proportions that matched the performance of the basic approach of
unsupervised pre-training followed by supervised fine-tuning. However, we found that
fine-tuning after pre-training on a mixture of tasks produced comparable performance
to unsupervised pre-training.


**Scaling**       
We compared various strategies for taking advantage of additional compute, including training the model on more data, training a larger model, and using an ensemble
of models. We found each approach conferred a significant boost in performance,
though training a smaller model on more data was often outperformed by training
a larger model for fewer steps. We also showed an ensemble of models can provide
substantially better results than a single model, which provides an orthogonal means
of leveraging additional computation. Ensembling models that were fine-tuned from
the same base pre-trained model performed worse than pre-training and fine-tuning
all models completely separately, though fine-tune-only ensembling still substantially
outperformed a single model.


**Pushing the limits**     
We combined our above insights and trained substantially larger
models (up to 11 billion parameters) to achieve state-of-the-art results across many of
the benchmarks we considered. For unsupervised training, we extracted text from our
C4 data set and applied a denoising objective that corrupts contiguous spans of tokens.
We pre-trained on a multi-task mixture before fine-tuning on individual tasks. Overall,
our models were trained on over 1 trillion tokens. In the interest of facilitating the
replication, extension, and application of our results, we release our code, the C4 data
set, and pre-trained model weights for each T5 variant.1

</div>
</details> 




---
---


# 3. Experiments 


## 3.1 Baseline

baselineì— ëŒ€í•œ ëª©í‘œëŠ” **typical modern ê´€í–‰ì„ ë°˜ì˜**í•˜ëŠ” ê²ƒì´ë‹¤.        
simple denoising objectiveë¥¼ ì‚¬ìš©í•˜ì—¬ Transformerë¥¼ pre-trainí•œ ë‹¤ìŒ ê° downstream tasksì„ ë³„ë„ë¡œ fine-tuneí•œë‹¤.       

### 3.1.1 Model


model: standard encoder-decoder Transformerë¥¼ ì‚¬ìš©      


ìš°ë¦¬ì˜ ê¸°ë³¸ ëª¨ë¸ì€ ì¸ì½”ë”ì™€ ë””ì½”ë”ê°€ ê°ê° "BERTBASE"(ë°ë¸”ë¦° ì™¸, 2018) ìŠ¤íƒê³¼ í¬ê¸°ì™€ êµ¬ì„±ì´ ìœ ì‚¬í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ì¸ì½”ë”ì™€ ë””ì½”ë”ëŠ” ëª¨ë‘ 12ê°œì˜ ë¸”ë¡(ê° ë¸”ë¡ì€ ìê¸° ì£¼ì˜, ì„ íƒì  ì¸ì½”ë”-ë””ì½”ë” ì£¼ì˜ ë° í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ë¡œ êµ¬ì„±ë¨)ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê° ë¸”ë¡ì˜ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ëŠ” ì¶œë ¥ ì°¨ì›ì´ dff = 3072ì¸ ê³ ë°€ë„ ë ˆì´ì–´ì™€ ReLU ë¹„ì„ í˜•ì„± ë° ë˜ ë‹¤ë¥¸ ê³ ë°€ë„ ë ˆì´ì–´ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ëª¨ë“  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ "í‚¤" ë° "ê°’" í–‰ë ¬ì€ dkv = 64ì˜ ë‚´ë¶€ ì°¨ì›ì„ ê°€ì§€ë©° ëª¨ë“  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì—ëŠ” 12ê°œì˜ í—¤ë“œê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë“  í•˜ìœ„ ëª¨ë¸ ë° ì„ë² ë”©ì˜ ì¹˜ìˆ˜ëŠ” d ëª¨ë¸ = 768ì…ë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ, ì´ê²ƒì€ ì•½ 2ì–µ 2ì²œë§Œ ê°œì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê°€ì§„ ëª¨ë¸ì´ ë©ë‹ˆë‹¤. ê¸°ì¤€ ëª¨ë¸ì— í•˜ë‚˜ì˜ ë ˆì´ì–´ ìŠ¤íƒì´ ì•„ë‹Œ ë‘ ê°œì˜ ë ˆì´ì–´ ìŠ¤íƒì´ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì´ëŠ” BERTBASE ë§¤ê°œ ë³€ìˆ˜ì˜ ì•½ ë‘ ë°°ì…ë‹ˆë‹¤. ì •ê·œí™”ë¥¼ ìœ„í•´ ëª¨ë¸ì— ë“œë¡­ì•„ì›ƒì´ ì ìš©ë˜ëŠ” ëª¨ë“  ê³³ì—ì„œ ë“œë¡­ì•„ì›ƒ í™•ë¥  0.1ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

Our baseline model is designed so that the encoder and decoder are each similar in
size and configuration to a â€œBERTBASEâ€ (Devlin et al., 2018) stack. Specifically, both the
encoder and decoder consist of 12 blocks (each block comprising self-attention, optional
encoder-decoder attention, and a feed-forward network). The feed-forward networks in each
block consist of a dense layer with an output dimensionality of dff = 3072 followed by a
ReLU nonlinearity and another dense layer. The â€œkeyâ€ and â€œvalueâ€ matrices of all attention
mechanisms have an inner dimensionality of dkv = 64 and all attention mechanisms have 12
heads. All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total,
this results in a model with about 220 million parameters. This is roughly twice the number
of parameters of BERTBASE since our baseline model contains two layer stacks instead of
one. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied
in the model.


### 3.1.2 Training
As described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to
always train using standard maximum likelihood, i.e. using teacher forcing (Williams and
Zipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and
Stern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability
logit at every timestep).

We pre-train each model for 2
19 = 524,288 steps on C4 before fine-tuning. We use a
maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible,
we â€œpackâ€ multiple sequences into each entry of the batch10 so that our batches contain
roughly 2
16 = 65,536 tokens. In total, this batch size and number of steps corresponds
to pre-training on 2
35 â‰ˆ 34B tokens. This is considerably less than BERT (Devlin et al.,
2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly
2.2T tokens. Using only 2
35 tokens results in a reasonable computational budget while still
providing a sufficient amount of pre-training for acceptable performance. We consider the
effect of pre-training for more steps in Sections 3.6 and 3.7. Note that 2
35 tokens only covers
a fraction of the entire C4 data set, so we never repeat any data during pre-training





During pre-training, we use an â€œinverse square rootâ€ learning rate schedule: 1
p
max(n, k)
where n is the current training iteration and k is the number of warm-up steps (set to 104
in all of our experiments). This sets a constant learning rate of 0.01 for the first 104
steps,
then exponentially decays the learning rate until pre-training is over. We also experimented
with using a triangular learning rate (Howard and Ruder, 2018), which produced slightly
better results but requires knowing the total number of training steps ahead of time. Since
we will be varying the number of training steps in some of our experiments, we opt for the
more generic inverse square root schedule.


Our models are fine-tuned for 2
18 = 262,144 steps on all tasks. This value was chosen
as a trade-off between the high-resource tasks (i.e. those with large data sets), which
benefit from additional fine-tuning, and low-resource tasks (smaller data sets), which overfit
quickly. During fine-tuning, we continue using batches with 128 length-512 sequences (i.e.
2
16 tokens per batch). We use a constant learning rate of 0.001 when fine-tuning. We save
a checkpoint every 5,000 steps and report results on the model checkpoint corresponding
to the highest validation performance. For models fine-tuned on multiple tasks, we choose
the best checkpoint for each task independently. For all of the experiments except those in
Section 3.7, we report results in the validation set to avoid performing model selection on
the test set















